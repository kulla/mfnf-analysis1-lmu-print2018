\documentclass[fontsize=9pt,
               parskip=half-,
               DIV=14,
               listof=chapterentry,
               tocflat]{scrbook}

% paper with page trim: 161mm:241mm
% paper without page trim: 155mm:235mm

% set font size to 9pt and base line height to 12pt
\changefontsizes[12pt]{9pt}
\usepackage[paperwidth=161mm, paperheight=241mm, top=20.5mm, bottom=32.6mm, outer=22mm, inner=18.5mm]{geometry}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[ngerman]{babel}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{cancel}
\usepackage{booktabs}
\usepackage[space]{grffile}
\usepackage{verbatim}
\usepackage{changepage}
\usepackage{marvosym}
\usepackage{multicol}
\usepackage[usenamesm,svgnames,dvipsnames,table]{xcolor}

% pdfencoding=auto for supporting special characters in PDF's table of contents
\usepackage[hidelinks,breaklinks,pdfencoding=auto]{hyperref}
\usepackage[font={color=sblau},justification=raggedright,labelformat=empty]{caption}
\usepackage{float}
\usepackage[normalem]{ulem}
\usepackage{csquotes}
\usepackage[framemethod=default]{mdframed}
\usepackage[toctextentriesindented]{tocstyle}
\usepackage{enumitem}
\usepackage{etoolbox}
\usepackage{tabularx}
\usepackage{longtable}
\usepackage{tabu}
\usepackage{scrlayer-scrpage}
\usepackage{pdfpages}
\usepackage{microtype}

\addtocontents{lof}{{Alle Abbildungen sind auffindbar auf \textbf{Wikimedia Commons}, einer Sammlung von frei verwendbaren Mediendateien, zu der jeder beitragen kann. Wenn du eine Abbildung weiterverwenden willst, findest du hier weitere Informationen:\\ \textbf{https://commons.wikimedia.org/wiki/File:<BILDNAME>}
}\\}
\addtocontents{lof}{}

% remove vertical space after floats

% Support for \euro in math and text mode
\usepackage{eurosym}
\DeclareRobustCommand{\officialeuro}{%
  \ifmmode\expandafter\text\fi
    {\fontencoding{U}\fontfamily{eurosym}\selectfont e}}

\newcounter{imagelabel}
\setlength{\intextsep}{0pt plus 2pt}   % default value 12pt plus 2pt minus 2pt

% allow max height and max width for image includes
\usepackage{adjustbox}

\DeclareCaptionLabelFormat{blank}{}

% show overfull boxes
\overfullrule=1mm

% Redefine \rm for math environment -> raises an error otherwise
% TODO: Is there a better solution
\renewcommand*{\rm}[1]{\mathrm{#1}}

\usepackage{fontspec}
% voodoo
% fontspec and titlesec have errors displaying math font, unless \maketitle is
% used. This piece of black magic solves the problem.
% https://github.com/wspr/unicode-math/issues/253
\makeatletter
\AtBeginDocument{%
 \let\glb@currsize\relax
}
\makeatother
\setmainfont[Ligatures=TeX,
             Extension=.ttf,
             BoldFont=Karmilla-Bold-016m,
             ItalicFont=Karmilla-Italic-016m,
             Path=karmilla/ttf/]{Karmilla-Regular-016m}
% fallback font or unicode (like smileys)
\newfontfamily\DejaSans{DejaVu Sans}

\newlength{\lineheight}
\makeatletter\setlength{\lineheight}{\f@size pt}\makeatother

\allowdisplaybreaks[4]
\predisplaypenalty=0

\setcounter{secnumdepth}{1}

% define colors in rgb colorspace (for digital pdfs)
% \definecolor{sblau}{HTML}{007ec1}
% \colorlet{shellblau}{sblau!15}
% \definecolor{forestgreen}{rgb}{0.13, 0.55, 0.13}

% define colors in cmyk colorspace (for print)
%\definecolor{sblau}{cmyk}{0.83,0.41,0,0}
%\definecolor{shellblau}{cmyk}{0.39,0.05,0.03,0}
%\definecolor{forestgreen}{cmyk}{0.82,0.19,1,0.05}
\definecolor{sblau}{cmyk}{0,0,0,0.5}
\definecolor{shellblau}{cmyk}{0,0,0,0.3}
\definecolor{forestgreen}{cmyk}{0.82,0.19,1,0.05}

% change font and color of headers and table of contents
\addtokomafont{disposition}{\rmfamily}
% change font of descriptionlabels
\addtokomafont{descriptionlabel}{\rmfamily}
% change font sizes of headings
\addtokomafont{title}{\changefontsizes[59pt]{44pt}}
\addtokomafont{part}{\changefontsizes[59pt]{44pt}\color{white}}
\addtokomafont{partprefix}{\changefontsizes[12pt]{9pt}\color{white}}
\KOMAoption{chapterprefix}{true}
\addtokomafont{chapter}{\changefontsizes[28pt]{21pt}}
\addtokomafont{section}{\changefontsizes[16pt]{12pt}}
\addtokomafont{subsection}{\changefontsizes[12pt]{9pt}}
\addtokomafont{chapterprefix}{\changefontsizes[12pt]{9pt}\color{sblau}}
\RedeclareSectionCommand[innerskip=-\baselineskip]{chapter}

\RedeclareSectionCommand[beforeskip=28pt plus 1pt minus 0pt,
                         afterskip=28pt plus 1pt minus 0pt]{chapter}
\RedeclareSectionCommand[beforeskip=20pt plus 1pt minus 0pt,
                         afterskip=1pt plus 1pt minus 0pt]{section}
\RedeclareSectionCommand[beforeskip=12pt plus 1pt minus 0pt,
                         afterskip=1pt plus 1pt minus 0pt]{subsection}
\RedeclareSectionCommand[beforeskip=12pt plus 1pt minus 0pt,
                         afterskip=1pt plus 1pt minus 0pt]{subsubsection}

\newcommand{\BluePageBG}{\colorbox{shellblau}{\parbox[t][\paperheight][l]{\paperwidth}{${}$}}}

\DeclareNewLayer[background,
                 mode=picture,
                 align=lt,
                 area={0pt}{0pt}{\paperwidth}{\paperheight},
                 contents=\putUL{\BluePageBG}]{partlayer}
\DeclarePageStyleByLayers{part}{partlayer}
\renewcommand\partpagestyle{part}
\addtokomafont{part}{\raggedright}
\addtokomafont{partprefix}{\raggedright}
\RedeclareSectionCommand[beforeskip=0pt,innerskip=0pt]{part}
\renewcommand*\thepart{\arabic{part}}

\makeatletter
\patchcmd{\scr@startchapter}{\if@openright\cleardoublepage\else\clearpage\fi}{\clearpage}{}{}
\def\@endpart{}
\makeatother

\errorcontextlines 10000

\mdfdefinestyle{semanticbox}{innerleftmargin=8.25pt,
                             innerrightmargin=0pt,
                             innertopmargin=0pt,
                             innerbottommargin=1pt,
                             startcode=\leavevmode,
                             linecolor=shellblau,
                             linewidth=2.75pt,
                             topline=false,
                             rightline=false,
                             bottomline=false,
                             theoremseparator=\\,
                             frametitlefont=\normalfont,
                             theoremtitlefont={\normalfont\bfseries\color{Black}},
                             frametitleaboveskip=0.5pt,
                             frametitlebelowskip=\baselineskip}

\mdfdefinestyle{importantbox}{innerleftmargin=8.25pt,
                              innerrightmargin=0pt,
                              innertopmargin=2pt,
                              innerbottommargin=1pt,
                              startcode=\leavevmode,
                              linecolor=shellblau,
                              linewidth=2.75pt,
                              topline=false,
                              rightline=false,
                              bottomline=false,
                              fontcolor=Black}

\newlength{\IconHeight}
\setlength\IconHeight{20px}

\newmdenv[style=importantbox]{importantparagraph*}
\newenvironment{indentblock}{\begin{adjustwidth}{.5cm}{}}{\end{adjustwidth}}

\newcommand{\BoxIcon}[1]{\rlap{\protect\makebox[-2.2\IconHeight]{\raisebox{-0.65\IconHeight}[0pt][0pt]{\includegraphics[height=\IconHeight]{icons/#1.pdf}}}}}

\mdtheorem[style=semanticbox]{theorem}{\BoxIcon{theorem}Satz}[chapter]
\mdtheorem[style=semanticbox]{definition}{\BoxIcon{definition}Definition}[chapter]
\mdtheorem[style=semanticbox]{exercise}{\BoxIcon{exercise}Übung}[chapter]
\mdtheorem[style=semanticbox]{solution}{\BoxIcon{solution}Lösung}[chapter]
\mdtheorem[style=semanticbox]{alternativeproof}{\BoxIcon{proof}Alternativer Beweis}[chapter]
\mdtheorem[style=semanticbox]{warning}{\BoxIcon{warning}Warnung}[chapter]
\mdtheorem[style=semanticbox]{hint}{\BoxIcon{hint}Hinweis}[chapter]
\mdtheorem[style=semanticbox]{explanation}{Erklärung}[chapter]
\mdtheorem[style=semanticbox]{proofsummary}{\BoxIcon{proofsummary}Beweiszusammenfassung}[chapter]
\mdtheorem[style=semanticbox]{solutionprocess}{\BoxIcon{solutionprocess}Lösungsweg}[chapter]
\mdtheorem[style=semanticbox]{example}{\BoxIcon{example}Beispiel}[chapter]
\mdtheorem[style=semanticbox]{answer}{Antwort}[chapter]
\mdtheorem[style=semanticbox]{induction}{Induktionsbeweis}[chapter]

\makeatletter
\let\proof\@undefined
\let\endproof\@undefined
\makeatother
\mdtheorem[style=semanticbox]{proof}{\BoxIcon{proof}Beweis}[chapter]

\newenvironment{authors}{\par\vspace*{\fill}\color{white}Autoren und
Autorinnen\\\bfseries}{\clearpage}

\newcommand{\ColoredTOC}{{\color{Black}\setcounter{tocdepth}{0}\tableofcontents}}
\newcommand{\ColoredLOF}{{\color{Black}\setcounter{tocdepth}{10}\listoffigures}}
\makeatletter
\renewcommand\@dotsep{10000}
\makeatother

\usetocstyle[lof]{KOMAlike}
\usetocstyle{KOMAlike}
\settocstylefeature[-1]{pagenumberbox}{\csname @gobble\endcsname}
%\settocstylefeature[0]{pagenumberbox}{\csname @gobble\endcsname}
\settocfeature[lof]{raggedhook}{\raggedright}
\settocfeature[lof]{entryvskip}{5pt plus 1pt minus 1pt}

% width of page numbers in toc
\makeatletter
\renewcommand\@pnumwidth{1.5cm}
\makeatother

\setlist{leftmargin=11pt,labelindent=0pt,font=\color{Black}}

\newcommand{\proofstep}[1]{\textbf{\textcolor{Black}{#1}}}
\newcommand{\proofcase}[2]{\textbf{\textcolor{Black}{Fall #1:}} #2}
\newcommand{\inductionstep}[2]{\textbf{\textcolor{Black}{#1.}} #2}

% no additional space around equations is needed since they are always in their own paragraph
% see https://tex.stackexchange.com/a/69665
\expandafter\def\expandafter\normalsize\expandafter{%
\normalsize
\abovedisplayskip=-\baselineskip % if a paragraph starts with an equation then a blank extra line appears
\abovedisplayshortskip=-\baselineskip
\belowdisplayskip=0pt
\belowdisplayshortskip=0pt
}

\title{Analysis1}

\date{}

\begin{document}

\sloppy

\clearpage%
\thispagestyle{empty}%
\null%
\clearpage

\includepdf[pages=-,addtotoc={4,part,-1,Über das Buchprojekt,sec:about}]{predesigned_pages/mfnf_prelude}

\addxcontentsline{lof}{part}[\arabic{part}]{Über das Buchprojekt}\ColoredTOC

\newpage

\part{Einleitung}

\addxcontentsline{lof}{part}[\arabic{part}]{Einleitung}\begin{authors}
Stephan Kulla, Autorenkollektiv „Auswahlaxiom“ (Charlotte Dietze, Matthias Paulsen, Anne Reif), Alexander Sedlmayr, Jenny Kilian, Theresa Plomer, Agnes Pauer, 0-Brane\end{authors}

\chapter{Was sind reelle Zahlen?}

Reelle Zahlen bilden die Grundlage für dieses Lehrbuch. Schließlich wollen wir uns mit Folgen reeller Zahlen oder mit reellwertigen Funktionen beschäftigen. Bevor wir mit dem Studium der Analysis beginnen, sollten wir uns zunächst fragen: \emph{Was sind reelle Zahlen?}

Das ist gar keine einfache Frage. Schauen wir uns zunächst auf der Meta-Ebene die Möglichkeiten an, diese zu definieren.

\section{Die Beschreibungsmöglichkeiten reeller Zahlen}

Jeder von uns hat bereits eine intuitive Vorstellung, was reelle Zahlen sind, auch wenn nicht alle diese Idee in Worte fassen können. Beispielsweise kann man sich die reellen Zahlen als Punkte auf der Zahlengeraden vorstellen:

\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Simple_number_line.svg}{\textbf{Simple\allowbreak\_number\allowbreak\_line.svg}} by Fleshgrinder \textit{(Public domain)}}\begin{center}
\adjincludegraphics[max width=0.5\textwidth,max height=0.2\textheight]{file58simple95number95line95a9c46f874f6374269a6dd92339e9a2be62338ab0}\end{center}

Unsere Aufgabe besteht darin, diese intuitive Idee in die exakte Sprache der Mathematik zu übersetzen. Dazu haben wir zwei Möglichkeiten: die \emph{axiomatische} und die \emph{konstruktive} Beschreibung.

\subsection{Die axiomatische Beschreibung}

\begin{figure}[h]
\vspace{\baselineskip}
\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Skizze_zum_mathematischen_Gebäude.svg}{\textbf{Skizze\allowbreak\_zum\allowbreak\_mathematischen\allowbreak\_Gebäude.svg}} by Autorenkollektiv „Auswahlaxiom“ (Charlotte Dietze, Matthias Paulsen, Anne Reif), Golmman, Stephan Kulla \textit{(CC BY 4.0)}}\centering
\adjincludegraphics[max width=.5\textwidth, max height=0.2\textheight]{file58skizze95zum95mathematischen95geb228ude95fc2a2d6fd53bfec539c258f8dceb6ac272d55c1c}
\caption*{In der axiomatischen Beschreibung der Analysis legen wir mit den Axiomen das Fundament, auf dem wir schrittweise die Theoreme der Analysis herleiten. (\arabic{imagelabel})}
\end{figure}
Bei der axiomatischen Beschreibung wird nicht direkt gesagt, was reelle Zahlen sind, es wird vielmehr nur erklärt, welche Eigenschaften sie haben. Bei dieser Herangehensweise sagen wir: „Die reellen Zahlen sind eine Menge von Objekten, die folgende charakteristische Eigenschaften besitzen: \textless{}Aufzählung der Eigenschaften reeller Zahlen\textgreater{}“. Die grundlegenden Eigenschaften werden dabei über \href{https://de.wikibooks.org/wiki/Mathe\_für\_Nicht-Freaks:\_Wörterbuch\#Anker:Axiom}
{Axiome} festgelegt. Zur Erinnerung: Ein Axiom ist eine \href{https://de.wikibooks.org/wiki/Mathe\_für\_Nicht-Freaks:\_Aussagenlogik\#Aussage}
{Aussage}, die ohne Beweis als wahr angenommen wird. Jedes mathematische Modell, das alle genannten Eigenschaften/Axiome erfüllt, wird als Modell der reellen Zahlen angesehen. Aussagen, die auf Grundlage der Axiome bewiesen werden, werden Theoreme genannt.

Bei der Wahl der Axiome müssen wir darauf achten, dass sie widerspruchsfrei sind. Es muss also mindestens ein Modell geben, das alle Axiome erfüllt. Beispielsweise können wir nicht sagen, dass gleichzeitig $1<0$ als auch $1\geq 0$ gelten soll. Eine solche Struktur kann es nämlich nicht geben. Außerdem sollten die Axiome nachvollziehbar sein, also wirklich Eigenschaften bezeichnen, die wir auch intuitiv den reellen Zahlen zuschreiben.

Des Weiteren müssen ausreichend viele Axiome zur Charakterisierung der reellen Zahlen definiert sein. Bei zu wenigen Axiomen könnten auch Strukturen diese erfüllen, die wir intuitiv nicht als Modell reeller Zahlen ansehen. So ist es beispielsweise nicht ausreichend zu sagen, dass es für die reellen Zahlen eine Addition $+$ mit $x+y=y+x$ für alle $x$ und $y$ gibt. Allein dieses Axiom ist zu wenig, denn die natürlichen Zahlen erfüllen diese Eigenschaft auch. Sie sind für uns aber kein Modell reeller Zahlen.

Außerdem soll eine gewisse Sparsamkeit beachtet werden: Es sollten keine Axiome unnötig definiert werden. Dies bedeutet, dass keine Eigenschaften als Axiome benannt werden, die sich bereits aus anderen Axiomen herleiten lassen. Wenn also aus den Axiomen $A_{1}$ und $A_{2}$ bereits folgt, dass auch die Eigenschaft $E$ erfüllt sein muss, dann wird $E$ nicht extra als Axiom definiert.

\subsection{Die konstruktive Beschreibung}

Bei der \emph{konstruktiven Beschreibung} werden die reellen Zahlen aus den \href{https://de.wikipedia.org/wiki/Rationale\%20Zahl}
{rationalen Zahlen} konstruiert. Das bedeutet, dass durch ein gewisses Verfahren aus rationalen Zahlen neue Objekte geschaffen werden, welche man danach als reelle Zahlen definiert.

Anders als bei der axiomatischen Beschreibung, welche die reellen Zahlen nur durch ihre Eigenschaften beschreibt, kann man beim konstruktiven Verfahren genau sagen, was die reellen Zahlen sind. Es sind genau die Objekte, die durch das Konstruktionsverfahren entstanden sind. Die Eigenschaften der reellen Zahlen müssen bei dieser Beschreibung auch nicht durch Axiome definiert werden, sondern ergeben sich aus den Eigenschaften der konstruierten Objekte.

Es gibt mehrere Konstruktionsverfahren der reellen Zahlen. Die dabei entstandenen Strukturen sind jedoch äquivalent in dem Sinne, dass sie dieselben Eigenschaften haben (man nennt solche Strukturen „isomorph“).

\subsection{Der Zusammenhang beider Beschreibungen}

Beide Vorgehensweisen liefern am Ende dieselben Ergebnisse. Wenn man für ein konstruiertes Modell alle diejenigen Eigenschaften nachweisen kann, die man als Axiome in der axiomatischen Beschreibung definiert hat, dann besitzt das Modell auch die Eigenschaften, die man aus den Axiomen hergeleitet hat. Wenn man umgekehrt in der konstruierten Beschreibung nur diejenigen Eigenschaften des Modells für spätere Argumentationen heranzieht, die man in der axiomatischen Beschreibung als Axiome definieren würde, dann kann man auch alle mit dem Modell bewiesenen Sätze in der axiomatischen Beschreibung beweisen.

Es ist also egal, welchen Weg wir wählen. Zu Beginn gehen wir den Weg der axiomatischen Beschreibung, weil er leichter zu verstehen ist (für die konstruktive Beschreibung brauchen wir Konzepte, die Studienanfänger in der Regel noch nicht oder nicht ausreichend kennen).

\section{Zusammenfassung: Weg zur axiomatischen Beschreibung reeller Zahlen}

Um die Axiome der reellen Zahlen zu finden, kann man folgendermaßen vorgehen:

\begin{enumerate}
\item \textbf{Intuitive Idee entwickeln:} Zunächst brauchen wir eine intuitive Idee der reellen Zahlen. Hierzu können wir beispielsweise auf die intuitive Idee zurückgreifen, dass reelle Zahlen Punkte auf der Zahlengeraden sind.
\item \textbf{Axiome definieren:} Nun müssen alle Axiome definiert werden. Dazu müssen wir folgende Punkte beachten: \begin{itemize}
\item \emph{Nachvollziehbarkeit:} Die Axiome sollten sinnvoll sein. Das bedeutet, dass jedes Axiom intuitiv nachvollziehbar ist.
\item \emph{Widerspruchsfreiheit:} Die Axiome müssen in sich widerspruchsfrei sein. Das kann implizit dadurch gezeigt werden, dass ein konkretes Modell der reellen Zahlen konstruiert werden kann. Damit es ein Modell der reellen Zahlen geben kann, müssen die Axiome nämlich in sich widerspruchsfrei sein.
\item \emph{Vollständigkeit:} Sobald ein Modell alle Axiome erfüllt, sollte es unserer intuitiven Vorstellung von reellen Zahlen entsprechen. Alle Theoreme über reelle Zahlen müssen wir aus den Axiomen herleiten können.
\item \emph{Sparsamkeit:} Kein Axiom kann aus den anderen hergeleitet werden.
\end{itemize}


\item \textbf{Begriffe definieren und Theoreme beweisen:} Aufbauend auf den Axiomen werden wir Grundbegriffe der Analysis einführen und die Theoreme der Analysis beweisen.
\end{enumerate}

In der Analysis werden alle Theoreme mit Hilfe der Axiome reeller Zahlen bewiesen. Dabei wird es zwangsweise vorkommen, dass wir Konzepte einführen oder Sätze beweisen, die schon aus der Schule bekannt sind – wie die Gleichung $1\cdot 0=0$. Bei diesen Beweisen ist es wichtig, dass wir nur solche Eigenschaften reeller Zahlen heranziehen, die entweder in den Axiomen definiert wurden oder die wir bereits bewiesen haben. Bekannte Tatsachen aus der Schule dürfen nicht ohne weiteres verwendet werden!

Die Axiome der reellen Zahlen können in drei Gruppen aufgeteilt werden: Die \emph{Körperaxiome}, die \emph{Anordnungsaxiome} und das \emph{Vollständigkeitsaxiom}.

\section{Die Körperaxiome}

Mit den Körperaxiomen wird die Addition und Multiplikation, also die arithmetische Struktur der reellen Zahlen, definiert. Die reellen Zahlen sind Objekte, die addiert und multipliziert werden können, wobei die grundlegenden Eigenschaften der Additionen und Multiplikationen genannt werden. Die Subtraktion und die Division werden auf die Addition beziehungsweise die Multiplikation zurückgeführt. Diese Axiomengruppe beschreibt, wie man mit reellen Zahlen rechnen kann.

\begin{definition*}[Körperaxiome]
Auf der Menge der reellen Zahlen $\mathbb {R} $ sind zwei Operationen $+:\mathbb {R} \times \mathbb {R} \to \mathbb {R} $ und $\cdot :\mathbb {R} \times \mathbb {R} \to \mathbb {R} $ definiert. Diese erfüllen folgende Eigenschaften:

\begin{itemize}
\item Eigenschaften der Addition: \begin{itemize}
\item \emph{Assoziativgesetz der Addition:} Für alle reellen Zahlen $x,y,z$ gilt $x+(y+z)=(x+y)+z$.
\item \emph{Kommutativgesetz der Addition:} Für alle reellen Zahlen $x$ und $y$ gilt $x+y=y+x$.
\item \emph{Existenz der Null:} Es gibt mindestens eine reelle Zahl $0\in \mathbb {R} $, für die $0+x=x$ für alle reellen Zahlen $x$ gilt.
\item \emph{Existenz des Negativen:} Für jede reelle Zahl $x$ gibt es mindestens eine reelle Zahl $-x$ mit $x+(-x)=0$.
\end{itemize}


\item Eigenschaften der Multiplikation: \begin{itemize}
\item \emph{Assoziativgesetz der Multiplikation:} Für alle reellen Zahlen $x,y,z$ gilt $x\cdot (y\cdot z)=(x\cdot y)\cdot z$.
\item \emph{Kommutativgesetz der Multiplikation:} Für alle reellen Zahlen $x$ und $y$ gilt $x\cdot y=y\cdot x$.
\item \emph{Existenz der Eins:} Es gibt mindestens eine reelle Zahl $1\in \mathbb {R} $ mit $1\neq 0$, für die $1\cdot x=x$ für alle reellen Zahlen $x$ gilt.
\item \emph{Existenz des Inversen:} Für jede reelle Zahl $x\neq 0$ gibt es mindestens eine reelle Zahl $x^{-1}$ mit $x\cdot x^{-1}=1$.
\end{itemize}


\item \emph{Distributivgesetz:} Für alle reellen Zahlen $x,y,z$ gilt $x\cdot (y+z)=x\cdot y+x\cdot z$.
\end{itemize}

\end{definition*}

\section{Die Anordnungsaxiome}

Die Anordnungsaxiome beschreiben die lineare Ordnung der reellen Zahlen. Die reellen Zahlen sind also Objekte, die man miteinander vergleichen kann, wobei für zwei verschiedene reelle Zahlen entweder die eine Zahl größer ist als die andere oder umgekehrt. Dadurch ergibt sich ein wesentlicher Zusammenhang der Struktur der reellen Zahlen mit der einer Geraden, da auch die Punkte einer Geraden in natürlicher Art und Weise geordnet sind. Diese Axiomengruppe ist also wesentlich für die Vorstellung der reellen Zahlen als Punkte auf einer Zahlengeraden. In dieser Axiomengruppe wird auch definiert, wie die Operationen der Addition und Multiplikation mit der Ordnungsstruktur in Verbindung stehen.

Die Ordnung der reellen Zahlen kann dadurch beschrieben werden, dass wir alle positiven Zahlen kennen. Wenn $x$ eine positive Zahl ist, schreiben wir $0<x$. Die Positivität der reellen Zahlen wird dabei über die Anordnungsaxiome definiert:

\begin{definition*}[Anordnungsaxiome]
Die Anordnungsaxiome lauten

\begin{itemize}
\item \emph{Trichotomie der Positivität:} Für alle reellen Zahlen $x$ gilt entweder $0<x$ oder $x=0$ oder $0<-x$. Mit den Abkürzungen {''}$\forall ${''} für {''}für alle{''} und {''}${\dot {\lor }}${''} für {''}entweder oder{''} können wir dies schreiben als

\begin{align*}
\forall x\in \mathbb {R} :0<x\ {\dot {\lor }}\ 0=x\ {\dot {\lor }}\ 0<-x
\end{align*}


\item \emph{Abgeschlossenheit bezüglich Addition:} Für alle reellen Zahlen $a$ und $b$ gilt: Wenn $0<a$ und $0<b$ ist, dann ist auch $0<a+b$. In Zeichen:

\begin{align*}
\forall a,b\in \mathbb {R} :0<a\land 0<b\implies 0<a+b
\end{align*}


\item \emph{Abgeschlossenheit bezüglich Multiplikation:} Für alle reellen Zahlen $a$ und $b$ gilt: Wenn $0<a$ und $0<b$ ist, dann ist auch $0<ab$. In Zeichen:

\begin{align*}
\forall a,b\in \mathbb {R} :0<a\land 0<b\implies 0<ab
\end{align*}


\end{itemize}

\end{definition*}

Mit Hilfe der Positivitätseigenschaft $0<x$ können wir die Kleiner-Relation definieren:

\begin{definition*}[Kleiner-Relation]
Die Kleiner-Relation $x<y$ ist durch folgende Äquivalenz definiert:

\begin{align*}
x<y:\iff 0<y-x
\end{align*}

\end{definition*}

Es ist also genau dann $x$ kleiner als $y$, wenn die Differenz $y-x$ positiv ist. Über die Kleiner-Relation können wir alle weiteren Ordnungsrelationen definieren:

\begin{definition*}[Weitere Ordnungsrelationen auf Grundlage der Kleiner-Relation]
Für die reellen Zahlen sind außerdem die Relationen $\geq $, $\leq $ und $>$ über folgende Äquivalenzen definiert:

\begin{itemize}
\item $x\leq y:\iff x<y\lor x=y$
\item $x\geq y:\iff y<x\lor x=y$
\item $x>y:\iff y<x$
\end{itemize}

\end{definition*}

\section{Das Vollständigkeitsaxiom}

Das Vollständigkeitsaxiom beschreibt den Übergang beziehungsweise den markanten Unterschied zwischen den rationalen und den reellen Zahlen. Während die obigen beiden Axiomengruppen noch durch die Menge der rationalen Zahlen erfüllt werden, gilt dies nicht mehr für das Vollständigkeitsaxiom. Der Grund dafür ist, dass es im Zahlenbereich der rationalen Zahlen „Lücken“ wie ${\sqrt {2}}$ gibt. Diese Lücken können zwar beliebig durch rationale Zahlen approximiert werden, sind aber selbst keine rationalen Zahlen mehr. Bei den reellen Zahlen gibt es solche Lücken nicht, weil das Vollständigkeitsaxiom die Existenz von Lücken ausschließt. Wenn man irgendetwas beliebig durch reelle Zahlen annähern kann, so existiert dieses „irgendetwas“ und ist wieder eine reelle Zahl.

Eine Approximation einer Zahl kann durch eine Intervallschachtelung realisiert werden. Diese ist eine Folge von Intervallen, die ineinander liegen und deren Länge gegen Null streben:

\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Illustration_nested_intervals.svg}{\textbf{Illustration\allowbreak\_nested\allowbreak\_intervals.svg}} by Stephan Kulla \textit{(CC0)}}\begin{center}
\adjincludegraphics[max width=0.5\textwidth,max height=0.2\textheight]{file58illustration95nested95intervals95c02de4ead4a750548a081970f235ff0d674c74b9}\end{center}

Eine Intervallschachtelung dient als Approximation einer reellen Zahl. Jedes Intervall schränkt den Bereich ein, in dem die zu approximierende Zahl liegt und im Laufe der Intervallschachtelung wird dieser Bereich immer kleiner. Das Intervallschachtelungsprinzip garantiert, dass durch jede Intervallschachtelung mindestens eine Zahl approximiert wird:

\begin{definition*}[Allgemeines Intervallschachtelungsprinzip]
Zu jeder allgemeinen Intervallschachtelung $I_{1}$, $I_{2}$, $I_{3}$... existiert eine reelle Zahl, die in allen Intervallen liegt und damit von allen Intervallen approximiert wird.

\end{definition*}

Dieses Vollständigkeitsaxiom beschreibt, dass die Menge der reellen Zahlen keine „Lücken“ besitzt. Um zu beschreiben, dass die reellen Zahlen die kleinstmögliche Erweiterung sind, um die Lücken der rationalen Zahlen zu füllen, müssen wir das Intervallschachtelungsprinzip um ein weiteres Axiom ergänzen. Hierzu müssen wir ausschließen, dass es keine unendlich kleinen bzw. unendlich großen Zahlen gibt. Eine positive Zahl $y$ wäre im Vergleich zu einer positiven Zahl $x$ unendlich groß, wenn $y$ größer als alle Vielfachen von $x$ wäre, wenn also keine der Vielfachen $x,2x,3x,4x,\ldots $ jemals über $y$ hinauswächst. Für \emph{alle} natürlichen Zahlen $n$ wäre also $nx<y$. Dies wollen wir nun ausschließen. Für je zwei positive Zahlen $x$ und $y$ soll es also \emph{mindestens eine} natürliche Zahl $n$ mit $nx\geq y$ geben. Genau diese Eigenschaft beschreibt das archimedische Axiom:

\begin{definition*}[Das Archimedische Axiom]
Für alle reellen Zahlen $x,y>0$ gibt es eine natürliche Zahl $n$, so dass $nx\geq y$ ist. Mit den Abkürzungen {''}$\forall ${''} für {''}für alle{''} und {''}$\exists ${''} für {''}es gibt{''} liest sich dies als

\begin{align*}
\forall x>0,y>0\,\exists n\in \mathbb {N} :nx\geq y
\end{align*}

\end{definition*}

Mit Hilfe des Vollständigkeitsaxioms kann man zeigen, dass reelle Zahlen beliebig durch rationale Zahlen angenähert werden können. Diese Eigenschaft ist wesentlich, denn sie ermöglicht das Rechnen mit rationalen anstelle von reellen Zahlen. Beispielsweise werden Computerberechnungen in der Regel nur mit rationalen Zahlen durchgeführt. Solche Rechnungen sind zwar fehleranfällig, ihre Fehler können aber in der Regel beliebig klein gemacht werden.

\part{Supremum und Infimum}

\addxcontentsline{lof}{part}[\arabic{part}]{Supremum und Infimum}\begin{authors}
Stephan Kulla, Paul Stapor, Fabiangabel, Autorenkollektiv „Auswahlaxiom“ (Charlotte Dietze, Matthias Paulsen, Anne Reif), Christoph Kehle, Paolo Martinoni, Daniel5Ko, Who2010, Menuja J. (MJ Studies), Tratormo, Agnes Pauer, PhilippHanemann, Juetho, 0-Brane, Letsluk, Jenny Kilian, Meitnerium266, Lonza\textasciitilde{}dewikibooks\end{authors}

\chapter{Supremum und Infimum}

\section{Einleitung}

\emph{Supremum} (aus dem Lateinischen von \href{https://de.wiktionary.org/wiki/supremum}
{„supremum“} = „das Höchste/das Oberste“) klingt, als ob es „das Maximum“ (also das größte Element der Menge) wäre. Im Laufe dieses Artikels werden wir allerdings sehen, dass das Supremum das Maximum verallgemeinert. Merken wir uns zu Beginn:

\begin{importantparagraph*}
Jedes Maximum ist ein Supremum, aber nicht jedes Supremum ist ein Maximum.

\end{importantparagraph*}

Während nämlich das \emph{Maximum} ein Element der betrachteten Menge sein muss, muss das nicht für das Supremum gelten. Deshalb sollten wir „Supremum“ treffender mit „die unmittelbar nach oben beschränkende Zahl“ übersetzen. Es ist „nach oben beschränkend“, weil es wie das Maximum \emph{größer oder gleich} jeder Zahl der Menge ist. Und es ist „unmittelbar“, weil es \emph{die kleinste} aller „nach oben beschränkenden“ Zahlen ist.

Analog ist das \emph{Infimum} eine Verallgemeinerung des Minimums. Es ist die „unmittelbar nach unten beschränkende Zahl“, also die größte aller „nach unten beschränkenden“ Zahlen einer Menge. Konkrete Beispiele werden wir in den kommenden Abschnitten kennenlernen.

Für uns ist der Begriff des Supremums wichtig, weil mit ihm die Vollständigkeit der reellen Zahlen alternativ beschrieben werden kann. Außerdem ist das Supremum ein nützliches Hilfsmittel in Beweisen oder zur Definition neuer Begriffe.

\section{Erklärung des Supremums}

\begin{figure}[h]
\vspace{\baselineskip}
\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Eine_Menge_mit_oberen_und_unteren_Schranken.svg}{\textbf{Eine\allowbreak\_Menge\allowbreak\_mit\allowbreak\_oberen\allowbreak\_und\allowbreak\_unteren\allowbreak\_Schranken.svg}} by Stephan Kulla \textit{(CC BY-SA 4.0)}}\centering
\adjincludegraphics[max width=.5\textwidth, max height=0.2\textheight]{file58eine95menge95mit95oberen95und95unteren95schranken9592566cfbc3ac266b6468a6eb438d8b40354fc8f0}
\caption*{Eine Menge mit eingezeichneten oberen und unteren Schranken (\arabic{imagelabel})}
\end{figure}
Um das Supremum zu erklären, werden wir untersuchen, wie man zu dessen genauer Definition kommt. Hierzu werden wir feststellen, wie das Supremum aus dem Maximum verallgemeinert werden kann. Zur Erinnerung: Das Maximum einer Menge ist ihr größtes Element. Das Maximum $m$ einer Menge $M$ hat also folgende Eigenschaften:

\begin{itemize}
\item $m$ ist Element von $M$.
\item Für jedes $y\in M$ ist $y\leq m$.
\end{itemize}

In der zweiten Eigenschaft steht deshalb ein Kleiner-Gleich- und kein Kleiner-Zeichen, weil in der Aussage auch $y$ gleich $m$ sein könnte. Bei endlichen Mengen ist das Maximum stets definiert, jedoch ist dies bei unendlichen Mengen nicht unbedingt der Fall.

Zunächst können wir auf das Problem stoßen, dass die betrachtete Menge nach oben unbeschränkt ist. Nimm zum Beispiel die Menge $\mathbb {R} ^{+}=\{x\in \mathbb {R} :x>0\}$. Diese Menge kann kein Maximum oder ähnliches besitzen, da es für jede reelle Zahl eine größere Zahl aus $\mathbb {R} ^{+}$ gibt. Diese Menge kann also kein größtes Element besitzen. Es gibt auch kein Element, das „unmittelbar das größte“ Element sein könnte. Demnach ist eine Frage danach bei dieser Menge schlicht nicht sinnvoll.

Für die Übertragung des Maximumbegriffs auf unendliche Mengen muss also die Menge nach oben beschränkt sein. Es muss also eine Zahl $b$ geben, welche größer gleich jedem Element der Menge ist. Dabei muss $b$ nicht zwangsläufig Element der Menge sein.

Doch auch dann kann es zu Problemen kommen. Nehmen wir zum Beispiel die Menge $M=\{x\in \mathbb {R} :x<1\}$. Diese Menge ist nach oben beschränkt, weil man für $b$ jede Zahl größer gleich $1$ wählen kann.

Hat die Menge $M$ ein Maximum? Leider nein. Für jedes $x\in M$ ist ${\tfrac {x+1}{2}}$ eine weitere Zahl aus $M$ mit der Eigenschaft $x<{\tfrac {x+1}{2}}$ (die Zahl ${\tfrac {x+1}{2}}$ liegt in der Mitte zwischen $x$ und $1$). So kann aber $M$ kein maximales Element besitzen, weil es zu jeder Zahl aus $M$ mindestens eine größere Zahl aus $M$ gibt.

Bei der Betrachtung unendlicher Mengen büßt das Maximum also eine Eigenschaft ein. Nämlich, dass es Element der Menge ist:

\begin{itemize}
\item \sout{m ist Element von M.}
\item Für jedes $y\in M$ ist $y\leq m$.
\end{itemize}

Es bleibt also erst einmal nur die Eigenschaft, dass die gesuchte Zahl größer als jedes Element der Menge ist. Eine solche Zahl wird „obere Schranke“ der Menge genannt:

\begin{definition*}[obere Schranke]
Sei $M$ eine Teilmenge von $\mathbb {R} $. Dann nennt man eine Zahl $u$, die größer gleich jedem Element von $M$ ist, eine obere Schranke. Es ist also $x\leq u$ für alle $x\in M$.

\end{definition*}

Analog ist eine untere Schranke eine Zahl, die eine Menge nach unten beschränkt:

\begin{definition*}[untere Schranke]
Sei $M$ eine Teilmenge von $\mathbb {R} $. Dann nennt man eine Zahl ${\tilde {u}}$, die kleiner gleich jedem Element von $M$ ist, eine untere Schranke. Es ist also $x\geq {\tilde {u}}$ für alle $x\in M$.

\end{definition*}

Wenn wir unsere neue Definition betrachten, stellen wir zwei Dinge fest. Erstens: Obere und untere Schranken müssen keine Elemente der betrachteten Menge sein, weil dies nicht von der Definition gefordert wird. Und zweitens: Die Definition sagt nichts über eine etwaige Eindeutigkeit der Schranken aus.

Betrachten wir zum Beispiel die Menge $M=\{x\in \mathbb {R} :x<1\}$. Hier fällt uns sicherlich zuerst $1$ als obere Schranke ein. Jedoch ist $17$ ebenfalls eine obere Schranke und erfüllt die Forderungen der Definition. Abgesehen davon, dass $17$ weit oberhalb unserer Beispielmenge liegt, sind beide Zahlen keine Elemente der Menge. Dieses Beispiel zeigt, dass es mehr als eine obere Schranke geben kann. Es wird aber noch beunruhigender: Eine beschränkte Teilmenge der reellen Zahlen hat immer \emph{unendlich viele} obere Schranken. Wenn $u$ eine obere Schranke von $M$ ist, so ist auch jede größere Zahl, also $u+a$ für alle $a>0$, eine obere Schranke.

Bei genauerer Betrachtung sind die Begriffe von oberer bzw. unterer Schranke nicht sehr treffend. Sie leisten viel weniger als ein Maximumbegriff. Das Maximum ist nämlich immer eindeutig: Es kann höchstens eins davon geben. Mit der oberen Schranke verhält es sich nicht so. Deshalb wollen wir versuchen, den Begriff zu verbessern.

Betrachten wir als Beispiel wieder die Menge $M=\{x\in \mathbb {R} :x<1\}$. Welche Zahl könnte man als Verallgemeinerung des Maximums für $M$ wählen? Intuitiv fällt uns die Zahl $1$ ein. Doch warum sollte man diese Zahl wählen?

Wir wollen einen allgemein gültigen Begriff, der auch dann funktioniert, wenn die Menge nicht mehr so anschaulich ist. Deswegen kommen zunächst alle oberen Schranken von $M$, also alle Zahlen größer gleich $1$, in Frage. Nun sollte unsere Zahl optimal in dem Sinne sein, dass sie möglichst klein ist. So kommen wir auf die Zahl $1$. Sie ist nicht nur eine obere Schranke, sie ist auch \emph{die kleinste} obere Schranke von $M$. Wir haben ja bereits gesehen, dass es für jedes $x<1$ eine andere Zahl $y<1$ mit $x<y$ gibt (nämlich $y={\tfrac {x+1}{2}}$). Damit kann keine Zahl kleiner $1$ eine obere Schranke von $M$ sein. $1$ ist also das, was wir als „unmittelbar darüberliegende“ Zahl von $\{x\in \mathbb {R} :x<1\}$ ansehen.

Die kleinste obere Schranke $s$ wird durch folgende zwei Eigenschaften charakterisiert:

\begin{itemize}
\item $s$ ist obere Schranke von $M$: Für jedes $y\in M$ ist $y\leq s$.
\item Jede obere Schranke $u$ von $M$ ist mindestens so groß wie $s$: Gilt $y\leq u$ für alle $y\in M$, so gilt auch $s\leq u$. Anders formuliert: Für jedes $u<s$ gibt es mindestens eine Zahl $y\in M$ mit $u<y$.
\end{itemize}

Das können wir als Definition des Supremums verwenden, da es offenbar die kleinste obere Schranke charakterisiert. Das Infimum wird analog als die größte untere Schranke definiert. 

\section{Definition des Supremums und Infimums}

\begin{figure}[h]
\vspace{\baselineskip}
\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Veranschaulichung_zum_Begriff_Supremum.svg}{\textbf{Veranschaulichung\allowbreak\_zum\allowbreak\_Begriff\allowbreak\_Supremum.svg}} by Stephan Kulla \textit{(CC BY-SA 4.0)}}\centering
\adjincludegraphics[max width=.5\textwidth, max height=0.2\textheight]{file58veranschaulichung95zum95begriff95supremum95b1a24a41557aa0ece5b981395141eee1020a9064}
\caption*{Das Supremum ist die kleinste obere Schranke einer Menge. (\arabic{imagelabel})}
\end{figure}
Die Definition des Supremums und des Infimums lautet:

\begin{definition*}[Supremum]
Sei $M$ eine Teilmenge von $\mathbb {R} $. Das Supremum $s$ einer Menge $M$ ist die kleinste obere Schranke von $M$. Das Supremum wird charakterisiert über die beiden Eigenschaften:

\begin{itemize}
\item Für jedes $y\in M$ ist $y\leq s$.
\item Keine Zahl $x$ kleiner als $s$ ist obere Schranke von $M$: Für alle $x<s$ gibt es mindestens eine Zahl $y\in M$ mit $x<y$.
\end{itemize}

\end{definition*}

\begin{definition*}[Infimum]
Sei $M$ eine Teilmenge von $\mathbb {R} $. Das Infimum ${\tilde {s}}$ einer Menge $M$ ist die größte untere Schranke von $M$. Das Infimum wird charakterisiert über die beiden Eigenschaften:

\begin{itemize}
\item Für jedes $y\in M$ ist $y\geq {\tilde {s}}$.
\item Keine Zahl $x$ größer als ${\tilde {s}}$ ist untere Schranke von $M$: Für alle $x>{\tilde {s}}$ gibt es mindestens eine Zahl $y\in M$ mit $x>y$.
\end{itemize}

\end{definition*}

\subsection{Die Epsilon-Definition}

In der zweiten Eigenschaft der Definition des Supremums $s$ als Teil einer Menge $M$ steht:

\begin{importantparagraph*}
„Jede Zahl $x$ kleiner als $s$ ist keine obere Schranke von $M$: Für alle $x<s$ gibt es mindestens eine Zahl $y\in M$ mit $x<y$.“

\end{importantparagraph*}

Hier ist es in einigen Lehrbüchern auch üblich, $x=s-\epsilon $ mit $\epsilon >0$ zu setzen. Dadurch erhält man folgende Aussage, die man auch als zweite Eigenschaft des Supremums nutzen kann:

\begin{importantparagraph*}
„Für alle $\epsilon >0$ gibt es ein $y\in M$ mit $s-\epsilon <y$.“

\end{importantparagraph*}

Bei Beweisen dürfen wir frei entscheiden, welche der beiden Aussagen wir heranziehen wollen. Da beide Aussagen zueinander äquivalent sind, ist es egal, welche bewiesen werden.

\begin{mdframed}[style=semanticbox,frametitleaboveskip=3pt,innerbottommargin=3pt,frametitle=Frage]
Wie lautet die Epsilon-Definition des Infimums?

\end{mdframed}

\begin{answer*}
${\tilde {s}}$ ist ein Infimum von $M$, wenn ${\tilde {s}}$ eine untere Schranke von $M$ ist und wenn es für alle $\epsilon >0$ ein $y\in M$ gibt, so dass ${\tilde {s}}+\epsilon >y$ ist.

\end{answer*}

\subsection{Maximum und Minimum}

Für das Maximum und Minimum haben wir bekanntlich folgende Definitionen:

\begin{definition*}[Maximum]
Das Maximum $m$ einer Menge $M$ ist eine Zahl mit den folgenden zwei Eigenschaften:

\begin{itemize}
\item $m\in M$.
\item Für alle $y\in M$ ist $y\leq m$.
\end{itemize}

\end{definition*}

\begin{definition*}[Minimum]
Das Minimum ${\tilde {m}}$ einer Menge $M$ ist eine Zahl mit den folgenden zwei Eigenschaften:

\begin{itemize}
\item ${\tilde {m}}\in M$.
\item Für alle $y\in M$ ist $y\geq {\tilde {m}}$.
\end{itemize}

\end{definition*}

Das Maximum ist stets Supremum der Menge. Sei nämlich $m$ Maximum einer Menge $M$. Zum einen ist $m$ per Definition obere Schranke von $M$. Zum anderen gibt es für alle $x$ mit $x<m$ ein $y\in M$ mit $x<y$, nämlich $y=m$. Umgekehrt ist nicht jedes Supremum Maximum, wie wir oben an der Menge $\{x\in \mathbb {R} :x<1\}$ gesehen haben. Die Zahl $1$ ist zwar Supremum dieser Menge, aber kein Maximum. Analoges gilt für Minimum und Infimum.

\subsection{Schreibweisen}


\renewcommand{\arraystretch}{1.5}

\begin{longtabu} to \linewidth {X[l]X[l]} \\ \toprule 
Schreibweise & Bedeutung \\ 
\midrule
$\sup M$ & Supremum von $M$ \\ 
$\sup _{x\in D}f(x)$ & Supremum von $\{f(x):x\in D\}$ \\ 
$\inf M$ & Infimum von $M$ \\ 
$\inf _{x\in D}f(x)$ & Infimum von $\{f(x):x\in D\}$ \\ 
$\max M$ & Maximum von $M$ \\ 
$\min M$ & Minimum von $M$ \\ 
\bottomrule
\end{longtabu}
\renewcommand{\arraystretch}{1.0}
\subsection{Das Dualitätsprinzip}

Wir haben bereits in den Definitionen und in der obigen Erklärung gesehen, dass die Begriffe des Supremums und des Infimums analog zueinander betrachtet werden können. Der Grund liegt darin, dass bei Umkehrung der Ordnung auf den reellen Zahlen das Supremum zum Infimum wird und umgekehrt. Wir können nämlich eine neue Ordnung $\leq _{\text{neu}}$ dadurch einführen, dass $x\leq _{\text{neu}}y$ genau dann ist, wenn $x\geq y$ ist (wir spiegeln hier die reelle Zahlengerade an der Null). Bei dieser neuen Ordnung verhält sich das ursprüngliche Supremum wie ein Infimum und umgekehrt. Beide Ordnungen $\leq _{\text{neu}}$ und $\leq $ haben dieselben ordnungstheoretischen Eigenschaften. Sie sind daher isomorph zueinander. Deshalb müssen auch die Eigenschaften von Supremum und Infimum bei umgekehrter Ordnung dieselben sein. Alles was wir in Zukunft für Suprema sagen, gilt in ähnlicher Weise auch für Infima und umgekehrt. Das Gleiche gilt folglich auch für Maximum und Minimum.

\begin{example*}[Dualitätsprinzip]
Für alle $x\in M$ ist $x\leq \sup M$. Analog ist für alle $x\in M$ die Ungleichung $x\geq \inf M$ erfüllt.

\end{example*}

\subsection{Existenz und Eindeutigkeit}

Wir haben bisher ganz selbstverständlich von \emph{dem} Supremum gesprochen. Das klingt so, als ob es immer eines gäbe und als ob es immer eindeutig wäre. Der Verdacht liegt auch nahe: Wozu sollten wir uns die Mühe machen, den Begriff „Supremum“ überhaupt zu definieren, wenn er das Grundproblem des Maximums (nämlich oftmals gar nicht zu existieren) gar nicht lösen könnte? Was wäre der Vorteil des Supremums gegenüber dem Begriff der „oberen Schranke“, wenn auch das Supremum nicht eindeutig wäre? Intuitiv ist irgendwie klar, dass es unter allen oberen Schranken \emph{genau eine} kleinste geben muss, aber bis jetzt haben wir das noch nicht streng mathematisch bewiesen.

Im folgenden Satz werden wir die Eindeutigkeit des Infimums und Supremums beweisen, also dass eine Menge höchstens ein Supremum und Infimum besitzen kann:

\begin{theorem*}[Eindeutigkeit des Supremums und Infimums]
Eine Menge kann höchstens ein Supremum und höchstens ein Infimum besitzen.

\end{theorem*}

\begin{proof*}[Eindeutigkeit des Supremums und Infimums]
Wir können die Standardbeweismethode für Eindeutigkeit nutzen: Zunächst nehmen wir eine Menge $M$ an, die zwei Suprema $s_{1}$ und $s_{2}$ besitzt, und zeigen dann, dass $s_{1}=s_{2}$ ist. Die beiden Suprema haben folgende Eigenschaften:

\begin{itemize}
\item $s_{1}$ und $s_{2}$ sind obere Schranken von $M$.
\item Keine Zahl kleiner als $s_{1}$ und $s_{2}$ ist eine obere Schranke von $M$.
\end{itemize}

Keine Zahl kleiner als $s_{1}$ ist obere Schranke von $M$. Da $s_{2}$ eine obere Schranke von $M$ ist, kann $s_{2}$ nicht kleiner als $s_{1}$ sein und muss damit größer gleich $s_{1}$ sein. Analog ist $s_{1}\geq s_{2}$. Aus $s_{2}\geq s_{1}$ und $s_{1}\geq s_{2}$ folgt $s_{1}=s_{2}$. Der Beweis für die Eindeutigkeit des Infimums ist analog.

\end{proof*}

Mit dem Vollständigkeitsaxiom kann auch die Existenz des Supremums einer nach oben beschränkten nicht-leeren Teilmenge der reellen Zahlen bewiesen werden. Dies werden wir in diesem Kapitel jedoch nicht behandeln. Analog besitzt eine nach unten beschränkte nicht-leere Teilmenge der reellen Zahlen stets ein Infimum. Somit ist es tatsächlich so, dass Supremum und Infimum einer nach oben beschränkten und nicht-leeren Teilmenge der reellen Zahlen immer existieren und immer eindeutig sind. Deswegen dürfen wir beruhigt von \emph{dem} Supremum sprechen.

\chapter{Uneigentliches Supremum und Infimum}

Damit eine Menge ein Supremum besitzen kann, muss sie nach oben beschränkt sein. In diesem Kapitel untersuchen wir den Fall unbeschränkter Mengen bzw. den Fall der leeren Menge.

\section{Uneigentliche Suprema und Infima für unbeschränkte Mengen}

Eine Menge $M$ ist nach oben unbeschränkt, wenn $M$ keine obere Schranke besitzt. Für alle $S\in \mathbb {R} $ gibt es also ein $x\in M$ mit $x>S$. Dies ist dann auch die Definition der Unbeschränktheit nach oben:

\begin{definition*}[nach oben unbeschränkte Menge]
Eine Menge $M$ ist nach oben unbeschränkt, wenn sie keine obere Schranke besitzt, wenn also

\begin{align*}
\forall S\in \mathbb {R} \,\exists x\in M:S<x
\end{align*}

\end{definition*}

Wenn $M$ nach oben unbeschränkt ist, schreiben wir nun

\begin{align*}
\sup M=\infty 
\end{align*}

Intuitiv lässt sich die Schreibweise gut erklären: „unendlich“ ist größer als jedes Element aus $M$ und gleichzeitig kann es keine obere Schranke kleiner „unendlich“ geben, weil $M$ nach oben unbeschränkt ist. Also macht es Sinn, „unendlich“ als Supremum einer nach oben unbeschränkten Menge anzusehen.

\emph{Aber Vorsicht!} Das Symbol $\infty $ ist keine reelle Zahl und damit bedeutet $\sup M=\infty $ auch nicht, dass $\infty $ Supremum von $M$ wäre, weil Suprema per Definition immer reell sein müssen. Es gibt auch kein Objekt $\infty $ in unserer Theorie, weil die von uns in den ersten Kapiteln formulierten Axiome kein Objekt $\infty $ zulassen. Deshalb müsste eine Schreibweise wie $\sup M=\infty $ von uns abgelehnt werden.

Um diese Widersprüche aufzulösen, sehen wir $\sup M=\infty $ nur als Kurzschreibweise für den Fakt an, dass $M$ nach oben unbeschränkt ist, und nennen $\infty $ das \emph{uneigentliche Supremum} von $M$:

\begin{definition*}[uneigentliches Supremum]
Ist eine Menge $M$ nach oben unbeschränkt, so nennen wir $\infty $ das \emph{uneigentliche Supremum} von $M$ und schreiben

\begin{align*}
\sup M=\infty 
\end{align*}

\end{definition*}

\begin{warning*}
Das Adjektiv „uneigentlich“ ist hier sehr wichtig. Achte darauf, dass du es immer verwendest. $\infty $ ist nämlich keine reelle Zahl und kann deswegen kein Supremum sein. Es verhält sich nur in mancher Hinsicht wie ein Supremum. Kurz: Auch wenn man $\sup M=\infty $ schreibt, dann besitzt $M$ trotzdem kein Supremum!

\end{warning*}

Analog gilt für nach unten unbeschränkte Mengen:

\begin{definition*}[uneigentliches Infimum]
Eine Menge $M$ ist nach unten unbeschränkt, wenn es für alle $S\in \mathbb {R} $ ein $x\in M$ mit $x<S$ gibt. In diesem Fall schreibt man

\begin{align*}
\inf M=-\infty 
\end{align*}

\end{definition*}

\section{Uneigentliches Supremum und Infimum der leeren Menge}

Ein weiterer Sonderfall ist die leere Menge. Hier ist nämlich nicht das Problem, dass es keine oberen beziehungsweise unteren Schranken gibt, sondern zu viele obere und untere Schranken existieren. In den Lehrbüchern findest du dafür folgende Definitionen:

\begin{definition*}[Uneigentliches Supremum und Infimum der leeren Menge]
Für die leere Menge $\emptyset $ gilt

\begin{align*}
\sup \emptyset &=-\infty \\\inf \emptyset &=\infty 
\end{align*}

\end{definition*}

Auch hier handelt es sich um uneigentliche und damit um \emph{keine echten} Suprema und Infima. Doch wieso macht obige Festlegung Sinn?

Gehen wir schrittweise vor: Per Definition ist das Supremum die kleinste obere Schranke einer Menge. Was sind also die oberen Schranken der leeren Menge? Eine Zahl $S$ ist per Definition eine obere Schranke von $\emptyset $, wenn

\begin{align*}
\forall x\in \emptyset :x\leq S
\end{align*}

\begin{mdframed}[style=semanticbox,frametitleaboveskip=3pt,innerbottommargin=3pt,frametitle=Frage]
Was sind die oberen Schranken von $\emptyset $?

\end{mdframed}

\begin{answer*}
Allaussagen über die leere Menge wie die obige sind immer wahr (es gibt nämlich kein $x$ in $\emptyset $, für welches man die Bedingung $x\leq S$ überprüfen müsste). Damit ist jede reelle Zahl eine obere Schranke der leeren Menge. Als Bezeichnung für die kleinste all dieser oberen Schranken von $\emptyset $ kann man also $-\infty $ verwenden. Jedoch ist $-\infty $ keine reelle Zahl und daher auch kein Supremum im eigentlichen Sinne.

\end{answer*}

\chapter{Supremum und Infimum bestimmen und beweisen}

\section{Allgemeine Vorgehensweise}

Um das Supremum oder Infimum einer Menge zu finden, kannst du folgendermaßen vorgehen:

\begin{enumerate}
\item \emph{Menge veranschaulichen:} Überlege dir, wie die Menge aussieht. Hierzu kannst du Skizzen anfertigen oder ggf. auch Computerprogramme verwenden.
\item \emph{Hypothese über Supremum und Infimum anstellen:} Ist die Menge nach oben beschränkt? Wenn ja, dann überlege dir, welche Zahl das Supremum sein kann. Wenn nein, dann besitzt die Menge kein Supremum. Analog schaue, ob die Menge nach unten beschränkt ist oder nicht, und überlege dir gegebenenfalls, welche Zahl das Infimum sein könnte.
\item \emph{Beweise für Supremum und Infimum finden:} Überlege dir auf einem Schmierblatt den Beweis dafür, dass die gefundene Zahl ein Supremum oder ein Infimum ist. Die notwendige Beweisstruktur findest du im nächsten Abschnitt.
\item \emph{Beweis ins Reine schreiben:} Zum Schluss musst du den Beweis aufschreiben. Dabei kannst du dich an der im nächsten Abschnitt folgenden Beweisstruktur für Supremum und Infimum orientieren.
\end{enumerate}

\section{Allgemeine Beweisstrukturen}

Die hier aufgelisteten Beweisstrukturen sollten dir helfen, deine Beweise richtig und sauber aufzuschreiben. Sie zeigen dir aber auch, worauf du in der Beweisfindung achten musst.

\subsection{Supremum: Beweisstruktur}

Um zu zeigen, dass eine Zahl $s$ Supremum einer Menge $M$ ist, kannst du folgendermaßen vorgehen:

\begin{enumerate}
\item \emph{Beweise, dass $s$ eine obere Schranke von $M$ ist:} Zeige hierzu, dass $y\leq s$ für alle $y\in M$ ist.
\item \emph{Beweise, dass keine Zahl $x<s$ obere Schranke von $M$ ist:} Nimm hierzu ein beliebiges $x<s$ und zeige, dass es ein $y\in M$ gibt mit $y>x$.
\end{enumerate}

\subsection{Infimum: Beweisstruktur}

Beweise, dass ${\tilde {s}}$ Infimum einer Menge $M$ ist, können so aussehen:

\begin{enumerate}
\item \emph{Beweise, dass ${\tilde {s}}$ eine untere Schranke von $M$ ist:} Zeige hierzu, dass $y\geq {\tilde {s}}$ für alle $y\in M$ ist.
\item \emph{Beweise, dass keine Zahl $x>{\tilde {s}}$ untere Schranke von $M$ ist:} Nimm hierzu ein beliebiges $x>{\tilde {s}}$ und zeige, dass es ein $y\in M$ gibt mit $y<x$.
\end{enumerate}

\subsection{Maximum: Beweisstruktur}

Hier kann man direkt der Definition des Maximums folgen:

\begin{enumerate}
\item \emph{Beweise, dass $m$ eine obere Schranke von $M$ ist:} Zeige hierzu, dass $y\leq m$ für alle $y\in M$ ist.
\item \emph{Zeige, dass $m\in M$ ist.}
\end{enumerate}

\subsection{Minimum: Beweisstruktur}

Um zu zeigen, dass ${\tilde {m}}$ Minimum der Menge $M$ ist, kann man analog zum Maximum vorgehen:

\begin{enumerate}
\item \emph{Beweise, dass ${\tilde {m}}$ eine untere Schranke von $M$ ist:} Zeige hierzu, dass $y\geq {\tilde {m}}$ für alle $y\in M$ ist.
\item \emph{Zeige, dass ${\tilde {m}}\in M$ ist.}
\end{enumerate}

\section{Beispielaufgaben für Supremum und Infimum}

\subsection{Menge von Folgengliedern}

\begin{figure}[h]
\vspace{\baselineskip}
\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Set_5_+_2_over_n_for_n_in_N.svg}{\textbf{Set\allowbreak\_5\allowbreak\_+\allowbreak\_2\allowbreak\_over\allowbreak\_n\allowbreak\_for\allowbreak\_n\allowbreak\_in\allowbreak\_N.svg}} by Stephan Kulla \textit{(CC0)}}\centering
\adjincludegraphics[max width=.5\textwidth, max height=0.2\textheight]{file58set955954395295over95n95for95n95in95n95bbf13a49c1f9392ebd692c1f5122a28d3f756eda}
\caption*{Die Menge $M=\left\{5+{\tfrac {2}{n}}:n\in \mathbb {N} \right\}$. (\arabic{imagelabel})}
\end{figure}
Wir werden nun folgende Aufgabe beweisen:

\begin{exercise*}[Menge von Folgengliedern]
Bestimme das Supremum und das Infimum der Menge $M=\left\{5+{\tfrac {2}{n}}:n\in \mathbb {N} \right\}$. Handelt es sich bei dem Supremum um ein Maximum und beim Infimum um ein Minimum? Beweise deine Behauptungen!

\end{exercise*}

\begin{solutionprocess*}[Menge von Folgengliedern]
Wir gehen nun schrittweise nach dem obigen Beweisverfahren vor:

\proofstep{Schritt 1:}
 Veranschauliche die Menge $M$.\begin{indentblock}
Die ersten Elemente der Menge $M$ lauten:


\renewcommand{\arraystretch}{1.5}

\begin{longtabu} to \linewidth {X[l]X[l]X[l]} \\ \toprule 
$n$ & $5+{\tfrac {2}{n}}$ & Folgerung \\ 
\midrule
$1$ & $5+{\tfrac {2}{1}}=7$ & $7\in M$ \\ 
$2$ & $5+{\tfrac {2}{2}}=6$ & $6\in M$ \\ 
$3$ & $5+{\tfrac {2}{3}}=5{\tfrac {2}{3}}$ & $5{\tfrac {2}{3}}\in M$ \\ 
$4$ & $5+{\tfrac {2}{4}}=5{\tfrac {1}{2}}$ & $5{\tfrac {1}{2}}\in M$ \\ 
$\vdots $ & $\vdots $ & $\vdots $ \\ 
\bottomrule
\end{longtabu}
\renewcommand{\arraystretch}{1.0}
Die Menge $M$ hat also die Gestalt $M=\left\{7,6,5{\tfrac {2}{3}},5{\tfrac {1}{2}},5{\tfrac {2}{5}},\dots \right\}$, wobei sich die fehlenden Elemente immer mehr der $5$ annähern.

\end{indentblock}

\proofstep{Schritt 2:}
 Stelle eine Hypothese an, welche Zahlen Supremum bzw. Infimum der Menge sind.\begin{indentblock}
Wir sehen, dass die Menge nach oben durch $7$ beschränkt ist. Gleichzeitig ist $7$ ein Element der Menge, womit $7$ Maximum der Menge sein muss. Außerdem ist die Menge nach unten durch $5$ beschränkt. Da sich die Elemente der Menge immer mehr der $5$ annähern, kann es keine untere Schranke größer als $5$ geben. Es folgt, dass $5$ wahrscheinlich das Infimum der Menge ist. Beachte, dass wir hier nur Vermutungen anstellen, weil wir intuitiv argumentieren. Es fehlt noch der handfeste Beweis.

\end{indentblock}

\proofstep{Schritt 3:}
 Finde einen Beweis für das Supremum / Maximum.\begin{indentblock}
Wir haben bereits festgestellt, dass $7$ wahrscheinlich das Maximum der Menge ist. Wir müssen also zwei Dinge zeigen:

\begin{itemize}
\item $7\in M$
\item $7\geq x$ für alle $x\in M$
\end{itemize}

Wir haben bereits im ersten Schritt gesehen, dass $7$ Element von $M$ ist, denn für $n=1$ ist $5+{\tfrac {2}{n}}=7$. Um zu zeigen, dass $7$ eine obere Schranke von $M$ ist, müssen wir zeigen, dass $7\geq 5+{\tfrac {2}{n}}$. Stellen wir diese Ungleichung schrittweise um:

\begin{align*}
7&\geq 5+{\tfrac {2}{n}}\\2&\geq {\tfrac {2}{n}}\\2n&\geq 2\\n&\geq 1
\end{align*}

Nun ist $n\geq 1$ eine für natürliche Zahlen offensichtliche Aussage. Im Beweis müssen wir aber den umgekehrten Weg gehen: Da wir die Ungleichung $7\geq 5+{\tfrac {2}{n}}$ zeigen wollen, müssen wir bei $n\geq 1$ anfangen und diese Ungleichung schrittweise in $7\geq 5+{\tfrac {2}{n}}$ umformen. Dies können wir machen, weil wir oben nur Äquivalenzumformungen verwendet haben.

Im letzten Kapitel haben wir gesehen, dass jedes Maximum einer Menge automatisch auch das Supremum der Menge ist (nur umgekehrt ist es nicht immer der Fall). Daraus folgt, dass $7$ Supremum von $M$ ist.

\end{indentblock}

\proofstep{Schritt 4:}
 Finde einen Beweis für das Infimum / Minimum.\begin{indentblock}
Um zu zeigen, dass $5$ Infimum ist, müssen wir zeigen:

\begin{itemize}
\item $5\leq x$ für alle $x\in M$
\item Für alle $y>5$ gibt es ein $x\in M$ mit $x<y$
\end{itemize}

Um auch zu zeigen, dass $5$ kein Minimum ist, haben wir außerdem zu beweisen:

\begin{itemize}
\item $5\notin M$
\end{itemize}

Zunächst muss ein Beweis für $5\leq 5+{\tfrac {2}{n}}$ für alle $n\in \mathbb {N} $ gefunden werden:

\begin{align*}
5&\leq 5+{\tfrac {2}{n}}\\0&\leq {\tfrac {2}{n}}
\end{align*}

Nun ist $0\leq {\tfrac {2}{n}}$ eine offensichtlich wahre Aussage, da ${\tfrac {2}{n}}$ positiv ist. Im späteren Beweis können wir also aus $0\leq {\tfrac {2}{n}}$ die Ungleichung $5\leq 5+{\tfrac {2}{n}}$ beweisen, indem wir obige Umformung rückwärts durchführen (also zu beiden Seiten $5$ addieren).

Sei nun weiterhin $y>5$ beliebig. Wir müssen nun ein $x$ mit $x=5+{\tfrac {2}{N}}$ mit $N\in \mathbb {N} $ finden, so dass $y>x=5+{\tfrac {2}{N}}$ ist. Wir wählen hier die Variable $N$ und nicht $n$, weil wir \emph{ein konkretes} Element der Menge $M$ finden wollen (in der Mathematik wird oft $N$ verwendet, wenn man ein konkretes $n$ sucht). Formen wir diese Ungleichung nach $N$ um, um so ein passendes $N\in \mathbb {N} $ zu finden:

\begin{align*}
y&>5+{\tfrac {2}{N}}\\y-5&>{\tfrac {2}{N}}\\{\tfrac {y-5}{2}}&>{\tfrac {1}{N}}
\end{align*}

Wegen $y>5$ ist $y-5>0$, also auch ${\tfrac {y-5}{2}}>0$. Das archimedische Axiom garantiert uns nun, dass wir ein passendes $N$ finden, da nach dem archimedischen Axiom der Bruch ${\tfrac {1}{N}}$ kleiner wird als jede positive reelle Zahl.

Als Letztes fehlt noch die Beweisidee dafür, dass $5\notin M$ ist. Hier müssen wir zeigen, dass $5\neq 5+{\tfrac {2}{n}}$ für alle $n\in \mathbb {N} $ gilt. Doch wegen ${\tfrac {2}{n}}>0$ ist $5+{\tfrac {2}{n}}>5$ und somit $5\neq 5+{\tfrac {2}{n}}$.

\end{indentblock}

\end{solutionprocess*}

\begin{proof*}[Menge von Folgengliedern]
Es ist $7$ Maximum (und damit Supremum) der Menge $M$ und $5$ ist Infimum, aber kein Minimum der Menge $M$.

\proofstep{Beweisschritt:}
 $7$ ist Maximum der Menge $M$\begin{indentblock}
\proofstep{Beweisschritt:}
 $7$ ist Element der Menge $M$\begin{indentblock}
Für $n=1$ ist $5+{\tfrac {2}{n}}=7$. Damit ist $7\in M$.

\end{indentblock}

\proofstep{Beweisschritt:}
 $7$ ist eine obere Schranke der Menge $M$\begin{indentblock}
Für alle $n\in \mathbb {N} $ gilt

\begin{align*}
&&n&\geq 1\\&\Rightarrow &2n&\geq 2\\&\Rightarrow &2&\geq {\tfrac {2}{n}}\\&\Rightarrow &7&\geq 5+{\tfrac {2}{n}}
\end{align*}

Damit ist $7$ größer gleich jedem Element von $M$.

\end{indentblock}

\end{indentblock}

\proofstep{Beweisschritt:}
 $5$ ist Infimum der Menge $M$\begin{indentblock}
\proofstep{Beweisschritt:}
 $5$ ist untere Schranke der Menge $M$\begin{indentblock}
Für alle $n\in \mathbb {N} $ gilt

\begin{align*}
&&0&\leq {\tfrac {2}{n}}\\&\Rightarrow &5&\leq 5+{\tfrac {2}{n}}
\end{align*}

Damit ist $5$ kleiner gleich jedem Element von $M$.

\end{indentblock}

\proofstep{Beweisschritt:}
 Keine Zahl größer $5$ ist untere Schranke der Menge $M$\begin{indentblock}
Sei $y>5$ beliebig. Es ist damit ${\tfrac {y-5}{2}}>0$ und somit gibt es nach dem archimedischen Axiom ein $N\in \mathbb {N} $ mit ${\tfrac {y-5}{2}}>{\tfrac {1}{N}}$. Es ist

\begin{align*}
&&{\tfrac {y-5}{2}}&>{\tfrac {1}{N}}\\&\Rightarrow &y-5&>{\tfrac {2}{N}}\\&\Rightarrow &y&>5+{\tfrac {2}{N}}
\end{align*}

Weil $5+{\tfrac {2}{N}}\in M$ ist, gibt es damit ein Element aus $M$, welches kleiner als $y$ ist. Somit ist $y$ keine untere Schranke von $M$.

\end{indentblock}

\end{indentblock}

\proofstep{Beweisschritt:}
 $5$ ist kein Minimum von $M$\begin{indentblock}
Es ist ${\tfrac {2}{n}}>0$ und damit ${\tfrac {2}{n}}+5>5$. Somit ist $5$ kein Element und damit auch kein Minimum von $M$.

\end{indentblock}

\end{proof*}

\includepdf[pages=-]{predesigned_pages/mfnf_participation}

\chapter{Eigenschaften Supremum und Infimum}

Da das Supremum auf Mengen angewandt wird, ist eine sehr naheliegende Frage: Was passiert mit dem Supremum, wenn wir die Menge verändern? Wenn wir sie mit einer anderen Menge beispielsweise schneiden oder vereinigen, wenn wir sie größer oder kleiner machen? Hier werden wir einige Regeln kennen lernen, die dir helfen werden, mit dem Supremum zu arbeiten.

\section{Übersicht der Regeln zum Supremum und Infimum}

Wir definieren zuerst einige Kurzschreibweisen.

\begin{definition*}
Für alle Mengen $A,B\subseteq \mathbb {R} $ und alle $\lambda \in \mathbb {R} $ definieren wir:

\begin{itemize}
\item $-A:=\{-x:x\in A\}$
\item $\lambda A:=\{\lambda x:x\in A\}$
\item $A+B:=\{a+b:a\in A,b\in B\}$
\item $A\cdot B:=\{a\cdot b:a\in A,b\in B\}$
\end{itemize}

\end{definition*}

Für das Supremum und Infimum gelten folgende Regeln. Dabei ist $A,B,D\subseteq \mathbb {R} $ und $f,g:D\rightarrow \mathbb {R} $ sowie $\lambda \in \mathbb {R} $. Im Folgenden wird immer angenommen, dass das Supremum beziehungsweise das Infimum existiert.

\subsection{Regeln für das Supremum}

\begin{figure}[h]
\vspace{\baselineskip}
\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:SupFunctions.svg}{\textbf{SupFunctions.svg}} by Autorenkollektiv „Auswahlaxiom“ (Charlotte Dietze, Matthias Paulsen, Anne Reif) \textit{(CC BY-SA 4.0)}}\centering
\adjincludegraphics[max width=.5\textwidth, max height=0.2\textheight]{file58supfunctions9523656bfd9deedcb12d698f4db6f61226508bf440}
\caption*{Das Supremum der Summe zweier Funktionen kann kleiner als die Summe ihrer Suprema sein. (\arabic{imagelabel})}
\end{figure}
\begin{itemize}
\item $\sup A\geq \inf A$
\item $A\subseteq B\Rightarrow \sup A\leq \sup B$
\end{itemize}

\begin{itemize}
\item $\sup(A\cup B)=\max\{\sup A,\sup B\}$
\item $\sup(A\cap B)\leq \min\{\sup A,\sup B\}$
\item $\sup(-A)=\sup(\{-x:x\in A\})=-\inf(A)$
\item $\sup(\{x^{-1}:x\in A\})=(\inf(A))^{-1}$, falls $\inf(A)>0$ ist.
\item $\sup(\lambda A)=\sup(\{\lambda x:x\in A\})=\lambda \cdot \sup(A)$ für $\lambda \geq 0$
\item $\sup(A+B)=\sup(\{x+y:x\in A\land y\in B\})=\sup(A)+\sup(B)$
\item $\sup(A\cdot B)=\sup(\{x\cdot y:x\in A\land y\in B\})=\sup(A)\cdot \sup(B)$, falls $A$ und $B$ nur nichtnegative Elemente enthalten.
\item $\sup(f+g)(D)\leq \sup f(D)+\sup g(D)$
\item Es gibt eine Folge $(a_{n})_{n\in \mathbb {N} }$ aus $A$ mit $\lim _{n\rightarrow \infty }a_{n}=\sup A$.
\end{itemize}

\begin{mdframed}[style=semanticbox,frametitleaboveskip=3pt,innerbottommargin=3pt,frametitle=Frage]
Warum gilt nicht $\sup A\leq \sup B\Rightarrow A\subseteq B$? Finde ein Gegenbeispiel!

\end{mdframed}

\begin{answer*}
Ein Gegenbeispiel hierfür ist $A:=\{1\},B:=\{2\}$.

\end{answer*}

\begin{mdframed}[style=semanticbox,frametitleaboveskip=3pt,innerbottommargin=3pt,frametitle=Frage]
Warum gilt nicht $\sup(A\cap B)=\min\{\sup A,\sup B\}$? Finde ein Gegenbeispiel!

\end{mdframed}

\begin{answer*}
Sei $A=\{1,2\}$ und $B=\{1,3\}$. Dann gilt $A\cap B=\{1\}$ und also $\sup A\cap B=1$, aber $\sup A=2$ und $\sup B=3$, also $\min\{\sup A,\sup B\}=2$.

\end{answer*}

\begin{mdframed}[style=semanticbox,frametitleaboveskip=3pt,innerbottommargin=3pt,frametitle=Frage]
Warum gilt nicht $\sup(f+g)(D)=\sup f(D)+\sup g(D)$? Finde ein Gegenbeispiel!

\end{mdframed}

\begin{answer*}
Wir setzen $D:=[0,1]$. Als Funktionen wählen wir $f:D\to \mathbb {R} ,x\mapsto x$ und $g:D\to \mathbb {R} ,x\mapsto 1-x$. Also ist $(f+g):D\to \mathbb {R} ,x\mapsto 1$. Es gilt

\begin{align*}
\sup(f+g)(D)=1<1+1=\sup f(D)+\sup g(D)
\end{align*}

\end{answer*}

\subsection{Regeln für das Infimum}

\begin{itemize}
\item $\inf A\leq \sup A$
\item $A\subseteq B\Rightarrow \inf A\geq \inf B$
\item $\inf(A\cup B)=\min\{\inf A,\inf B\}$
\item $\inf(A\cap B)\geq \max\{\inf A,\inf B\}$
\item $\inf(-A)=\inf(\{-x:x\in A\})=-\sup(A)$
\item $\inf(\{x^{-1}:x\in A\})=(\sup(A))^{-1}$, falls $\sup(A)>0$ ist.
\item $\inf(\lambda A)=\inf(\{\lambda x:x\in A\})=\lambda \cdot \inf(A)$ für $\lambda \geq 0$
\item $\inf(A+B)=\inf(\{x+y:x\in A\land y\in B\})=\inf(A)+\inf(B)$
\item $\inf(A\cdot B)=\inf(\{x\cdot y:x\in A\land y\in B\})=\inf(A)\cdot \inf(B)$, falls $A$ und $B$ nur nichtnegative Elemente enthalten.
\item $\inf(f+g)(D)\geq \inf f(D)+\inf g(D)$
\item Es gibt eine Folge $(a_{n})_{n\in \mathbb {N} }$ aus $A$ mit $\lim _{n\rightarrow \infty }a_{n}=\inf A$.
\end{itemize}

\part{Folgen}

\addxcontentsline{lof}{part}[\arabic{part}]{Folgen}\begin{authors}
Autorenkollektiv „Auswahlaxiom“ (Charlotte Dietze, Matthias Paulsen, Anne Reif), Stephan Kulla, Who2010, Paolo Martinoni, Werner Fröhlich, Chris ShuYu Dong, Nico Benti, Juetho, 0-Brane, Matthias Greger, Akram Chawki, PhilippHanemann, Meitnerium266, Mitja, Pythagora, Jenny Kilian, Braun\textasciitilde{}dewikibooks, 4tilden\end{authors}

\chapter{Definition}

Die \emph{Folge} ist einer der wichtigsten Begriffe in der Analysis. Anhand von Folgen werden wir nämlich später den Begriff des Grenzwerts definieren. Damit wiederum können wir alle wichtigen Konzepte der Analysis wie die Ableitung und die Stetigkeit einführen.

\section{Der Begriff der Folge im Alltag}

Der Begriff der „Folge“ ist uns bereits aus dem alltäglichen Leben bekannt:

\begin{mdframed}[style=semanticbox,frametitleaboveskip=3pt,innerbottommargin=3pt,frametitle=Frage]
Welche Beispiele für eine Folge kennst du?

\end{mdframed}

\begin{answer*}
Im Alltag gibt es einige Beispiele von Folgen, welche oftmals als „Abfolge“ oder „Reihenfolge“ bezeichnet werden. Denk an die Reihenfolge der Sachen, die du nach dem Aufstehen machst: 1. Zähneputzen, 2. Duschen, 3. Anziehen, 4. Kaffee kochen, 5. Kaffee trinken, …

Denk auch an die vielen Folgen, die im Laufe der Zeit verlängert werden. Ein Beispiel ist die Folge der Nationen, die die Fußballweltmeisterschaft gewonnen haben (sowohl bei den Männern als auch bei den Frauen): 1. Uruguay, 2. Italien, 3. Italien, 4. Uruguay, 6. Deutschland, …

\end{answer*}

Allen Beispielen ist gemeinsam, dass die Reihenfolge der Elemente einer Folge genau festgelegt ist. Es gibt eine genau definierte Anordnung der Folgenglieder, die beachtet werden muss. Nach dem Aufstehen sollte man zuerst Kaffee kochen, bevor man diesen trinkt (die umgekehrte Reihenfolge wäre suboptimal {\DejaSans 😊}).

Außerdem können in einer Folge ein oder mehrere Objekte mehrmals als Folgenglieder auftreten. In der Folge der Fußballweltmeister kommen einige Nationen mehr als einmal vor, weil sie die Fußballweltmeisterschaft mehr als einmal gewonnen haben. Dies unterscheidet eine Folge insbesondere von einer Menge. Für eine Menge kann nämlich nicht sinnvoll gefragt werden, wie oft ein Element in ihr vorkommt. Man kann nur fragen, ob ein Objekt Element einer Menge ist oder nicht.

Des Weiteren kannst du die einzelnen Folgenglieder einer Folge durchnummerieren. Du kannst sagen, wer der erste Präsident der USA, der zweite und so weiter war. Jedem Element einer Folge kann also eine oder mehrere Zahlen zugeordnet werden, die angeben, wo dieses Objekt in der Folge vorkommt.

Viele der im Alltag bekannten Folgen sind endlich, es sind aber auch unendliche Folgen vorstellbar. Würde man bis in alle Ewigkeit Fußballweltmeisterschaften austragen, wäre die Folge der Fußballweltmeister unendlich lang.

\section{Formale Definition}

Kommen wir nun zu dem Begriff der Folge in der Mathematik. Der große Unterschied zum Folgenbegriff im Alltag ist der, dass in der Mathematik eine Folge \emph{immer} unendlich lang ist. In der Mathematik gibt es auch endliche Folgen, welche aber \href{https://de.wikibooks.org/wiki/Mathe\_für\_Nicht-Freaks:\_Tupel\_und\_geordnetes\_Paar}
{„Tupel“} genannt werden. Diese sind endliche Abfolgen von Objekten und entsprechen den endlichen Folgen aus dem Alltag.

Eine Folge ist also eine unendliche Abfolge $(a_{1},a_{2},a_{3},a_{4},\ldots )$ von Objekten. Dabei steht $a_{1}$ für das Objekt an der ersten Stelle, $a_{2}$ für das Objekt an der zweiten Stelle und so weiter. Für Folgen gibt es die abkürzende Schreibweise $(a_{n})_{n\in \mathbb {N} }$. Damit lautet die (intuitive) Definition einer Folge:

\begin{importantparagraph*}
Eine Folge $(a_{n})_{n\in \mathbb {N} }$ ist eine unendliche Abfolge von Objekten:

\begin{align*}
(a_{n})_{n\in \mathbb {N} }=(a_{1},a_{2},a_{3},a_{4},\ldots )
\end{align*}

\end{importantparagraph*}

Diese Definition ist intuitiv, weil wir den Begriff „unendliche Abfolge“ nicht exakt definiert haben. Dies muss für eine exakte Definition nachgeholt werden.

\begin{mdframed}[style=semanticbox,frametitleaboveskip=3pt,innerbottommargin=3pt,frametitle={Verständnisaufgabe}]
Schreibe diese Folgen in der gerade kennengelernten Folgenschreibweise!

\begin{enumerate}
\item Folge der Quadratzahlen
\item Folge der positiven ungeraden Zahlen
\end{enumerate}

\end{mdframed}

\begin{answer*}
\begin{enumerate}
\item $(a_{n})_{n\in \mathbb {N} }=(1,\,4,\,9,\,16,\,25,\ldots )$
\item $(b_{n})_{n\in \mathbb {N} }=(1,\,3,\,5,\,7,\,9,\ldots )$
\end{enumerate}

\end{answer*}

\section{Wichtige Begriffe}

Die einzelnen Elemente $a_{n}$ einer Folge werden \emph{Folgenglieder} genannt. Dabei werden die Folgenglieder mit einer natürlichen Zahl $n$ durchnummeriert. Diese natürliche Zahl nennt man \emph{Index}. So ist beispielsweise $a_{4}$ das Folgenglied zum Index $4$.

Für eine Folge mit Elementen aus der Menge $M$ ist ein Folgenglied ein konkretes Element aus der Menge $M$. Nehmen wir die Folge $(1,\ 1,\ 1,\ 2,\ 3,\ 4,\ 5,\ 6,\ldots )$. Das Folgenglied $a_{1}$ ist das identische Objekt wie $a_{2}$ oder $a_{3}$, nämlich die Zahl $1$.

Meint man die gesamte Folge, schreibt man $\left(a_{n}\right)_{n\in \mathbb {N} }$. Zwei Kurzschreibweisen für $\left(a_{n}\right)_{n\in \mathbb {N} }$ sind $(a_{n})_{n}$ und $(a_{n})$. Häufig wird der Buchstabe $n$ als Indexvariable genutzt. Jeder Buchstabe kann aber als Indexvariable verwendet werden, solange er im jeweiligen Kontext keine andere Bedeutung hat. So sind auch die Schreibweisen $\left(a_{i}\right)_{i\in \mathbb {N} }$ oder $\left(a_{k}\right)_{k\in \mathbb {N} }$ möglich.

In der Analysis 1 betrachten wir vor allem Folgen mit reellen Zahlen als Folgenglieder. Diese speziellen Folgen nennt man \emph{reelle Folgen}. In folgender Übersicht und in folgender Tabelle sind alle wesentlichen Begriffe zu den Bestandteilen von Folgen zusammengefasst:

\begin{align*}
(a_{n})_{n\in \mathbb {N} }=\overbrace {(a_{1},\,{\color {OliveGreen}\underbrace {a_{2}} _{\text{Folgenglied}}},\,a_{3},\,a_{\color {Blue}\underbrace {{}_{4}} _{\text{Index}}},\,\ldots )} ^{\text{Folge}}
\end{align*}


\renewcommand{\arraystretch}{1.5}

\begin{longtabu} to \linewidth {X[l]X[l]X[l]} \\ \toprule 
Begriff & Schreibweise & Definition \\ 
\midrule
Folge & $(a_{n})_{n\in \mathbb {N} }$ oder kurz $(a_{n})$ & Eine Folge ist eine unendliche Abfolge von Objekten. \\ 
Folgenglied & $a_{n}$ & Ein Folgenglied ist ein konkretes Objekt, das in der Folge an einer bestimmten Stelle vorkommt. \\ 
Index & $n$ & Der Index ist eine natürliche Zahl, die die Folgenglieder durchnummeriert. \\ 
\bottomrule
\end{longtabu}
\renewcommand{\arraystretch}{1.0}
\begin{warning*}
Einige Studenten stellen sich unter einer reellen Folge eine kontinuierliche Funktion vor (insbesondere, wenn sie diese zeichnen wollen). Dies ist jedoch falsch, da eine reelle Folge nur aus einer Abfolge einzelner reeller Zahlen besteht. Dies demonstriert die folgende Gegenüberstellung der harmonischen Folge $(a_{n})_{n\in \mathbb {N} }=\left({\tfrac {1}{n}}\right)_{n\in \mathbb {N} }=\left({\tfrac {1}{1}},{\tfrac {1}{2}},{\tfrac {1}{3}},\ldots \right)$ mit der Funktion $f:\mathbb {R} ^{+}\rightarrow \mathbb {R} ^{+}$ mit $f(x)={\tfrac {1}{x}}$. Beachte, dass es bei der harmonischen Folge im Gegensatz zur Funktion $f(x)={\tfrac {1}{x}}$ nur diskrete Werte (einzelne Punkte) im Graphen gibt:

\begin{tabularx}{\linewidth}{XX}
\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:First members of harmonic sequence.svg}{\textbf{First members of harmonic sequence.svg}} by Stephan Kulla \textit{(CC0)}}\begin{minipage}[t]{\linewidth}
\begin{figure}[H]
\begin{minipage}[t][0.2\textheight][c]{\linewidth}
\centering
\adjincludegraphics[max width=1.\linewidth, max height=0.2\textheight]{file58first32members32of32harmonic32sequence95a8bfb6a23a9dc3a9083db63c5f821c5769c949f5}
\end{minipage}
\caption*{Die harmonische Folge, definiert durch $a_{n}={\frac {1}{n}}$, hat nur diskrete Werte. (\arabic{imagelabel})}
\end{figure}

\end{minipage}
&
\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:The function 1 over x.svg}{\textbf{The function 1 over x.svg}} by Stephan Kulla \textit{(CC0)}}\begin{minipage}[t]{\linewidth}
\begin{figure}[H]
\begin{minipage}[t][0.2\textheight][c]{\linewidth}
\centering
\adjincludegraphics[max width=1.\linewidth, max height=0.2\textheight]{file58the32function32132over32x95ea52564f44865a47125367351f56bb78835a1b27}
\end{minipage}
\caption*{Zum Vergleich: Die Funktion $f(x)={\tfrac {1}{x}}$ mit dem Definitionsbereich der positiven reellen Zahlen. (\arabic{imagelabel})}
\end{figure}

\end{minipage}
\end{tabularx}

\end{warning*}

\section{Definition als Funktion}

\subsection{Definition}

Oben haben wir die Folge intuitiv als unendliche Abfolge von Objekten definiert. Mit Hilfe des Funktionenbegriffs kann diese Definition konkretisiert und vor allem mathematisch exakt formuliert werden. Hierzu nehmen wir die Menge der natürlichen Zahlen und ordnen jeder natürlichen Zahl $n$ ein beliebiges Objekt $a_{n}$ zu (wobei diese Objekte aus einer Menge $M$ stammen, in der wir eine Folge bilden möchten). Damit erhalten wir eine unendliche und durchnummerierte Abfolge beliebiger Objekte, wie die folgende Skizze verdeutlicht:

\begin{align*}
{\begin{array}{ccccl}1&2&3&4&\ldots \\\downarrow &\downarrow &\downarrow &\downarrow &\\a_{1}&a_{2}&a_{3}&a_{4}&\ldots \end{array}}
\end{align*}

Eine solche Zuordnung ist nichts anderes als eine Funktion $\mathbb {N} \rightarrow M$, also eine Abbildung von den natürlichen Zahlen in eine Menge $M$, die alle möglichen Folgenglieder enthält. So haben wir bei der Folge der deutschen Bundeskanzler die Zuordnung:

\begin{align*}
{\begin{array}{cccccl}1&2&3&4&5&\ldots \\\downarrow &\downarrow &\downarrow &\downarrow &\downarrow &\\{\text{Adenauer}}&{\text{Erhard}}&{\text{Kiesinger}}&{\text{Brandt}}&{\text{Schmidt}}&\ldots \\\end{array}}
\end{align*}

Die „unendliche Abfolge“ von Folgengliedern können wir somit als eine Funktion auffassen, die für jede natürliche Zahl $n$ angibt, was das $n$-te Folgenglied sein soll:

\begin{definition*}[Folge]
Eine Folge in einer Menge $M$ ist eine Abbildung

\begin{align*}
f:\mathbb {N} \to M:n\mapsto f(n)=:a_{n}
\end{align*}

Wir schreiben $(a_{n})_{n\in \mathbb {N} }$ anstelle von $f$.

\end{definition*}

Obige Definition nutzt nur bereits bekannte Konzepte und erfüllt damit die Anforderungen an eine mathematisch exakte Definition. Deswegen wird sie in den meisten Lehrbüchern als Definition einer Folge verwendet.

\chapter{Explizite und rekursive Bildungsgesetze}

Zur Definition einer Folge muss man eine Zuordnungsvorschrift angeben, die den einzelnen Indizes die Folgenglieder zuweist. Diese Zuordnungsvorschrift wird \emph{Bildungsgesetz} der Folge (manchmal auch \emph{Bildungsvorschrift}) genannt. Diese Zuordnungsvorschrift kann im Allgemeinen sehr kompliziert sein. Da eine Folge stets unendlich viele Glieder besitzt, kann man die Zuordnungsvorschrift nicht durch Aufzählung aller Folgenglieder definieren. Stattdessen gibt es andere Möglichkeiten wie \emph{explizite} und \emph{rekursive} Bildungsgesetze.

\section{Explizite Bildungsgesetze}

Bei einer \emph{expliziten Bildungsvorschrift} wird ein vom Index der Folge abhängiger Funktionsterm angegeben, mit der man die einzelnen Folgenglieder ausrechnen kann. Ein solches Bildungsgesetz wird meist folgendermaßen aufgeschrieben: Für alle $n\in \mathbb {N} $ wird definiert

\begin{align*}
a_{n}:={\text{ irgendein Term mit }}n
\end{align*}

Ein Beispiel ist die Vorschrift $a_{n}=n^{2}$ für alle $n\in \mathbb {N} $ für die Folge aller Quadratzahlen. Man kann diese Folge so aufschreiben:

\begin{align*}
(a_{n})_{n\in \mathbb {N} }=\left(n^{2}\right)_{n\in \mathbb {N} }&=(1^{2},\,{\color {OliveGreen}2^{2}},\,{\color {RedOrange}3^{2}},\,{\color {Fuchsia}4^{2}},\ldots )\\&=(1,\,{\color {OliveGreen}4},\,{\color {RedOrange}9},\,{\color {Fuchsia}16},\ldots )
\end{align*}

Eine explizite Bildungsvorschrift der Folge zeichnet sich dadurch aus, dass die einzelnen Folgenglieder berechnet werden können, ohne andere Folgenglieder kennen zu müssen. Wenn man also ein bestimmtes Folgenglied berechnen möchte, so muss man nur den gewünschten Index in die Formel der expliziten Bildungsvorschrift einsetzen und den Wert dieser Formel berechnen.

\section{Rekursive Bildungsgesetze}

Eine \emph{rekursive Bildungsvorschrift} zeichnet sich dadurch aus, dass man zur Berechnung einzelner Folgenglieder die Vorgänger dieser Folgenglieder kennen muss. Dies erkennt man daran, dass in der Funktion zur Berechnung eines Folgenglieds die vorhergehenden Folgenglieder mit auftauchen. Allgemein kann eine reelle Folge $(a_{n})_{n\in \mathbb {N} }$ folgendermaßen rekursiv definiert werden:

\begin{align*}
a_{1}&:=r{\text{ für ein }}r\in \mathbb {R} \\[0.3em]a_{n+1}&:={\text{irgendein Term mit }}n,a_{1},a_{2},\ldots ,a_{n}\ {\text{ für alle }}n\in \mathbb {N} 
\end{align*}

Da man zur Berechnung einzelner Folgenglieder bereits die Vorgänger kennen muss, muss bei der rekursiven Definition einer Folge das erste Folgenglied explizit benannt werden. So ist ein Beispiel für ein rekursives Bildungsgesetz:

\begin{align*}
a_{1}&=-6\\a_{n+1}&=a_{n}+2\ {\text{ für alle }}n\in \mathbb {N} 
\end{align*}

Die erste Formel $a_{1}=-6$ definiert das erste Folgenglied explizit und wird \emph{Rekursionsanfang} genannt. Durch die zweite Formel, welche man \emph{Rekursionsschritt} nennt, kann ein neues Folgenglied aus dessen Vorgänger berechnet werden. Zunächst gibt man über den Rekursionsanfang das erste Folgenglied vor und berechnet dann durch wiederholte Anwendung des Rekursionsschritts weitere Folgenglieder. Es ist:

\begin{align*}
a_{1}&={\color {Blue}-6}\\a_{2}&=a_{1+1}=a_{1}+2={\color {Blue}-6}+2={\color {OliveGreen}-4}\\a_{3}&=a_{2+1}=a_{2}+2={\color {OliveGreen}-4}+2={\color {RedOrange}-2}\\a_{4}&=a_{3+1}=a_{3}+2={\color {RedOrange}-2}+2=0\\&\vdots 
\end{align*}

Rekursive Bildungsgesetze für Folgen sind meist einfacher zu finden als explizite Bildungsvorschriften. Bei expliziten Bildungsvorschriften sind aber die Eigenschaften einer Folge meist einfacher aus dem Bildungsgesetz ablesbar als bei rekursiv definierten Folgen. Auch ist bei expliziten Bildungsvorschriften die Berechnung der Folgenglieder einfacher. Angenommen, wir möchten das $1000$-te Folgenglied berechnen. Bei einem expliziten Bildungsgesetz können wir $1000$ direkt in die gegebene Formel einsetzen. Bei einer rekursiven Bildungsvorschrift muss man erst einmal alle unbekannten $998$ Vorgänger ausrechnen.

\clearpage

\section{Beispiele zur Bildung von Folgen}

\begin{example*}[Angabe eines Funktionsterms (Explizite Bildungsvorschrift)]
Wir definieren die Folge $(a_{n})_{n\in \mathbb {N} }$ in $\mathbb {N} $ durch $a_{n}:=n^{3}+7$ für alle $n\in \mathbb {N} $. Statt $(a_{n})_{n\in \mathbb {N} }$ schreiben wir auch $(n^{3}+7)_{n\in \mathbb {N} }$.

\end{example*}

\begin{example*}[Berechnung eines Folgenglieds in Abhängigkeit von vorherige Folgenglieder (Rekursive Bildungsvorschrift)]
Wir definieren die Folge $(a_{n})_{n\in \mathbb {N} }$ in $\mathbb {R} $ durch $a_{1}:=1$ und für alle $n\geq 2$ setzen wir $a_{n}:=(a_{n-1}+1)^{2}$. Dies ist eine \emph{Rekursionsvorschrift} für die Folge $(a_{n})_{n\in \mathbb {N} }$.

Wenn wir das $n$-te Folgenglied ausrechnen wollen, können wir zuerst $a_{2}=(a_{1}+1)^{2}$ aus $a_{1}$ berechnen. Anschließend können wir $a_{3}$ aus $a_{2}$ berechnen, indem wir den Term $a_{3}=(a_{2}+1)^{2}$ ausrechnen. Das machen wir solange, bis wir bei $n$ angekommen sind. Also wird durch die obige Rekursionsvorschrift eindeutig eine Folge definiert.

\end{example*}

\begin{example*}[Folge der Primzahlen]
Definiere die Folge $(p_{n})_{n\in \mathbb {N} }=(2,3,5,7,11,\ldots )$ in $\mathbb {N} $ als die aufsteigende Folge der Primzahlen, d.h. es soll $p_{n}<p_{n+1}$ für alle $n\in \mathbb {N} $ gelten und $\{p_{n}:n\in \mathbb {N} \}$ soll die Menge aller Primzahlen sein.

Diese Definition hört sich sehr abstrakt an und es ist nicht klar, wie man ein bestimmtes Folgenglied ausrechnet. Dennoch wird dadurch eindeutig eine Folge definiert. Das zeigt die folgende Überlegung:

Die Menge aller Primzahlen ist eine Teilmenge der natürlichen Zahlen. Also können wir die Primzahlen der Größe nach ordnen und anschließend durchnummerieren. Da es unendlich viele Primzahlen gibt, wird dadurch jeder natürlichen Zahl $n$ eindeutig eine Primzahl $p_{n}$ zugeordnet, so dass $p_{n}<p_{n+1}$ für alle $n\in \mathbb {N} $ gilt und es für jede Primzahl $p$ ein $n\in \mathbb {N} $ mit $p=p_{n}$ gibt.

\end{example*}

\chapter{Beispiele und Eigenschaften}

\section{Beispiele}

\subsection{Konstante Folge}

Eine Folge heißt \emph{konstant}, wenn alle ihre Folgenglieder gleich sind. So ist folgende Folge konstant:

\begin{align*}
\left(a_{n}\right)_{n\in \mathbb {N} }=(2,\,2,\,2,\,2,\,2,\,\ldots )
\end{align*}

Mit $c\in \mathbb {R} $ lautet die allgemeine Formel einer konstanten Folge $a_{n}:=c$ für alle $n\in \mathbb {N} $.

\subsection{Arithmetische Folgen}

\emph{Arithmetische Folgen} haben die Eigenschaft, dass die Differenz zweier benachbarter Folgenglieder konstant ist. So ist die Folge der ungeraden natürlichen Zahlen eine arithmetische Folge, da sie eine konstante Differenz von $2$ zwischen zwei Folgengliedern besitzt:

\begin{align*}
\left(a_{n}\right)=(1,\,3,\,5,\,7,\,9,\,11,\,\ldots )
\end{align*}

\begin{mdframed}[style=semanticbox,frametitleaboveskip=3pt,innerbottommargin=3pt,frametitle=Frage]
Wie lautet die allgemeine rekursive Formel einer arithmetischen Folge?

\end{mdframed}

\begin{answer*}
Das erste Folgenglied $a_{1}$ ist beliebig. Das nächste Folgenglied hat eine konstante Differenz zu $a_{1}$. Nennen wir diese Differenz $d$. Damit muss $a_{2}-a_{1}=d$ und somit $a_{2}=a_{1}+d$ sein. Analog ist wegen $a_{3}-a_{2}=d$ das Folgenglied $a_{3}=a_{2}+d$ und so weiter. Damit haben wir die rekursive Definition:

\begin{align*}
a_{1}&\in \mathbb {R} \ {\text{beliebig}}\\a_{n+1}&:=a_{n}+d\ {\text{für alle }}n\in \mathbb {N} \\
\end{align*}

\end{answer*}

\begin{mdframed}[style=semanticbox,frametitleaboveskip=3pt,innerbottommargin=3pt,frametitle=Frage]
Wie lautet die allgemeine explizite Formel einer arithmetischen Folge?

\end{mdframed}

\begin{answer*}
Die rekursive Formel für die arithmetische Formel lautet $a_{n+1}=a_{n}+d$ für alle $n\in \mathbb {N} $, wobei $a_{1}\in \mathbb {R} $ vorgegeben ist. Das heißt $a_{2}=a_{1}+d$ und $a_{3}=a_{2}+d=(a_{1}+d)+d=a_{1}+2\cdot d$. Analog ist $a_{4}=a_{1}+3\cdot d$. Damit erhält man die explizite Formel für alle $n\in \mathbb {N} $:

\begin{align*}
a_{n}=a_{1}+(n-1)\cdot d
\end{align*}

\end{answer*}

\subsection{Geometrische Folge}

\begin{figure}[h]
\vspace{\baselineskip}
\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Geometrische_Folge.svg}{\textbf{Geometrische\allowbreak\_Folge.svg}} by Stephan Kulla \textit{(CC BY-SA 3.0)}}\centering
\adjincludegraphics[max width=.5\textwidth, max height=0.2\textheight]{file58geometrische95folge95651a5c41481f267934607d64e90536ad4d99de86}
\caption*{Beispiel einer geometrischen Folge: $a_{n}=2^{n}$ für alle $n\in \mathbb {N} $ (\arabic{imagelabel})}
\end{figure}
Bei der \emph{geometrischen Folge} ist das Verhältnis zweier aufeinanderfolgender Folgenglieder konstant. Dabei darf kein Folgenglied 0 sein, da man sonst kein Verhältnis zum nächsten Folgenglied bilden könnte. Ein Beispiel hierfür ist die Zahlenfolge $a_{n}=2^{n}$ mit dem konstanten Verhältnis $2$:

\begin{align*}
\left(a_{n}\right)=(2,\,4,\,8,\,16,\,32,\,64,\,\ldots )
\end{align*}

\begin{mdframed}[style=semanticbox,frametitleaboveskip=3pt,innerbottommargin=3pt,frametitle=Frage]
Wie lautet die allgemeine rekursive Formel der geometrischen Folge?

\end{mdframed}

\begin{answer*}
Das erste Folgenglied $a_{1}\neq 0$ ist beliebig. Das nächste Folgenglied steht im konstanten Verhältnis zu $a_{1}$. Nennen wir dieses Verhältnis $q\neq 0$. Damit muss ${\tfrac {a_{2}}{a_{1}}}=q$, also $a_{2}=a_{1}\cdot q$ sein. Analog ist wegen ${\tfrac {a_{3}}{a_{2}}}=q$ das Folgenglied $a_{3}=a_{2}\cdot q$ und so weiter. Damit haben wir die rekursive Definition:

\begin{align*}
a_{1}&\in \mathbb {R} \setminus \{0\}{\text{ beliebig}}\\a_{n+1}&:=a_{n}\cdot q
\end{align*}

\end{answer*}

\begin{mdframed}[style=semanticbox,frametitleaboveskip=3pt,innerbottommargin=3pt,frametitle=Frage]
Wie lautet die allgemeine explizite Formel der geometrischen Folge?

\end{mdframed}

\begin{answer*}
Die rekursive Formel für die geometrische Folge lautet $a_{n+1}=a_{n}\cdot q$. Das heißt $a_{2}=a_{1}\cdot q$ und $a_{3}=a_{2}\cdot q=a_{1}\cdot q^{2}$. Analog ist $a_{4}=a_{1}\cdot q^{3}$. Damit lautet die explizite Formel einer geometrischen Folge:

\begin{align*}
a_{n}=a_{1}\cdot q^{n-1}
\end{align*}

\end{answer*}

\subsection{Alternierende Folgen}

Bei einer \emph{alternierenden Folge} ändert sich das Vorzeichen zwischen zwei Folgengliedern. Der Begriff „alternierend“ bedeutet hier „regelmäßiger Vorzeichenwechsel“. So wechselt bei der Folge $a_{n}=(-1)^{n}$ der Wert immer zwischen $1$ und $-1$, so dass diese Folge eine alternierende Folge ist. Ein weiteres Beispiel ist die Folge $a_{n}=(-1)^{n+1}\cdot n$ mit $\left(a_{n}\right)_{n\in \mathbb {N} }=(1,\,-2,\,3,\,-4,\,5,\,-6,\,\ldots )$.

\section{Eigenschaften und wichtige Begriffe}

\subsection{Beschränkte Folge}

\begin{figure}[h]
\vspace{\baselineskip}
\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Alternierende_harmonische_Folge_mit_Schranken.svg}{\textbf{Alternierende\allowbreak\_harmonische\allowbreak\_Folge\allowbreak\_mit\allowbreak\_Schranken.svg}} by Stephan Kulla \textit{(CC BY-SA 3.0)}}\centering
\adjincludegraphics[max width=.5\textwidth, max height=0.2\textheight]{file58alternierende95harmonische95folge95mit95schranken956531e3bb2aabff276f4ca7a4bf98f892d8f01841}
\caption*{Ein Beispiel einer beschränkten Folge ($a_{n}=(-1)^{n+1}\cdot {\tfrac {1}{n}}$) mit einigen eingezeichneten Schranken. (\arabic{imagelabel})}
\end{figure}
Eine Folge nennt man in der Mathematik \emph{nach oben beschränkt}, wenn es eine Zahl gibt, die die Folgenglieder nie überschreiten. Eine solche reelle Zahl wird \emph{obere Schranke} der Folge genannt („diese Zahl beschränkt die Folge von oben“). Damit ergibt sich folgende Definition einer nach oben beschränkten Folge:

\begin{align*}
\left(a_{n}\right)_{n\in \mathbb {N} }{\text{ ist nach oben beschränkt }}:\iff \exists S\in \mathbb {R} :\ \forall n\in \mathbb {N} :\ a_{n}\leq S
\end{align*}

Analog ist eine Folge nach unten beschränkt, wenn sie eine \emph{untere Schranke} besitzt. Es gibt also eine reelle Zahl, die die Folgenglieder nicht unterschreiten. Dementsprechend ist die untere Beschränktheit definiert mit:

\begin{align*}
\left(a_{n}\right)_{n\in \mathbb {N} }{\text{ ist nach unten beschränkt }}\ :\iff \ \exists s\in \mathbb {R} :\ \forall n\in \mathbb {N} :\ a_{n}\geq s
\end{align*}

Wenn eine Folge sowohl nach oben als auch nach unten beschränkt ist, nennt man diese Folge \emph{beschränkt}. Damit haben wir die Definitionen:

\begin{description}[style=nextline]
\item[obere Schranke]
Eine obere Schranke ist eine Zahl, die größer als jedes Folgenglied einer Folge ist. $S$ ist eine obere Schranke von $(a_{n})_{n\in \mathbb {N} }$, wenn $a_{n}\leq S$ für alle $n\in \mathbb {N} $ ist.\item[nach oben beschränkte Folge]
Eine Folge ist nach oben beschränkt, wenn sie irgendeine obere Schranke besitzt.\item[untere Schranke]
Eine untere Schranke ist eine Zahl, die kleiner als jedes Folgenglied einer Folge ist. $s$ ist eine untere Schranke von $(a_{n})_{n\in \mathbb {N} }$, wenn $a_{n}\geq s$ für alle $n\in \mathbb {N} $ ist.\item[nach unten beschränkte Folge]
Eine Folge ist nach unten beschränkt, wenn sie irgendeine untere Schranke besitzt.\item[beschränkte Folge]
Eine Folge ist beschränkt, wenn sie sowohl nach oben als auch nach unten beschränkt ist.\end{description}

\begin{hint*}
Eine obere Schranke muss nicht zwangsläufig die kleinstmögliche, obere Schranke sein und eine untere Schranke nicht unbedingt die größtmögliche! Wenn zum Beispiel eine Folge nach oben durch $1$ beschränkt ist, ist sie auch durch $2$, $44$, $123$ und $502$ nach oben beschränkt. Um zu die Beschränktheit einer Folge nach oben zu zeigen, reicht es, irgendeine beliebige obere Schranke anzugeben (auch wenn sie noch so groß sein sollte).

\end{hint*}

\subsection{Monotone Folgen}

Folgen werden auch nach ihrem Wachstumsverhalten unterschieden: Wenn eine Folge mit jedem Folgenglied wächst (also jedes nachfolgende Folgenglied $a_{n+1}$ größer als $a_{n}$ ist), nennt man diese Folge eine \emph{streng monoton wachsende Folge}. Analog heißt eine immer kleiner werdende Folge \emph{streng monoton fallende Folge}. Wenn man bei diesen Begriffen auch zulassen möchte, dass eine Folge zwischen zwei Folgengliedern konstant sein darf, nennt man die Folge \emph{monoton wachsende Folge} oder \emph{monoton fallende Folge}. Merke dir: „streng monoton“ bedeutet so viel, wie „immer größer“ oder „immer kleiner“ werdend. Demgegenüber bedeutet „monoton“, ohne das „streng“, so viel wie „immer größer werdend oder konstant bleibend“ bzw. „immer kleiner werdend oder konstant bleibend“. Wir erhalten folgende Definition:

\begin{definition*}[monotone Folgen]
Für eine reelle Folge $\left(a_{n}\right)_{n\in \mathbb {N} }$ definieren wir:

\begin{align*}
&\left(a_{n}\right)_{n\in \mathbb {N} }{\text{wächst streng monoton }}&:\iff \ \forall n\in \mathbb {N} :\ a_{n+1}>a_{n}\\&\left(a_{n}\right)_{n\in \mathbb {N} }{\text{wächst monoton }}&:\iff \ \forall n\in \mathbb {N} :\ a_{n+1}\geq a_{n}\\&\left(a_{n}\right)_{n\in \mathbb {N} }{\text{fällt streng monoton }}&:\iff \ \forall n\in \mathbb {N} :\ a_{n+1}<a_{n}\\&\left(a_{n}\right)_{n\in \mathbb {N} }{\text{fällt monoton }}&:\iff \ \forall n\in \mathbb {N} :\ a_{n+1}\leq a_{n}\\
\end{align*}

\end{definition*}

\part{Konvergenz und Divergenz}

\addxcontentsline{lof}{part}[\arabic{part}]{Konvergenz und Divergenz}\begin{authors}
Stephan Kulla, Autorenkollektiv „Auswahlaxiom“ (Charlotte Dietze, Matthias Paulsen, Anne Reif), Who2010, Braun\textasciitilde{}dewikibooks, Caroline Pfannschmidt, Chris ShuYu Dong, Ekin Köksal, Akram Chawki, Christoph Kehle, Phoible, Werner Fröhlich, Daniel5Ko, Maths CA, 0-Brane, PhilippHanemann, Mjchael, Paolo Martinoni, Peter Gröbner, Hanpetko, OmaMath, Meitnerium266, Jenny Kilian, Juetho, Katharina Kircher, HirnSpuk, Nevsor, Alexander Herzog, Beezle73, Letsluk\end{authors}

\chapter{Definition Grenzwert}

In diesem Kapitel wird das Konzept des \emph{Grenzwerts} (auch \emph{Limes} genannt) bzw. der \emph{Konvergenz einer Folge} eingeführt. Da Begriffe wie Stetigkeit, Ableitung und Integral mithilfe des Grenzwertbegriffes definiert werden, ist der Grenzwert sehr wichtig. Er bildet damit das Rückgrat der Analysis.

\section{Intuition hinter der Idee der Konvergenz}

Um eine mathematische Definition des Grenzwerts zu finden, sollten wir zunächst eine intuitive Idee für diesen Begriff bekommen. Schauen wir uns dafür die harmonische Folge $\left({\tfrac {1}{n}}\right)_{n\in \mathbb {N} }$ an. Sie hat die Folgenglieder

\begin{align*}
1,\,{\tfrac {1}{2}},\,{\tfrac {1}{3}},\,{\tfrac {1}{4}},\,{\tfrac {1}{5}},\ldots 
\end{align*}

Diese nähern sich von oben immer mehr der Null an und man kann intuitiv sagen:

\begin{itemize}
\item Die Folge geht beliebig nah an $0$.
\item Je größer $n$ ist, desto mehr nähert sich $a_{n}={\tfrac {1}{n}}$ der $0$ an.
\item Die Folge $a_{n}={\tfrac {1}{n}}$ strebt gegen $0$.
\item Die Folge $a_{n}={\tfrac {1}{n}}$ erreicht im Unendlichen die $0$.
\item ...
\end{itemize}

Alle diese Erklärungen beschreiben intuitiv, was wir in der Analysis den \emph{Grenzwert} einer Folge nennen. In diesem Fall ist $0$ der Grenzwert der harmonischen Folge $\left({\tfrac {1}{n}}\right)_{n\in \mathbb {N} }$.

\section{Herleitung der Definition des Grenzwerts}

\subsection{Erste Schritte}

Um als Mathematiker mit dem Begriff des Grenzwerts arbeiten zu können, brauchen wir eine klare und exakte Definition. Diese können wir finden, indem wir mit einer intuitiven Idee starten und diese so lange konkretisieren, bis wir eine exakte mathematische Definition haben. Die Konkretisierung erfolgt so lange, bis wir eine Formulierung finden, die nur noch bereits definierte Begriffe enthält. Fangen wir mit der folgenden intuitiven Beschreibung des Grenzwerts an:

\begin{importantparagraph*}
„Eine Folge hat einen Grenzwert $a$, wenn ihre Folgenglieder beliebig nahe an $a$ gehen.”

\end{importantparagraph*}

Was bedeutet „beliebig nahe“ im obigen Satz? Wir können es so übersetzen: Stellen wir uns die Folgenglieder in einem Koordinatensystem vor, wobei auf der x-Achse die Indizes $n$ und auf der y-Achse die Werte der Folgenglieder $a_{n}$ stehen. Jedes Folgenglied wird durch einen Punkt in diesem Koordinatensystem dargestellt. Den Grenzwert $a$ veranschaulichen wir durch eine gestrichelte Linie.

\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Folgenglieder_im_KOSY.svg}{\textbf{Folgenglieder\allowbreak\_im\allowbreak\_KOSY.svg}} by Caroline Pfannschmidt, Stephan Kulla \textit{(CC BY-SA 4.0)}}\begin{center}
\adjincludegraphics[max width=0.5\textwidth,max height=0.2\textheight]{file58folgenglieder95im95kosy9565a488a1501045b30c4fdef5fc3120e50c6df00c}\end{center}

Wenn die Folgenglieder nun {''}beliebig nahe{''} an $a$ herangehen, wird der Abstand zum Grenzwert immer kleiner. Nun nehmen wir einen sehr schmalen {''}Schlauch{''} (man kann es sich wie einen Gartenschlauch vorstellen), der den Radius $\epsilon $ hat. Diesen {''}fädeln{''} wir von rechts über den Grenzwert. Solange der Abstand der Folgenglieder zum Grenzwert kleiner als der Schlauch dick ist, kann man den Schlauch noch weiter nach links schieben. Alle Punkte befinden sich immer noch innerhalb des Schlauches. Sobald ein Punkt aber einen größeren Abstand zum Grenzwert hat, kann er nicht mehr innerhalb des Schlauches liegen. An dieser Stelle müssen wir aufhören, den Schlauch weiter aufzufädeln.

\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Epsilonschlauch.svg}{\textbf{Epsilonschlauch.svg}} by Caroline Pfannschmidt, Stephan Kulla \textit{(CC BY-SA 4.0)}}\begin{center}
\adjincludegraphics[max width=0.5\textwidth,max height=0.2\textheight]{file58epsilonschlauch95fbb1891a76f2f1c52923bf30c41408073be5a07c}\end{center}

Im Bild ist der rote Punkt das erste Folgenglied, das nicht mehr in den Schlauch passt. Der grüne Punkt ${\color {forestgreen}a_{N_{0}}}$ ist damit das Folgenglied, ab dem alle späteren Folgenglieder (also mit Index größer gleich $N_{0}$) innerhalb des Schlauches liegen. Wenn wir den Schlauch jetzt dünner machen, können vielleicht nicht mehr alle Punkte innerhalb des Schlauches liegen, die vorher im großen Schlauch lagen. Deshalb kann man den dünnen Schlauch nicht mehr so weit nach links schieben, wenn noch alle Punkte innerhalb des Schlauches liegen sollen. Jedoch ist es auch bei ihm möglich, dass fast alle Folgenglieder „eingefangen“ werden können:

\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Epsilonschlauch_klein.svg}{\textbf{Epsilonschlauch\allowbreak\_klein.svg}} by Caroline Pfannschmidt, Stephan Kulla \textit{(CC BY-SA 4.0)}}\begin{center}
\adjincludegraphics[max width=0.5\textwidth,max height=0.2\textheight]{file58epsilonschlauch95klein956b1ad4a0a5e54c8ad1f7174f228970ee52b44a60}\end{center}

Nun ist der rote Punkt, der nicht mehr in den dünneren Schlauch passt, weiter rechts als im vorherigen Bild. Das neue erste Folgenglied im Schlauch nennen wir ${\color {forestgreen}a_{N_{1}}}$. Alle Folgenglieder mit einem Index größer gleich $N_{1}$ liegen in dem dünneren Schlauch.

Dieser Schlauch hat keine nähere mathematische Bedeutung. Wir haben ihn nur verwendet, um zu zeigen, dass die Folgenglieder immer näher an der gestrichtelten Linie liegen. Sie gehen also immer näher an $a$ heran und insbesondere auch nicht mehr weiter weg, da sie ja ab einem bestimmten Index alle innerhalb des Schlauches liegen, egal wie dünn dieser ist. Haben wir das verstanden, brauchen wir den „Schlauch“ nicht mehr. Was bisher unser beliebig dünner Schlauch mit Radius $\epsilon $ war, werden wir $\epsilon $-Umgebung nennen.

\subsection{In jeder Umgebung um den Grenzwert liegen fast alle Folgenglieder}

\begin{figure}[h]
\vspace{\baselineskip}
\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Konvergenz.svg}{\textbf{Konvergenz.svg}} by Platonides, Youthenergy \textit{(CC-BY-SA-3.0)}}\centering
\adjincludegraphics[max width=.5\textwidth, max height=0.2\textheight]{file58konvergenz954d770c4cfe5dd30e8da5e586b6b226bdf89d2836}
\caption*{Der Grenzwert ist eine Zahl, so dass für jede $\epsilon $-Umgebung fast alle Folgenglieder in dieser Umgebung um die Zahl liegen. (\arabic{imagelabel})}
\end{figure}
Wir haben Indizes wie $N_{0}$ bzw. $N_{1}$ gefunden, ab dem alle nachfolgenden Folgenglieder innerhalb der jeweiligen $\epsilon $-Schläuche liegen. Machen wir den Schlauch noch dünner, finden wir entsprechend ein $N_{2}$, ab dem alle Folgenglieder im Schlauch liegen und so weiter. Egal wie dünn wir den Schlauch machen, es wird immer einen Punkt geben, ab dem alle weiteren Folgenglieder im Schlauch liegen.

Da solche Startindizes wie $N_{0}$ natürliche Zahlen sind, kann es nur endlich viele Folgenglieder geben, die außerhalb des Schlauches liegen (nämlich höchstens $N_{0}-1$ Folgenglieder). Alle restlichen Folgenglieder liegen innerhalb des Schlauchs. Da eine Folge unendlich viele Folgenglieder hat, kann man die endlich vielen Glieder, die außerhalb liegen, vernachlässigen und sagen, dass \emph{fast alle} Glieder innerhalb des Schlauches liegen. Das geht selbst, wenn das $N_{0}$ sehr groß ist. Denn in Relation zu unendlich vielen Folgegliedern, die innerhalb des Schlauchs liegen, sind endlich viele Folgeglieder außerhalb des Schlauchs wenig – egal wie groß $N_{0}$ ist. Das zu verstehen ist wichtig, um den Grenzwertbegriff zu verstehen.

Wir haben also herausgefunden, dass \emph{fast alle} Folgenglieder in dem Schlauch liegen, egal wie dünn dieser ist. Das heißt, dass die Folgenglieder immer näher an den \emph{Grenzwert} $a$ herangehen. Und das ist es, was den Grenzwert ausmacht. Die Folgenglieder $a_{n}$ liegen beliebig nah am Grenzwert $a$, wenn wir hinreichend große $n$ betrachten.

\subsection{Was bedeutet „fast alle“?}

Dazu stellen wir uns ein Koordinatensystem vor, auf dem \emph{unendlich viele} Folgenglieder einer konvergierenden Folge dargestellt sind. Nun nehmen wir einen $\epsilon $-Schlauch und fädeln ihn von rechts über den Grenzwert ein. Dann passt eine \emph{endliche} Anzahl an Folgengliedern \emph{nicht} in den Schlauch, weil der Abstand zum Grenzwert $a$ nicht klein genug ist. Jedoch liegen \emph{unendlich viele} Folgenglieder innerhalb des Intervalls $(a-\epsilon ,a+\epsilon )$ und damit im $\epsilon $-Schlauch.

Die Anzahl der Folgenglieder, die innerhalb des Intervalls $(a-\epsilon ,a+\epsilon )$ liegen, ist also überwältigend groß im Vergleich zur Anzahl der Folgenglieder außerhalb dieses Intervalls. Man sagt daher, dass fast alle Folgenglieder im Intervall $(a-\epsilon ,a+\epsilon )$ liegen.

Wenn „fast alle“ Folgenglieder im Intervall $(a-\epsilon ,a+\epsilon )$ liegen, so bedeutet es, dass „alle bis auf endlich viele“ Folgenglieder ein Element dieses Intervalls sind.

\subsection{Verfeinerung der mathematischen Definition}

Insgesamt können wir definieren:

\begin{importantparagraph*}
„Eine Folge hat einen Grenzwert $a$, wenn es für jede $\epsilon $-Umgebung von $a$, also $]a-\epsilon ,a+\epsilon [$, ein Folgenglied gibt, ab dem alle folgenden Folgenglieder Elemente der Umgebung sind.“

\end{importantparagraph*}

Obige Aussage könnten wir als mathematische Definition des Grenzwerts verwenden. Jedoch ist es sinnvoll, diese Definition noch weiter zu formalisieren.

Nun ist ein Folgenglied $a_{n}$ genau dann ein Element von $(a-\epsilon ,a+\epsilon )$, wenn $|a_{n}-a|<\epsilon $ ist. Also:

\begin{importantparagraph*}
„Eine Folge $\left(a_{n}\right)_{n\in \mathbb {N} }$ hat einen Grenzwert $a$, wenn es zu jedem $\epsilon >0$ ein Folgenglied $a_{N}$ gibt, ab dem für alle folgenden Folgenglieder $a_{n}$ die Ungleichung $|a_{n}-a|<\epsilon $ erfüllt ist.

\end{importantparagraph*}

Den Teil „es gibt ein Folgenglied, ab dem gilt ...“ können wir umformulieren zu „es gibt eine natürliche Zahl $N$, so dass für alle $a_{n}$ mit $n\geq N$ gilt...“. Somit:

\begin{importantparagraph*}
„Eine Folge $\left(a_{n}\right)_{n\in \mathbb {N} }$ hat einen Grenzwert $a$, wenn es zu jedem $\epsilon >0$ eine natürliche Zahl $N$ gibt, so dass $|a_{n}-a|<\epsilon $ für alle $n\geq N$ ist.

\end{importantparagraph*}

Dies ist dann auch die mathematische Definition des Grenzwerts.

\section{Definition des Grenzwerts}

\begin{definition*}[Grenzwert]
Eine Folge $\left(a_{n}\right)_{n\in \mathbb {N} }$ besitzt einen Grenzwert $a$, wenn es zu jedem $\epsilon >0$ einen Folgenindex $N\in \mathbb {N} $ gibt, so dass für alle Folgenglieder $a_{n}$ mit $n\geq N$ die Ungleichung $|a_{n}-a|<\epsilon $ erfüllt ist.

\end{definition*}

In mathematischer Notation lautet obige Definiton:

\begin{align*}
{\begin{array}{c}{\text{Die Folge }}\left(a_{n}\right)_{n\in \mathbb {N} }{\text{ besitzt einen Grenzwert }}a.\\[1ex]:\Leftrightarrow \\[1ex]\forall \epsilon >0\,\exists N\in \mathbb {N} \,\forall n\geq N:|a_{n}-a|<\epsilon \end{array}}
\end{align*}

In kommentierter Version:

\begin{align*}
{\begin{array}{c}{\text{Eine Folge }}\left(a_{n}\right)_{n\in \mathbb {N} }{\text{ besitzt einen Grenzwert }}a.\\[2ex]\underbrace {:\Leftrightarrow } _{\ldots {\text{ ist per Definition }}\ldots }\\[4ex]\underbrace {{\underset {}{}}\forall \epsilon >0:} _{{\text{Zu jeder Schranke }}\epsilon >0}\ \underbrace {{\underset {}{}}\exists N\in \mathbb {N} :} _{{\text{ existiert ein Index }}N}\ \underbrace {{\underset {}{}}\forall n\geq N:} _{{\text{so dass für alle Indizes }}n\geq N}\ \underbrace {{\underset {}{}}|a_{n}-a|<\epsilon } _{{\text{ der Abstand von }}a_{n}{\text{ zu }}a{\text{ kleiner als }}\epsilon {\text{ ist}}}\end{array}}
\end{align*}

Es folgen einige Definitionen im Zusammenhang mit dem Grenzwert:

\begin{description}[style=nextline]
\item[Konvergenz]
Eine Folge heißt \emph{konvergent}, wenn sie einen Grenzwert besitzt. Man sagt auch, dass eine Folge \emph{gegen $a$ konvergiert}, wenn sie den Grenzwert $a$ besitzt.\item[Divergenz]
Eine Folge nennt man \emph{divergent}, wenn sie keinen Grenzwert besitzt.\item[Nullfolge]
Eine Nullfolge ist eine konvergente Folge mit dem Grenzwert $0$.\end{description}

Wenn eine Folge gegen $a$ konvergiert, schreibt man:

\begin{align*}
\lim _{n\to \infty }a_{n}=a
\end{align*}

oder

\begin{align*}
a_{n}\rightarrow a{\text{ für }}n\rightarrow \infty 
\end{align*}

Man spricht hier „Limes von $a_{n}$ für $n$ gegen unendlich ist $a$“.

\begin{mdframed}[style=semanticbox,frametitleaboveskip=3pt,innerbottommargin=3pt,frametitle=Frage]
Wie lautet die Aussage in Prädikatenlogik dafür, dass $(a_{n})_{n\in \mathbb {N} }$ konvergiert?

\end{mdframed}

\begin{answer*}
Wie oben besprochen, ist die Aussage dafür, dass eine Folge $(a_{n})_{n\in \mathbb {N} }$ den Grenzwert $a$ besitzt, folgende:

\begin{align*}
\forall \epsilon >0\,\exists N\in \mathbb {N} \,\forall n\geq N:|a_{n}-a|<\epsilon 
\end{align*}

Wenn man nur die Konvergenz haben will, dann ist die Aussage entsprechend:

\begin{align*}
\exists a\in \mathbb {R} \,\forall \epsilon >0\,\exists N\in \mathbb {N} \,\forall n\geq N:|a_{n}-a|<\epsilon 
\end{align*}

\end{answer*}

\begin{mdframed}[style=semanticbox,frametitleaboveskip=3pt,innerbottommargin=3pt,frametitle=Frage]
Wie lautet die Aussage in Prädikatenlogik dafür, dass eine Folge $(a_{n})_{n\in \mathbb {N} }$ divergiert?

\end{mdframed}

\begin{answer*}
Hierzu muss obige gefundene Aussage negiert werden.  Es werden dabei All- zu Existenzquantoren und umgekehrt. Die negierte Aussage lautet:

\begin{align*}
\forall a\in \mathbb {R} \,\exists \epsilon >0\,\forall N\in \mathbb {N} \,\exists n\geq N:|a_{n}-a|\geq \epsilon 
\end{align*}

In Worten: Zu jedem $a\in \mathbb {R} $ gibt es eine reelle Zahl $\epsilon >0$, so dass es für alle $N\in \mathbb {N} $ ein $n\geq N$ mit $|a_{n}-a|\geq \epsilon $ gibt.

\end{answer*}

\begin{hint*}
Für den Betrag gilt $|-x|=|x|$. Dementsprechend ist

\begin{align*}
|a_{n}-a|=|-(a-a_{n})|=|a-a_{n}|
\end{align*}

Es ist also egal, ob wir $|a_{n}-a|$ oder $|a-a_{n}|$ in der Definition verwenden.

\end{hint*}

\begin{warning*}
Im Studium begegnet man hin und wieder der Fehlvorstellung „Eine Folge divergiert genau dann, wenn sie unbeschränkt ist.“ Diese Aussage ist \emph{falsch}!

Der intuitive Denkfehler dahinter ist wahrscheinlich oft der voreilige Schluss: „Das Gegenteil von $\forall \epsilon >0\ldots |a_{n}-a|<\epsilon $ ist $\forall \epsilon >0\ \ldots \ |a_{n}-a|\geq \epsilon $, also muss $(a_{n})_{n\in \mathbb {N} }$ beliebig groß werden.“ Dies entspricht aber nicht der hergeleiteten Definition für Divergenz von oben!

Zwar ist jede unbeschränkte Folge divergent (siehe hierzu das Kapitel \href{https://de.wikibooks.org/wiki/Mathe\_für\_Nicht-Freaks:\_Unbeschränkte\_Folgen\_divergieren}
{„Unbeschränkte Folgen divergieren“}), aber nicht jede divergente Folge muss zwangsläufig unbeschränkt sein. Ein Beispiel hierfür ist die Folge $\left((-1)^{n}\right)_{n\in \mathbb {N} }=(-1,1,-1,1,-1,1,\ldots )$, welche beschränkt und divergent ist.

\end{warning*}

\section{Erklärung der Konvergenz}

Neben der obigen Herleitung gibt es eine weitere Intuition für den Grenzwertbegriff: Die Größe $|a_{n}-a|$ ist der Abstand zwischen dem n-ten Folgenglied und $a$. Sie ist ein Maß für den Fehler bzw. Unterschied zwischen $a_{n}$ und $a$. Die Ungleichung $|a_{n}-a|<\epsilon $ bedeutet also, dass der Fehler zwischen $a_{n}$ und $a$ garantiert kleiner als der Maximalfehler $\epsilon $ ist. Damit kann die Definition des Grenzwerts folgendermaßen gedeutet werden: Egal was für einen Maximalfehler $\epsilon >0$ man vorgibt, fast alle Folgenglieder haben einen Unterschied kleiner als $\epsilon $ vom Grenzwert $a$. Der Fehler $|a_{n}-a|$ zwischen den Folgengliedern und dem Grenzwert wird also beliebig klein.

Diese Interpretation kann auch durch die Wahl der Variablen gestützt werden. \href{https://de.wikipedia.org/wiki/Augustin-Louis\%20Cauchy}
{Augustin-Louis Cauchy}, auf den obige Definition zurückgeht, könnte mit $\epsilon $ das französische Wort „erreur“ für „Fehler“ gemeint haben.

\section{Beispiel: Konvergenz der harmonischen Folge}

\begin{figure}[h]
\vspace{\baselineskip}
\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:First_members_of_harmonic_sequence.svg}{\textbf{First\allowbreak\_members\allowbreak\_of\allowbreak\_harmonic\allowbreak\_sequence.svg}} by Stephan Kulla \textit{(CC0)}}\centering
\adjincludegraphics[max width=.5\textwidth, max height=0.2\textheight]{file58first95members95of95harmonic95sequence95a8bfb6a23a9dc3a9083db63c5f821c5769c949f5}
\caption*{Die ersten zehn Folgenglieder der harmonischen Folge (\arabic{imagelabel})}
\end{figure}
Schauen wir uns das Ganze bei der harmonischen Folge mit dem allgemeinen Glied $a_{n}={\tfrac {1}{n}}$ an. Diese konvergiert intuitiv gesehen gegen $0$. Sie müsste also auch die obige Definition für Konvergenz gegen $0$ erfüllen.

Nimm zum Beispiel $\epsilon ={\tfrac {1}{2}}$. Ab dem dritten Folgenglied $a_{3}={\tfrac {1}{3}}$ ist der Abstand von $a_{n}$ zu $0$ kleiner als $\epsilon ={\tfrac {1}{2}}$. Damit liegen ab dem dritten Folgenglied alle weiteren Folgenglieder in der $\epsilon $-Umgebung $]-{\tfrac {1}{2}},{\tfrac {1}{2}}[$. Für $\epsilon ={\tfrac {1}{10}}$ ist der Abstand der Folge zu $0$ ab $n=11$ und für $\epsilon =0,00001$ ab $n=100001$ kleiner als das jeweils gewählte $\epsilon $.

Machen wir das nun ganz allgemein und denken uns ein beliebiges $\epsilon >0$. Aus dem archimedischen Axiom folgt, dass es ein $N\in \mathbb {N} $ gibt, so dass ${\tfrac {1}{n}}<\epsilon $ für alle $n\geq N$ ist. (Siehe \href{https://de.wikibooks.org/wiki/Mathe\_für\_Nicht-Freaks:\_Reelle\_Zahlen}
{Archimedisches Axiom} mit der Wahl von $x=1$ und $y={\frac {2}{\epsilon }}$.) Ab diesem $N$ liegen alle folgenden Folgenglieder in der $\epsilon $-Umgebung $]-\epsilon ,\epsilon [$. Dementsprechend ist der Grenzwert der harmonischen Folge gleich $0$.

\section{Der Grenzwert ist eindeutig}

\begin{theorem*}[Eindeutigkeit des Grenzwerts]
Jede konvergente Folge besitzt nur einen einzigen Grenzwert.

\end{theorem*}

\begin{explanation*}[Eindeutigkeit des Grenzwerts]
Dieser Satz macht Ausdrücke wie $\lim _{n\to \infty }a_{n}$ erst sinnvoll. Stell dir vor, es gäbe eine Folge $\left(b_{n}\right)_{n\in \mathbb {N} }$ mit mehr als einem Grenzwert. Dann könntest du dem Ausdruck $\lim _{n\to \infty }b_{n}$ keine eindeutige Zahl zuordnen, weil du nicht weißt, welchen der Grenzwerte $\lim _{n\rightarrow \infty }b_{n}$ bezeichnen soll. Weil nun aber $\left(b_{n}\right)_{n\in \mathbb {N} }$ maximal einen Grenzwert besitzt, ist stets klar, dass $\lim _{n\to \infty }b_{n}$ diesen eindeutigen Grenzwert bezeichnen soll (unter der Voraussetzung natürlich, dass $\left(b_{n}\right)_{n\in \mathbb {N} }$ konvergiert). Dank des obigen Satzes kann man von „dem Grenzwert“ und nicht nur von „einem Grenzwert“ sprechen.

\end{explanation*}

\begin{solutionprocess*}[Eindeutigkeit des Grenzwerts]
Den Satz werden wir indirekt über einen Widerspruchsbeweis zeigen. Hierzu gehen wir davon aus, dass es eine konvergente Folge $\left(a_{n}\right)_{n\in \mathbb {N} }$ mit zwei verschiedenen Grenzwerten gibt. Diese Annahme müssen wir nun zum Widerspruch führen.

Nennen wir die beiden Grenzwerte $a$ und $b$. Um den Widerspruch zu finden, können wir folgende Methode verwenden: Wir können versuchen, den Gegensatz von dem, was wir eigentlich zeigen wollen, zu beweisen. Dieser Versuch ist natürlich zum Scheitern verurteilt. Wenn wir aber verstehen, warum der Versuch scheitert, dann gibt uns das Hinweise, wie wir den eigentlichen Beweis zu führen haben. Versuchen wir also zu beweisen, dass es eine Folge mit zwei verschiedenen Grenzwerten gibt.

Nimm also ein Blatt Papier und zeichne eine \href{https://de.wikibooks.org/wiki/Mathe\_für\_Nicht-Freaks:\_Zahlengerade}
{Zahlengerade} ein. Markiere nun zwei verschiedene Zahlen auf der Zahlengerade, die unsere zwei Grenzwerte $a$ und $b$ darstellen sollen. Versuche nun eine reelle Folge auf der Zahlengeraden zu finden, die gleichzeitig gegen beide Zahlen konvergiert (Denk daran, dass deine Zahlenfolge ab einem bestimmten Folgenglied in jeder noch so kleinen Umgebung um $a$ beziehungsweise um $b$ sein muss). Für Umgebungen von $a$ und $b$, welche sich nicht überlappen, ist es unmöglich, dass sich dort fast alle Folgenglieder befinden. Die folgende Zeichnung verdeutlicht das Problem.

\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:2_Zahlen_mit_Epsilon_Umgebung.svg}{\textbf{2\allowbreak\_Zahlen\allowbreak\_mit\allowbreak\_Epsilon\allowbreak\_Umgebung.svg}} by Stephan Kulla \textit{(CC-BY-SA-3.0)}}\begin{center}
\adjincludegraphics[max width=0.5\textwidth,max height=0.2\textheight]{file58295zahlen95mit95epsilon95umgebung95a21476dce40f2eb805b6bbe7a5446779cdde2423}\end{center}

Wenn man also $\epsilon $ so klein wählt, dass sich $]a-\epsilon ,a+\epsilon [$ und $]b-\epsilon ,b+\epsilon [$ nicht überschneiden, dann sollte sich ein Widerspruch ergeben. Wir wählen $\epsilon ={\tfrac {|a-b|}{3}}$. Wir wissen, dass es ein $a_{n}$ geben muss, welches sowohl in $]a-\epsilon ,a+\epsilon [$ als auch $]b-\epsilon ,b+\epsilon [$ liegt. Ein solches $a_{n}$ kann es aber nicht geben, weil sich die beiden Umgebungen für $\epsilon ={\tfrac {|a-b|}{3}}$ nicht überschneiden. Einen Widerspruch erhalten wir dann über die Dreiecksungleichung:

\begin{align*}
|a-b|&=|a+\overbrace {(-a_{n}+a_{n})} ^{=0}-b|\\[1ex]&=|(a-a_{n})+(a_{n}-b)|\\[1ex]&\quad {\color {OliveGreen}\left\downarrow \ {\text{Dreiecksungleichung}}\right.}\\[1ex]&\leq |a-a_{n}|+|a_{n}-b|\\[1ex]&<{\frac {|a-b|}{3}}+{\frac {|a-b|}{3}}\\[1ex]&={\frac {2\cdot |a-b|}{3}}
\end{align*}

Nach Kürzung beider Seiten mit $|a-b|$ haben wir den Widerspruch $1<{\tfrac {2}{3}}$.

\end{solutionprocess*}

\begin{proof*}[Eindeutigkeit des Grenzwerts]
\textbf{Widerspruchsbeweis:} Sei $\left(a_{n}\right)_{n\in \mathbb {N} }$ eine Folge mit mindestens zwei verschiedenen Grenzwerten $a$ und $b$ . Damit ist $|a-b|>0$. Per Definition des Grenzwertes gibt es zwei natürliche Zahlen $N_{1}$ und $N_{2}$ mit

\begin{align*}
\forall n\geq N_{1}:\ |a_{n}-a|<{\frac {|a-b|}{3}}
\end{align*}

und

\begin{align*}
\forall n\geq N_{2}:\ |a_{n}-b|<{\frac {|a-b|}{3}}
\end{align*}

Dies folgt aus der Definition des Grenzwerts mit $\epsilon ={\tfrac {|a-b|}{3}}$. Damit gilt für alle Folgenglieder $a_{n}$ mit $n\geq \mathrm {max} \{N_{1},\,N_{2}\}$, dass gleichzeitig $|a_{n}-a|<{\tfrac {|a-b|}{3}}$ und $|a_{n}-b|<{\tfrac {|a-b|}{3}}$ ist. In diesem Fall wäre also

\begin{align*}
|a-b|&=|a+(-a_{n}+a_{n})-b|\\[1ex]&=|(a-a_{n})+(a_{n}-b)|\\[1ex]&\quad {\color {OliveGreen}\left\downarrow \ {\text{Dreiecksungleichung}}\right.}\\[1ex]&\leq |a-a_{n}|+|a_{n}-b|\\[1ex]&<{\frac {|a-b|}{3}}+{\frac {|a-b|}{3}}\\[1ex]&={\frac {2\cdot |a-b|}{3}}
\end{align*}

Wegen $a\neq b$ ist $|a-b|>0$ und wir können beide Seiten der obigen Ungleichung durch $|a-b|$ dividieren. So erhalten wir den Widerspruch

\begin{align*}
1<{\frac {2}{3}}
\end{align*}

Beachte, wie wichtig es für den Beweis ist, dass $a\neq b$ und damit $|a-b|>0$ ist. Sonst hätten wir nicht beide Seiten durch $|a-b|$ dividieren können und wir hätten nicht $\epsilon ={\tfrac {|a-b|}{3}}>0$ wählen können.

\end{proof*}

\includepdf[pages=-]{predesigned_pages/mfnf_fundraising}

\chapter{Konvergenz und Divergenz beweisen}

In diesem Kapitel wird erläutert, wie man die Konvergenz und Divergenz einer Folge beweisen kann. Normalerweise teilt sich diese Arbeit in zwei Arbeitsschritte auf: Zunächst versucht man auf einem Schmierblatt, eine Beweisidee zu finden, die man danach im zweiten Schritt in einem Beweis umsetzt und ins Reine schreibt. Dabei ist oftmals der Lösungsweg auf dem Schmierblatt ein völlig anderer als die letztendliche Beweisargumentation. Dies werden wir auch bei den Beispielaufgaben in diesem Kapitel sehen.

Jedoch gibt es kein Schema F zur Lösung von Grenzwertaufgaben! Auch wenn ich dir in diesem Kapitel einige Tipps und Tricks mit an die Hand gebe und dir im Studium auch immer wieder neue Lösungen für Konvergenzaufgaben begegnen werden, wirst du auf Übungsaufgaben stoßen, bei denen die bisher gelernten Lösungsstrategien nicht funktionieren. Hier musst du selbst kreativ werden und auf Basis der dir bereits bekannten Sätze versuchen, neue Lösungswege zu finden. Dies ist aber gewollt. Denn du sollst im Mathematikstudium lernen, innovative Lösungsstrategien für neue Problemtypen zu entwickeln {\DejaSans 😊}.

\section{Beweise für Konvergenz führen}

\subsection{Allgemeine Beweisstruktur}

Bevor wir uns einer konkreten Beispielaufgabe zuwenden, ist es sinnvoll, die allgemeine Beweisstruktur für die Konvergenz einer Folge zu verstehen. So weiß man nämlich, wie der finale Beweis aussehen muss. Aus dem letzten Kapitel wissen wir, dass die Konvergenz der Folge $(a_{n})_{n\in \mathbb {N} }$ gegen $a$ durch folgende Aussage beschrieben wird:

\begin{align*}
{\color {Red}\forall \epsilon >0}\ {\color {Orange}\exists N\in \mathbb {N} }\ {\color {OliveGreen}\forall n\geq N}\ {\color {Blue}|a_{n}-a|<\epsilon }
\end{align*}

Diese Aussage gibt die allgemeine Beweisstruktur vor:

\begin{importantparagraph*}
{\textcolor{Red}{Sei ${\color {Red}\epsilon >0}$ beliebig.}} {\textcolor{Orange}{Wähle ${\color {Orange}N=\ldots }$. Die Zahl ${\color {Orange}N}$ existiert, weil …}} {\textcolor{OliveGreen}{Sei ${\color {OliveGreen}n\geq N}$ beliebig.}} {\textcolor{Blue}{Es ist:}}

\begin{align*}
{\color {Blue}|a_{n}-a|<\ldots <\epsilon }
\end{align*}

\end{importantparagraph*}

Anhand der Farben sehen wir, welche Teile des Beweises zu welchem Teil der zu beweisenden Aussage gehören. Der Satz „$N$ existiert, weil…“ kann im Übrigen entfallen, wenn dies offensichtlich ist. Dies ist zum Beispiel der Fall, wenn $N$ explizit angegeben wird und klar ist, dass $N$ eine natürliche Zahl ist.

\subsection{Beispielaufgabe und allgemeines Vorgehen}

Die Beispielaufgabe lautet

\begin{importantparagraph*}
„Konvergiert die Folge $(a_{n})_{n\in \mathbb {N} }$ mit $a_{n}={\tfrac {n}{n+1}}$? Wenn ja, gegen welchen Grenzwert? Beweise alle deine Behauptungen.“

\end{importantparagraph*}

Der Lösungsweg involviert folgende Schritte:

\begin{enumerate}
\item Grenzwert finden
\item notwendige Beweisschritte auf Schmierblatt finden
\item Beweis nach obiger Beweisstruktur aufschreiben
\end{enumerate}

\subsection{Grenzwert finden}

Zunächst müssen wir bestimmen, ob die Folge $a_{n}={\tfrac {n}{n+1}}$ konvergiert und welchen Grenzwert sie im Fall der Konvergenz besitzt. Hierzu bieten sich folgende Techniken an:

\begin{itemize}
\item \textbf{Erste Folgenglieder berechnen:} Du kannst die ersten Folgenglieder berechnen und gegebenenfalls in ein Diagramm einzeichnen. Möglicherweise bekommst du so schon Ideen über die Eigenschaften der Folge und über einen möglichen Grenzwert.
\item \textbf{Große Folgenglieder ausrechnen:} Mit einem Taschenrechner oder einem Computer kannst du sehr große Folgenglieder ausrechnen. Liegen all diese Folgenglieder in der Nähe einer bestimmten reellen Zahl? Dann könnte diese Zahl der Grenzwert der Folge sein.
\item \textbf{Mutmaßungen anstellen:} Du kannst deine Intuition verwenden, um den Grenzwert zu erraten. Du kannst aber auch Überlegungen anstellen, was der Grenzwert sein müsste.
\end{itemize}

Fangen wir also damit an, die ersten zehn Folgenglieder von $a_{n}={\tfrac {n}{n+1}}$ zu berechnen:


\renewcommand{\arraystretch}{1.5}

\begin{longtabu} to \linewidth {X[l]X[l]} \\ \toprule 
$n$ & ${\tfrac {n}{n+1}}$ \\ 
\midrule
1 & 0,5 \\ 
2 & 0,666… \\ 
3 & 0,75 \\ 
4 & 0,8 \\ 
5 & 0,833… \\ 
6 & 0,857… \\ 
7 & 0,875 \\ 
8 & 0,888… \\ 
9 & 0,9 \\ 
10 & 0,909… \\ 
\bottomrule
\end{longtabu}
\renewcommand{\arraystretch}{1.0}
Diese können wir in einem Diagramm einzeichnen:

\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Folge_n_durch_n+1.svg}{\textbf{Folge\allowbreak\_n\allowbreak\_durch\allowbreak\_n+1.svg}} by Stephan Kulla \textit{(CC0)}}\begin{center}
\adjincludegraphics[max width=0.5\textwidth,max height=0.2\textheight]{file58folge95n95durch95n43195753aab80539c38961945e602d980b662baabb92e}\end{center}

Wir sehen, dass die ersten Folgenglieder monoton steigen, wobei der Anstieg zwischen den Folgengliedern immer kleiner wird. Wir können deswegen vermuten, dass die Folge konvergiert. Ein klarer Kandidat für einen Grenzwert ist noch nicht erkennbar. Hierfür können wir hohe Folgenglieder ausrechnen, weil diese in der Nähe des Grenzwerts liegen müssten. Es ist

\begin{align*}
a_{1000}=0,99900099900\ldots 
\end{align*}

und

\begin{align*}
a_{1000000}=0,99999900000099\ldots 
\end{align*}

Große Folgenglieder liegen also in der Nähe von $1$ und deswegen liegt die Hypothese nahe, dass $1$ der Grenzwert der betrachteten Folge ist. Aber auch folgende Überlegungen stützen diese Hypothese: Wenn $n$ sehr groß ist, dann ist $n+1\approx n$, weil die Addition von eins bei großen Zahlen kaum etwas am Wert ändert. Es müsste also gelten

\begin{align*}
{\tfrac {n}{n+1}}\approx {\tfrac {n}{n}}=1
\end{align*}

Wegen diesen Betrachtungen kommen wir zur Hypothese, dass $1$ der Grenzwert der Folge $a_{n}={\tfrac {n}{n+1}}$ ist.

\begin{warning*}
Obige Argumentationen erfüllen nicht die Voraussetzungen eines gültigen Beweises. Durch sie kann nur eine Vermutung gewonnen werden, was der Grenzwert einer Folge sein könnte. Einen Beweis musst du danach immer gesondert führen.

\end{warning*}

\subsection{Beweisschritte finden}

Der Kern des Beweises ist die Abschätzung $|a_{n}-a|<\ldots <\epsilon $. Um diese zu finden, fängt man am Besten mit dem Betrag $|a_{n}-a|$ an und versucht diesen so lange zu vereinfachen und nach oben abzuschätzen, bis man einen Ausdruck findet, der kleiner als $\epsilon $ ist. Bei den Abschätzungen dürfen wir beliebige Bedingungen für $n$ der Form $n\geq N$ stellen, wobei $N$ eine natürliche Zahl ist, die nur von $\epsilon $ und $a$ abhängen darf ($N$ darf also nicht von $a_{n}$ abhängen!).

Auch kann man probieren, $|a_{n}-a|<\epsilon $ nach $n$ umzustellen, um die gewünschte Bedingung für $n$ zu finden. Jedoch muss man hier darauf achten, dass man nur Äquivalenzumformungen verwendet. Am Ende müssen nämlich alle Umformungen auch in die Gegenrichtung geführt werden können, damit man im Beweis aus $n\geq \ldots $ wieder die Zielungleichung $|a_{n}-a|<\epsilon $ zeigen kann. In diesem und nächsten Kapitel sind dafür einige Beispiele.



Kehren wir zur obigen Beispielaufgabe zurück und fangen an, $\left|1-{\tfrac {n}{n+1}}\right|$ zu vereinfachen:

\begin{align*}
\left|1-{\frac {n}{n+1}}\right|&=\left|{\frac {n+1}{n+1}}-{\frac {n}{n+1}}\right|=\left|{\frac {n+1-n}{n+1}}\right|\\[0.5em]&=\left|{\frac {1}{n+1}}\right|={\frac {1}{n+1}}
\end{align*}

Von diesem Ausdruck wissen wir aufgrund des archimedischen Axioms, dass er irgendwann kleiner als $\epsilon $ ist. Das archimedische Axiom fordert nämlich, dass es für alle $\epsilon >0$ ein $M\in \mathbb {N} $ gibt, so dass ${\tfrac {1}{m}}<\epsilon $ für alle $m\geq M$ ist. Damit ${\tfrac {1}{n+1}}<\epsilon $ ist, muss für unser $n$ also die Bedingung

\begin{align*}
{\begin{array}{rrl}&n+1&\geq M\\\Leftrightarrow \ &n&\geq M-1\end{array}}
\end{align*}

erfüllt sein. Damit haben wir die gewünschte Abschätzung mit der einzigen Bedingung $n\geq M-1$. Wir wählen im Beweis also $N=M-1$, wobei $M$, wie oben genannt, mit dem archimedischen Axiom gewählt wird.

\subsection{Beweis aufschreiben}

Wir schreiben nun den Beweis ins Reine (zur Übung kannst du selbst probieren, den Beweis nach dem obigen Schema aufzuschreiben):

\begin{proof*}
Sei $\epsilon >0$ beliebig. Nach dem archimedischen Axiom gibt es ein $M\in \mathbb {N} $ mit ${\tfrac {1}{m}}<\epsilon $ für alle $m\geq M$. Wähle $N=M-1$. Sei $n\geq N$ beliebig. Es ist

\begin{align*}
\left|1-{\frac {n}{n+1}}\right|&=\left|{\frac {n+1}{n+1}}-{\frac {n}{n+1}}\right|=\left|{\frac {n+1-n}{n+1}}\right|\\[0.5em]&=\left|{\frac {1}{n+1}}\right|={\frac {1}{n+1}}<\epsilon 
\end{align*}

\end{proof*}

Wenn wir den Beweis und den Lösungsweg miteinander vergleichen, dann sehen wir, dass sie völlig verschieden formuliert sind. Im Beweis scheint die Wahl von $M$ und $N$ vom Himmel zu fallen, weil ohne bekannten Lösungsweg nicht klar ist, warum man diese Zahlen so wählen sollte. Dies zeigt, dass man niemals den Beweis eines Mathematikers mit dem Lösungsweg zum Beweis verwechseln sollte!

\subsection{Übungsaufgabe}

Wir empfehlen euch, genau wie eben beschrieben, die folgende Aufgabe zu versuchen.

\begin{exercise*}[Konvergenz einer Folge]
Beweise, dass die Folge $(a_{n})_{n\in \mathbb {N} }$ mit

\begin{align*}
a_{n}={\frac {1-2n}{5+3n}}
\end{align*}

konvergiert. Wie lautet ihr Grenzwert?

\end{exercise*}

\begin{solutionprocess*}[Konvergenz einer Folge]
Wir gegehen genau wie oben beschrieben vor. Zunächst benötigen wir einen Grenzwert:

\proofstep{Lösungsschritt:}
 Grenzwert finden\begin{indentblock}
Wir können wie oben die ersten Folgenglieder ausrechnen, oder wir überlegen uns gleich folgendes: Für sehr große $n$ gilt für den Zähler der Folge $1-2n\approx -2n$, und für den Nenner $5+3n\approx 3n$. Insgesamt gilt daher

\begin{align*}
a_{n}={\frac {1-2n}{5+3n}}\approx {\frac {-2n}{3n}}=-{\frac {2}{3}}
\end{align*}

falls $n$ sehr groß ist. Unsere starke Vermutung ist somit, dass $(a_{n})$ gegen den Grenzwert $-{\tfrac {2}{3}}$ konvergiert.

\end{indentblock}

Nun folgt die rechnerische Vorarbeit, um anschließend den Beweis sauber aufschreiben zu können:

\proofstep{Lösungsschritt:}
 Nötige Beweisschritte finden\begin{indentblock}
Laut der Definition der Konvergenz müssen wir zu jedem $\epsilon >0$ ein $N\in \mathbb {N} $ finden, so dass für alle $n\geq N$ gilt: $|a_{n}-(-{\tfrac {2}{3}})|<\epsilon $. Dazu vereinfachen wir den Ausdruck $|a_{n}-(-{\tfrac {2}{3}})|$ zunächst:

\begin{align*}
|a_{n}-(-{\tfrac {2}{3}})|&=\left|{\frac {1-2n}{5+3n}}+{\frac {2}{3}}\right|\\[0.3em]&=\left|{\frac {3(1-2n)}{3(5+3n)}}+{\frac {2(5+3n)}{3(5+3n)}}\right|\\[0.3em]&=\left|{\frac {3(1-2n)+2(5+3n)}{3(5+3n)}}\right|\\[0.3em]&=\left|{\frac {3-6n+10+6n}{15+9n}}\right|\\[0.3em]&=\left|{\frac {13}{15+9n}}\right|\\[0.3em]&={\frac {13}{15+9n}}
\end{align*}

Nun formen wir die Ungleichung ${\tfrac {13}{15+9n}}<\epsilon $ um, zu einer Ungleichung der Form $n>\ldots $:

\begin{align*}
&{\frac {13}{15+9n}}<\epsilon \\[0.3em]\iff &13<\epsilon (15+9n)\\[0.3em]\iff &13<15\epsilon +9n\epsilon \\[0.3em]\iff &13-15\epsilon <9n\epsilon \\[0.3em]\iff &{\frac {13-15\epsilon }{9\epsilon }}<n\\[0.3em]\iff &n>{\frac {13-15\epsilon }{9\epsilon }}
\end{align*}

Damit haben wir eine passende Bedingung für $n$, und damit auch $N$ gefunden. Wählen wir nämlich $N>{\tfrac {13-15\epsilon }{9\epsilon }}$, was nach dem archimedischen Axiom möglich ist, so folgt aus dem eben hergeleiteten für alle $n\geq N$: $|a_{n}-(-{\tfrac {2}{3}})|={\tfrac {13}{15+9n}}<\epsilon $.

\end{indentblock}

Damit sind wir mit unserer Vorarbeit fertig, und müssen den Beweis nur noch in „Mathematikerdeutsch“ formulieren.

\end{solutionprocess*}

\begin{proof*}[Konvergenz einer Folge]
Sei $\epsilon >0$ beliebig. Nach dem archimedischen Axiom gibt es ein $N\in \mathbb {N} $ mit $N>{\frac {13-15\epsilon }{9\epsilon }}$. Sei $n\geq N$ beliebig. Dann ist

\begin{align*}
\left|{\frac {1-2n}{5+3n}}-\left(-{\frac {2}{3}}\right)\right|&=\left|{\frac {3(1-2n)}{3(5+3n)}}+{\frac {2(5+3n)}{3(5+3n)}}\right|=\left|{\frac {13}{15+9n}}\right|\\[0.5em]&={\frac {13}{15+9n}}<\epsilon 
\end{align*}

\end{proof*}

\section{Beweise für Divergenz führen}

\subsection{Allgemeine Beweisstruktur}

Die Divergenz einer Folge tritt per Definition genau dann ein, wenn die Folge nicht konvergent ist. Die aussagenlogische Formulierung von Divergenz ist also genau die Negation der Konvergenz-Definition. Dafür tauschen wir alle Quantoren aus und ändern im Teil nach den Quantoren $<$ zu $\geq $. (Analog würden wir bei Negation $>$ zu $\leq $ und $=$ zu $\neq $ umändern.) Bei Divergenz der Folge $(a_{n})_{n\in \mathbb {N} }$ haben wir also folgende Aussage zu beweisen:

\begin{align*}
{\color {Red}\forall a\in \mathbb {R} }\ {\color {Orange}\exists \epsilon >0}\ {\color {Plum}\forall N\in \mathbb {N} }\ {\color {OliveGreen}\exists n\geq N}{\color {Blue}|a_{n}-a|\geq \epsilon }
\end{align*}

Die damit verbundene Beweisstruktur ist:

\begin{importantparagraph*}
{\textcolor{Red}{Sei ${\color {Red}a\in \mathbb {R} }$ beliebig.}} {\textcolor{Orange}{Wähle ${\color {Orange}\epsilon =\ldots }$. Die Zahl ${\color {Orange}\epsilon }$ existiert und ist positiv, weil …}} {\textcolor{Plum}{Sei ${\color {Plum}N\in \mathbb {N} }$ beliebig.}} {\textcolor{OliveGreen}{Wähle ${\color {OliveGreen}n=\ldots }$. Die Zahl ${\color {OliveGreen}n}$ existiert, und es ist ${\color {OliveGreen}n\geq N}$, weil …}} {\textcolor{Blue}{Es ist}}

\begin{align*}
{\color {Blue}|a_{n}-a|\geq \ldots \geq \epsilon }
\end{align*}

\end{importantparagraph*}

Auch hier können Teile des Beweisschemas weggelassen werden, wenn sie offensichtlich sind. Jedoch muss die grundlegende Beweisstruktur erhalten bleiben.

\subsection{Beispielaufgabe}

\begin{figure}[h]
\vspace{\baselineskip}
\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Geometrische_Folge.svg}{\textbf{Geometrische\allowbreak\_Folge.svg}} by Stephan Kulla \textit{(CC BY-SA 3.0)}}\centering
\adjincludegraphics[max width=.5\textwidth, max height=0.2\textheight]{file58geometrische95folge95651a5c41481f267934607d64e90536ad4d99de86}
\caption*{Die Folge $(2^{n})_{n\in \mathbb {N} }$ (\arabic{imagelabel})}
\end{figure}
Schauen wir uns den Divergenzbeweis exemplarisch an folgender Aufgabe an:

\begin{importantparagraph*}
„Divergiert die Folge $(a_{n})_{n\in \mathbb {N} }$ mit $a_{n}=2^{n}$? Beweise deine Behauptung.“

\end{importantparagraph*}

Auch hier können wir mit den obigen Techniken (erste Folgenglieder berechnen, große Folgenglieder ausrechnen usw.) eine Vermutung aufstellen, ob diese Folge divergiert. Wir sehen aber schnell, dass die Folge über alle Grenzen hinweg wächst und sich dabei keiner reellen Zahl annähert. Die Folge $(a_{n})_{n\in \mathbb {N} }$ sollte also divergieren. Jetzt versuchen wir, einen Beweis für diese Behauptung zu finden.

\subsection{Lösungsweg}

Kern des späteren Beweises ist die zu zeigende Ungleichungskette

\begin{align*}
|a_{n}-a|\geq \ldots \geq \epsilon 
\end{align*}

Starten wir also wieder mit dem Betrag $|a_{n}-a|$. Auf einem Schmierblatt versuchen wir diesen Ausdruck so lange zu vereinfachen und nach unten abzuschätzen, bis wir einen Term $\epsilon >0$ haben. $a$ ist dabei beliebig vorgegeben und wir können keinen Einfluss auf den Wert von $a$ nehmen. Schließlich müssen wir den Beweis \emph{für alle} Zahlen $a\in \mathbb {R} $ führen.

Jedoch können wir $\epsilon $ und $n$ frei wählen. Es muss nur gesichert sein, dass $\epsilon >0$ und $n\geq N$ ist, wobei $N$ eine beliebige natürliche Zahl ist. Da $\epsilon $ nach $a$ im Beweis eingeführt wird, darf $\epsilon $ von $a$ abhängen (jedoch nicht von $n$). Die natürliche Zahl $n$ darf sowohl von $a$, als auch von $\epsilon $ abhängen. Wir können also während der Abschätzung nach unten beliebige Bedingungen an $\epsilon $ und $n$ sammeln. Diese Bedingungen werden zum Schluss ähnlich wie beim Konvergenzbeweis zusammengefasst.

Fangen wir also an mit $|2^{n}-a|$. Um den Term zu vereinfachen, können wir $2^{n}\geq a$ fordern, weil wir dann den Betrag weglassen können. Dass für ein $n\in \mathbb {N} $ die Ungleichung $2^{n}\geq a$ erfüllt ist, erhalten wir aus den \href{https://de.wikibooks.org/wiki/Mathe\_für\_Nicht-Freaks:\_Bernoulli-Ungleichung\#Anker:Folgerung}
{Folgerungen der Bernoulli-Ungleichung}. Eine davon besagt:

\begin{importantparagraph*}
„Für jede Zahl $p>1$ und jede Zahl $M\in \mathbb {R} $ gibt es ein $n\in \mathbb {N} $, so dass $p^{n}>M$ ist.“

\end{importantparagraph*}

Wir müssen nur $M=a$ und $p=2$ setzen. So erhalten wir mit der Bedingung $2^{n}\geq a$:

\begin{align*}
|2^{n}-a|=2^{n}-a
\end{align*}

Nun müssen wir $2^{n}-a\geq \epsilon $ beweisen, also formen wir dies durch Äquivalenzumformungen um:

\begin{align*}
{\begin{array}{rrl}&2^{n}-a&\geq \epsilon \\\Leftrightarrow \ &2^{n}&\geq a+\epsilon \end{array}}
\end{align*}

So erhalten wir die neue Bedingung $2^{n}\geq a+\epsilon $, womit wir die letzte Ungleichung beweisen können. Für $\epsilon $ haben wir noch keine Bedingungen und können damit diese Zahl frei wählen. Dass es tatsächlich für jedes $\epsilon $ ein $n$ gibt mit $2^{n}\geq a+\epsilon $ , liegt daran, dass wir die Folgerung aus der Bernoulli-Ungleichung auch mit $M=a+\epsilon $ benutzen können. Wir müssen nur aufpassen, dass $\epsilon >0$ ist. So wählen wir einfach $\epsilon =1$. Für $n$ haben wir die beiden Bedingungen $2^{n}\geq a$ und $2^{n}\geq a+\epsilon =a+1$. Also wählen wir $2^{n}\geq \max\{a,\,a+1\}=a+1$, um beide Bedingungen zusammenzufassen.

\subsection{Beweis aufschreiben}

Nun haben wir alle notwendigen Schritte, um den Beweis zu führen:

\begin{proof*}
Sei $a\in \mathbb {R} $ beliebig. Wähle $\epsilon =1$. Sei $N\in \mathbb {N} $ beliebig. Wähle $n\geq N$ so, dass $2^{n}\geq a+1$ ist. Dies ist aufgrund der Folgerungen aus der Bernoulli-Ungleichung möglich. Es ist nun

\begin{align*}
|2^{n}-a|&=2^{n}-a\geq (a+1)-a=1=\epsilon 
\end{align*}

\end{proof*}

\chapter{Beispiele für Grenzwerte}

\section{Übersicht über wichtige Grenzwerte}

\begin{itemize}
\item $\lim _{n\rightarrow \infty }c=c$ für alle $c\in \mathbb {R} $
\item $\lim _{n\rightarrow \infty }{\tfrac {1}{n}}=0$
\item $\lim _{n\rightarrow \infty }{\tfrac {1}{n^{k}}}=0$ für alle $k\in \mathbb {N} $
\item $\lim _{n\rightarrow \infty }{\tfrac {1}{\sqrt[{k}]{n}}}=0$ für alle $k\in \mathbb {N} $
\item $\lim _{n\rightarrow \infty }q^{n}=0$ für alle $q\in \mathbb {R} $ mit $|q|<1$
\item $\lim _{n\rightarrow \infty }{\sqrt[{n}]{c}}=1$ für alle $c>0$
\item $\lim _{n\rightarrow \infty }{\sqrt[{n}]{n}}=1$
\item $\lim _{n\rightarrow \infty }{\tfrac {n^{k}}{z^{n}}}=0$ für alle $k\in \mathbb {N} $ und $z\in \mathbb {R} $ mit $|z|>1$
\item $\lim _{n\rightarrow \infty }n^{k}q^{n}=0$ für alle $k\in \mathbb {N} $ und $q\in \mathbb {R} $ mit $|q|<1$
\item $\lim _{n\rightarrow \infty }{\tfrac {z^{n}}{n!}}=0$ für alle $z\in \mathbb {R} $ mit $|z|>1$
\item $\lim _{n\rightarrow \infty }{\tfrac {n!}{n^{n}}}=0$
\item $\lim _{n\rightarrow \infty }\left(1+{\tfrac {1}{n}}\right)^{n}=e$
\end{itemize}



\begin{hint*}
In der Analysis ist es sehr wichtig, das Wachstumsverhalten verschiedener Folgen einschätzen zu können. So folgt aus dem Grenzwert $\lim _{n\rightarrow \infty }{\tfrac {n^{k}}{z^{n}}}=0$, dass die Folge $(z^{n})_{n\in \mathbb {N} }$ viel schneller wächst als die Folge $(n^{k})_{n\in \mathbb {N} }$. Schreiben wir $a_{n}\ll b_{n}$ für $\lim _{n\to \infty }{\tfrac {a_{n}}{b_{n}}}=0$, folgt aus den obigen Grenzwerten:

\begin{align*}
n^{k}\ll z^{n}\ll n!\ll n^{n}
\end{align*}

\end{hint*}

\clearpage

\section{Konstante Folge}

\begin{theorem*}[Grenzwert der konstanten Folge]
Jede konstante Folge konvergiert gegen den Wert ihrer Folgenglieder:

\begin{align*}
\lim _{n\rightarrow \infty }c=c
\end{align*}

\end{theorem*}

\begin{proof*}[Grenzwert der konstanten Folge]
Sei $a_{n}=c$ eine konstante Folge und sei $\epsilon >0$ beliebig. Für alle $n\in \mathbb {N} $ (und damit für fast alle $n\in \mathbb {N} $) gilt

\begin{align*}
|a_{n}-c|=|c-c|=0<\epsilon 
\end{align*}

\end{proof*}

\section{Geometrische Folge}

\begin{theorem*}
Für alle $q\in \mathbb {R} $ mit $|q|<1$ ist $\lim _{n\rightarrow \infty }q^{n}=0$ .

\end{theorem*}

\begin{proof*}
Sei $\epsilon >0$ und $q\in \mathbb {R} $ mit $|q|<1$ beliebig. Über die Bernoulli-Ungleichung kann man beweisen, dass es ein $N\in \mathbb {N} $ mit $|q|^{N}<\epsilon $ gibt. Dann gilt für alle $n\geq N$:

\begin{align*}
\left|q^{n}-0\right|&=\left|q^{n}\right|=|q|^{n}=\underbrace {|q|^{n-N}} _{\leq \ 1}\cdot \underbrace {|q|^{N}} _{<\ \epsilon }<\epsilon 
\end{align*}

\end{proof*}

\section{n-te Wurzel von n}

\begin{theorem*}[Grenzwert n-te Wurzel von n]
Es ist $\lim _{n\rightarrow \infty }{\sqrt[{n}]{n}}=1$.

\end{theorem*}

\begin{solutionprocess*}[Grenzwert n-te Wurzel von n]
Der abzuschätzende Betrag ist $\left|{\sqrt[{n}]{n}}-1\right|<\epsilon $. Auch hier können wir versuchen, die Ungleichung umzustellen, um Bedingungen für $n$ zu finden:

\begin{align*}
{\begin{array}{rrl}&\left|{\sqrt[{n}]{n}}-1\right|&<\epsilon \\[0.3em]&&{\color {OliveGreen}\left\downarrow \ {\sqrt[{n}]{n}}\geq 1\right.}\\[0.3em]\Leftrightarrow \ &{\sqrt[{n}]{n}}-1&<\epsilon \\\Leftrightarrow \ &{\sqrt[{n}]{n}}&<1+\epsilon \\\Leftrightarrow \ &n&<(1+\epsilon )^{n}\\\end{array}}
\end{align*}

$(1+\epsilon )^{n}$ und $n$ wachsen beide über alle Grenzen hinaus. Wir müssen also zeigen, dass $(1+\epsilon )^{n}$ irgendwann größer wird als $n$. Sehen wir uns dazu $(1+\epsilon )^{n}$ an. Nach dem binomischen Lehrsatz ist

\begin{align*}
(1+\epsilon )^{n}=\sum _{k=0}^{n}{\binom {n}{k}}\epsilon ^{k}=1+n\epsilon +{\frac {n(n-1)}{2}}\epsilon ^{2}+\ldots +\epsilon ^{n}
\end{align*}

Jeder Summand dieser Summe ist größer-gleich Null. Wenn wir zeigen können, dass $n$ kleiner als eine Teilsumme von $1+n\epsilon +{\frac {n(n-1)}{2}}\epsilon ^{2}+\ldots +\epsilon ^{n}$ ist, wissen wir, dass $n$ kleiner als $1+n\epsilon +{\frac {n(n-1)}{2}}\epsilon ^{2}+\ldots +\epsilon ^{n}=(1+\epsilon )^{n}$ ist. Wir wählen die Summanden $1$ und ${\tfrac {n(n-1)}{2}}\epsilon ^{2}$ (wir werden gleich sehen, dass diese beiden Summanden ausreichen) und zeigen

\begin{align*}
n<1+{\frac {n(n-1)}{2}}\epsilon ^{2}
\end{align*}

 Indem wir \begin{align*}
n<1+{\frac {n(n-1)}{2}}\epsilon ^{2}
\end{align*}

zeigen, wobei gleichzeitig

\begin{align*}
1+{\frac {n(n-1)}{2}}\epsilon ^{2}\leq 1+n\epsilon +{\frac {n(n-1)}{2}}\epsilon ^{2}+\ldots +\epsilon ^{n}=(1+\epsilon )^{n}
\end{align*}

gilt, zeigen wir auch

\begin{align*}
n<(1+\epsilon )^{n}
\end{align*}

Um eine Bedingung für $n$ zu finden, formen wir $n<1+{\tfrac {n(n-1)}{2}}\epsilon ^{2}$ um:

\begin{align*}
{\begin{array}{rrl}&n&<1+{\frac {n(n-1)}{2}}\epsilon ^{2}\\[0.3em]\Leftrightarrow \ &n-1&<{\frac {n(n-1)}{2}}\epsilon ^{2}\\[0.3em]\Leftrightarrow \ &1&<{\frac {n}{2}}\epsilon ^{2}\\[0.3em]\Leftrightarrow \ &2&<n\epsilon ^{2}\\[0.3em]\Leftrightarrow \ &{\frac {2}{\epsilon ^{2}}}&<n\end{array}}
\end{align*}

Über das archimedische Axiom finden wir ein $N$, sodass ${\tfrac {2}{\epsilon ^{2}}}<n$ für alle $n\geq N$ gilt.

\end{solutionprocess*}

\begin{proof*}[Grenzwert n-te Wurzel von n]
Sei $\epsilon >0$ beliebig. Wir wählen $N\in \mathbb {N} $ nach dem archimedischen Axiom so, dass $N>{\tfrac {2}{\epsilon ^{2}}}$ ist. Sei $n\geq N$ beliebig. Es ist

\begin{align*}
{\begin{array}{rrl}&{\frac {2}{\epsilon ^{2}}}&<n\\[0.3em]\Rightarrow \ &2&<n\epsilon ^{2}\\[0.3em]\Rightarrow \ &1&<{\frac {n}{2}}\epsilon ^{2}\\[0.3em]\Rightarrow \ &n-1&<{\frac {n(n-1)}{2}}\epsilon ^{2}\\[0.3em]\Rightarrow \ &n&<1+{\frac {n(n-1)}{2}}\epsilon ^{2}\\[0.3em]&&{\color {OliveGreen}\left\downarrow \ 1+{\frac {n(n-1)}{2}}\epsilon ^{2}\leq \sum _{k=0}^{n}{\binom {n}{k}}\epsilon ^{k}=(1+\epsilon )^{n}\right.}\\[0.3em]\Rightarrow \ &n&<(1+\epsilon )^{n}\\[0.3em]\Rightarrow \ &{\sqrt[{n}]{n}}&<1+\epsilon \\[0.3em]\Rightarrow \ &{\sqrt[{n}]{n}}-1&<\epsilon \\[0.3em]&&{\color {OliveGreen}\left\downarrow \ {\sqrt[{n}]{n}}\geq 1\right.}\\[0.3em]\Rightarrow \ &\left|{\sqrt[{n}]{n}}-1\right|&<\epsilon \\[0.3em]\end{array}}
\end{align*}

\end{proof*}

\section{Fakultätfolge durch $n^{n}$}

\begin{theorem*}
Es gilt $\lim _{n\rightarrow \infty }{\tfrac {n!}{n^{n}}}=0$.

\end{theorem*}

\begin{proof*}
Sei $\epsilon >0$. Wähle $N\in \mathbb {N} $ so, dass $N\geq {\tfrac {1}{\epsilon }}+1$. Für alle $n\geq N$ gilt dann

\begin{align*}
\left|{\frac {n!}{n^{n}}}-0\right|&={\frac {n!}{n^{n}}}={\frac {1}{n}}\cdot {\frac {2\cdot 3\cdot \ldots \cdot n}{n\cdot n\cdot \ldots \cdot n}}\\[0.3em]&={\frac {1}{n}}\cdot \prod _{k=2}^{n}{\frac {k}{n}}\\[0.3em]&{\color {OliveGreen}\left\downarrow \ {\frac {k}{n}}\leq 1{\text{ für alle }}k\in \{2,\ldots ,n\}{\text{ und somit }}\prod _{k=2}^{n}{\frac {k}{n}}\leq 1\right.}\\[0.3em]&\leq {\frac {1}{n}}\leq {\frac {1}{{\tfrac {1}{\epsilon }}+1}}<\epsilon 
\end{align*}

\end{proof*}

\chapter{Unbeschränkte Folgen divergieren}

In diesem Kapitel werden wir sehen, dass unbeschränkte Folgen divergieren müssen. Daraus werden wir folgern, dass konvergente Folgen beschränkt sein müssen.

\section{Unbeschränkte Folgen divergieren}

Im Kapitel \href{https://de.wikibooks.org/wiki/Mathe\_für\_Nicht-Freaks:\_Konvergenz\_und\_Divergenz\_beweisen\#Anker:Divergenz}
{„Konvergenz und Divergenz beweisen“} haben wir bereits gezeigt, dass die Folge $\left(2^{n}\right)_{n\in \mathbb {N} }$ divergiert. Wir hatten ausgenutzt, dass diese Folge über alle Grenzen hinauswächst. Wenn wir nämlich ein beliebiges $a\in \mathbb {R} $ festhalten, dann gibt es ein $N\in \mathbb {N} $ mit $2^{N}\geq a+1$. Auch für alle $n\in \mathbb {N} $ mit $n\geq N$ ist $2^{n}\geq a+1$ und damit

\begin{align*}
|2^{n}-a|&=2^{n}-a\\&\geq (a+1)-a\\&=1
\end{align*}

Unendlich viele Folgenglieder von $\left(2^{n}\right)_{n\in \mathbb {N} }$ liegen damit außerhalb der $\epsilon $-Umgebung $(a-1;a+1)$. Deshalb kann $\left(2^{n}\right)_{n\in \mathbb {N} }$ nicht gegen $a$ konvergieren. Sonst müssten fast alle Folgenglieder von $\left(2^{n}\right)_{n\in \mathbb {N} }$ in $(a-1;a+1)$ liegen, was aber nicht der Fall ist. Weil $a$ beliebig gewählt wurde, kann $\left(2^{n}\right)_{n\in \mathbb {N} }$ keinen Grenzwert besitzen und muss also divergieren.

Diese Beweisskizze können wir auf beliebige unbeschränkte Folgen verallgemeinern. Wir hatten ausgenutzt, dass $\left(2^{n}\right)_{n\in \mathbb {N} }$ beliebig groß wird. Erinnern wir uns an die Definition einer unbeschränkten Folge:

\begin{importantparagraph*}
Eine Folge $(a_{n})_{n\in \mathbb {N} }$ ist unbeschränkt, wenn es für alle $S\geq 0$ unendlich viele Folgenglieder $a_{n}$ mit $|a_{n}|\geq S$ gibt.

\end{importantparagraph*}

Diese Eigenschaft können wir verwenden, um folgenden Satz zu beweisen:

\begin{theorem*}[Unbeschränkte Folgen divergieren]
Sei $(a_{n})_{n\in \mathbb {N} }$ eine unbeschränkte Folge. Für alle $S\geq 0$ gibt es also unendlich viele Folgenglieder $a_{n}$ mit $|a_{n}|\geq S$. Dann muss die Folge $(a_{n})_{n\in \mathbb {N} }$ divergieren.

\end{theorem*}

\begin{explanation*}[Unbeschränkte Folgen divergieren]
Mit diesem Satz können wir beweisen, dass eine Folge divergiert. Wenn wir nachweisen können, dass eine Folge unbeschränkt ist, wissen wir also sofort. dass sie divergiert.

\end{explanation*}

\begin{proof*}[Unbeschränkte Folgen divergieren]
Sei $(a_{n})_{n\in \mathbb {N} }$ eine unbeschränkte Folge. Sei $a\in \mathbb {R} $ beliebig. Weil $(a_{n})_{n\in \mathbb {N} }$ unbeschränkt ist, gibt es unendlich viele Folgenglieder $a_{n}$ mit $|a_{n}|\geq |a|+1$. Für diese Folgenglieder ist dann

\begin{align*}
&|a_{n}-a|\\[0.3em]&{\color {OliveGreen}\left\downarrow \ {\text{umgekehrte Dreiecksungleichung}}\right.}\\[0.3em]&\geq ||a_{n}|-|a||\\[0.3em]&{\color {OliveGreen}\left\downarrow \ |a_{n}|\geq |a|+1\geq |a|\right.}\\[0.3em]&\geq |a_{n}|-|a|\\[0.3em]&{\color {OliveGreen}\left\downarrow \ |a_{n}|\geq |a|+1\right.}\\[0.3em]&\geq (|a|+1)-|a|\\&=1
\end{align*}

Diese unendlich vielen Folgenglieder $a_{n}$ liegen außerhalb des Intervalls $(a-1;a+1)$. Damit kann $(a_{n})_{n\in \mathbb {N} }$ nicht gegen $a$ konvergieren. Weil wir $a$ beliebig gewählt haben, muss $(a_{n})_{n\in \mathbb {N} }$ divergieren.

\end{proof*}

\begin{example*}[Geometrische Folge]
Eine Verallgemeinerung des Einführungsbeispiels ist: Für $|q|>1$ divergiert die geometrische Folge $\left(q^{n}\right)_{n\in \mathbb {N} }$. Nach den \href{https://de.wikibooks.org/wiki/Mathe\_für\_Nicht-Freaks:\_Bernoulli-Ungleichung\#Anker:Folgerung}
{Folgerungen aus der Bernoulli-Ungleichung} gibt es zu jedem $S>0$ unendlich viele $n\in \mathbb {N} $ mit $|q^{n}|=|q|^{n}>S$. Also ist $\left(q^{n}\right)_{n\in \mathbb {N} }$ unbeschränkt und somit divergent.

\end{example*}

\section{Konvergente Folgen sind beschränkt}

\subsection{Beweis über Kontraposition}

Laut dem obigen Satz müssen unbeschränkte Folgen divergieren. Mit Hilfe von Kontraposition können wir folgern, dass konvergente Folgen beschränkt sein müssen. Das Prinzip der Kontraposition lautet:

\begin{align*}
(A\implies B)\iff (\neg B\implies \neg A)
\end{align*}

Obiger Satz ist die Implikation:

\begin{align*}
(a_{n})_{n\in \mathbb {N} }{\text{ ist unbeschränkt}}\implies (a_{n})_{n\in \mathbb {N} }{\text{ divergiert}}
\end{align*}

Also muss nach dem Prinzip der Kontraposition gelten:

\begin{align*}
\neg {\big (}(a_{n})_{n\in \mathbb {N} }{\text{ divergiert}}{\big )}\implies \neg {\big (}(a_{n})_{n\in \mathbb {N} }{\text{ ist unbeschränkt}}{\big )}
\end{align*}

Dies bedeutet dasselbe wie

\begin{align*}
(a_{n})_{n\in \mathbb {N} }{\text{ konvergiert}}\implies (a_{n})_{n\in \mathbb {N} }{\text{ ist beschränkt}}
\end{align*}

Wer daran zweifelt, dass Kontraposition tatsächlich funktioniert, kann sich die Wahrheitstafeln von $(A\implies B)$ und $(\neg B\implies \neg A)$ aufschreiben und vergleichen. Ein kleines Beispiel ist: {''}Wenn es regnet ($A$), wird der Boden nass ($B$).{''} Deshalb gilt auch: {''}Wenn der Boden nicht nass ist ($\neg B$), kann es nicht regnen ($\neg A$).{''} Aus der zweiten Implikation können wir umgekehrt auch die erste folgern. Durch die Kontraposition gilt also folgender Satz, den wir insbesondere in späteren Beweisen nutzen werden:

\begin{theorem*}[konvergente Folgen sind beschränkt]
Jede konvergente Folge ist beschränkt. Wenn also eine Folge $(a_{n})_{n\in \mathbb {N} }$ konvergiert, dann gibt es ein $S\geq 0$ mit $|a_{n}|\leq S$ für alle $n\in \mathbb {N} $.

\end{theorem*}

\begin{hint*}
Die Umkehrung des Satzes muss nicht gelten. Das bedeutet: Eine \emph{beschränkte} Folge muss nicht konvergieren. Eine \emph{divergente} Folge muss nicht unbeschränkt sein.

Ein Gegenbeispiel ist die Folge $((-1)^{n})_{n\in \mathbb {N} }$. Diese Folge ist beschränkt, jedoch nicht konvergent.

\end{hint*}

\chapter{Grenzwertsätze}

Epsilon-Beweise für Grenzwerte können sehr aufwendig werden. In diesem Kapitel behandeln wir einige Sätze, die die Bestimmung von Grenzwerten vereinfachen.

\section{Die Grenzwertsätze}

Die Grenzwertsätze für konvergente Folgen lauten:

\begin{theorem*}[Grenzwertsätze]
Seien $\left(a_{n}\right)_{n\in \mathbb {N} }$ und $\left(b_{n}\right)_{n\in \mathbb {N} }$ zwei konvergente Folgen mit $\lim _{n\rightarrow \infty }a_{n}=a$ und $\lim _{n\rightarrow \infty }b_{n}=b$. Sei außerdem $k\in \mathbb {N} $ beliebig. Es gilt

\begin{itemize}
\item $\lim _{n\rightarrow \infty }|a_{n}|=|a|$
\item $\lim _{n\rightarrow \infty }a_{n}+b_{n}=a+b$
\item $\lim _{n\rightarrow \infty }\lambda \cdot a_{n}=\lambda \cdot a$ für alle $\lambda \in \mathbb {R} $
\item $\lim _{n\rightarrow \infty }a_{n}\cdot b_{n}=a\cdot b$
\item $\lim _{n\rightarrow \infty }a_{n}^{k}=a^{k}$
\end{itemize}

Wenn außerdem $b\neq 0$ und $b_{n}\neq 0$ für alle $n\in \mathbb {N} $ ist, dann gilt auch

\begin{itemize}
\item $\lim _{n\rightarrow \infty }{\frac {1}{b_{n}}}={\frac {1}{b}}$
\item $\lim _{n\rightarrow \infty }{\frac {a_{n}}{b_{n}}}={\frac {a}{b}}$
\end{itemize}

Für $k\in \mathbb {N} $ und $a_{n}\geq 0$ für alle $n\in \mathbb {N} $ gilt:

\begin{align*}
\lim _{n\rightarrow \infty }{\sqrt[{k}]{a_{n}}}={\sqrt[{k}]{a}}
\end{align*}

\end{theorem*}

\begin{warning*}
Diese Regeln gelten nur, wenn \emph{alle} Teilfolgen, die in den Grenzwertregeln vorkommen, konvergieren. Wenn auch nur eine dieser Folgen divergiert, können wir den Satz nicht anwenden.

Wir müssen außerdem beachten, dass $\infty $ und $-\infty $ keine reellen Zahlen sind und damit auch keine gültigen Grenzwerte. Wenn also beispielsweise $\lim _{n\rightarrow \infty }a_{n}=\infty $ ist, dann divergiert $\left(a_{n}\right)_{n\in \mathbb {N} }$ und wir können keinen der Grenzwertsätze anwenden.

\end{warning*}

\section{Monotonieregel: Grenzwerte abschätzen}

Außerdem gilt die Monotonieregel, die wir zum Abschätzen der Grenzwerte verwenden können:

\begin{theorem*}[Monotonieregel]
Seien $\left(a_{n}\right)_{n\in \mathbb {N} }$ und $\left(b_{n}\right)_{n\in \mathbb {N} }$ zwei konvergente Folgen. Wenn $a_{n}\leq b_{n}$ für fast alle $n\in \mathbb {N} $ ist, dann gilt die Ungleichung:

\begin{align*}
\lim _{n\rightarrow \infty }a_{n}\leq \lim _{n\rightarrow \infty }b_{n}
\end{align*}

\end{theorem*}

\section{Beispiel: Grenzwert einer Folge berechnen}

Betrachten wir die Folge

\begin{align*}
x_{n}={\frac {5-{\sqrt[{n}]{23}}}{{\sqrt[{n}]{n}}+{\tfrac {1}{n^{2}}}}}
\end{align*}

Ein Beweis mit $\epsilon $-Umgebung zur Bestimmung der Konvergenz wäre sehr kompliziert. Zum Glück erkennen wir in der Folgendefinition viele Folgen, deren Konvergenzverhalten wir bereits kennen. So ist zum Beispiel $\lim _{n\rightarrow \infty }{\sqrt[{n}]{23}}=1$. Durch schrittweise Anwendung der Grenzwertsätze können wir den Grenzwert bestimmen:

\begin{align*}
\lim _{n\rightarrow \infty }x_{n}&=\lim _{n\rightarrow \infty }{\frac {5-{\sqrt[{n}]{23}}}{{\sqrt[{n}]{n}}+{\tfrac {1}{n^{2}}}}}\\[0.3em]&\quad {\color {OliveGreen}\left\downarrow \ {\text{Quotientenregel}}\right.}\\[0.3em]&={\frac {\lim _{n\rightarrow \infty }5-{\sqrt[{n}]{23}}}{\lim _{n\rightarrow \infty }{\sqrt[{n}]{n}}+{\tfrac {1}{n^{2}}}}}\\[0.3em]&\quad {\color {OliveGreen}\left\downarrow \ {\text{Summenregel}}\right.}\\[0.3em]&={\frac {\left(\lim _{n\rightarrow \infty }5\right)+\left(\lim _{n\rightarrow \infty }-{\sqrt[{n}]{23}}\right)}{\left(\lim _{n\rightarrow \infty }{\sqrt[{n}]{n}}\right)+\left(\lim _{n\rightarrow \infty }{\tfrac {1}{n^{2}}}\right)}}\\[0.3em]&\quad {\color {OliveGreen}\left\downarrow \ {\text{Faktorregel}}\right.}\\[0.3em]&={\frac {\left(\lim _{n\rightarrow \infty }5\right)-\left(\lim _{n\rightarrow \infty }{\sqrt[{n}]{23}}\right)}{\left(\lim _{n\rightarrow \infty }{\sqrt[{n}]{n}}\right)+\left(\lim _{n\rightarrow \infty }{\tfrac {1}{n^{2}}}\right)}}\\[0.3em]&\quad {\color {OliveGreen}\left\downarrow \ {\text{bekannte Grenzwerte ausrechnen}}\right.}\\[0.3em]&={\frac {5-1}{1+0}}=4
\end{align*}

So können wir zeigen, dass $(x_{n})_{n\in \mathbb {N} }$ konvergiert und den Grenzwert $4$ besitzt. Diese Herleitung hat aber einen Haken: Wir benutzen die Grenzwertsätze, bevor wir die Konvergenz der einzelnen Folgen gezeigt haben. Dass diese Folgen konvergieren, ergibt sich erst im Argumentationsverlauf, nachdem wir die Grenzwertsätze schon verwendet haben. Deswegen ist diese Herleitung kein gültiger Beweis. Ein gültiger Beweis ist zum Beispiel folgender:

\begin{align*}
{\begin{array}{ll}&\lim _{n\rightarrow \infty }5=5\land \lim _{n\rightarrow \infty }{\sqrt[{n}]{23}}=1\land \lim _{n\rightarrow \infty }{\sqrt[{n}]{n}}=1\land \lim _{n\rightarrow \infty }{\tfrac {1}{n^{2}}}=0\\[0.7em]&\quad {\color {OliveGreen}\left\downarrow \ {\text{Faktorregel}}\right.}\\[0.7em]\Rightarrow \ &\lim _{n\rightarrow \infty }5=5\land \lim _{n\rightarrow \infty }-{\sqrt[{n}]{23}}=-1\land \lim _{n\rightarrow \infty }{\sqrt[{n}]{n}}=1\land \lim _{n\rightarrow \infty }{\tfrac {1}{n^{2}}}=0\\[0.7em]&\quad {\color {OliveGreen}\left\downarrow \ {\text{Summenregel}}\right.}\\[0.7em]\Rightarrow \ &\lim _{n\rightarrow \infty }5-{\sqrt[{n}]{23}}=5-1=4\land \lim _{n\rightarrow \infty }{\sqrt[{n}]{n}}+{\tfrac {1}{n^{2}}}=1+0=1\\[0.7em]&\quad {\color {OliveGreen}\left\downarrow \ {\text{Quotientenregel}}\right.}\\[0.7em]\Rightarrow \ &\lim _{n\rightarrow \infty }{\frac {5-{\sqrt[{n}]{23}}}{{\sqrt[{n}]{n}}+{\tfrac {1}{n^{2}}}}}={\frac {4}{1}}=4\end{array}}
\end{align*}

Wir beginnen mit der Konvergenz der Folgen, deren Konvergenzverhalten wir kennen. Durch schrittweise Anwendung der Grenzwertsätze in umgekehrter Reihenfolge leiten wir dann die Konvergenz der betrachteten Folge $(x_{n})_{n\in \mathbb {N} }$ und ihren Grenzwert her. Beim Zeichen $\land $ handelt es sich um die \href{https://de.wikibooks.org/wiki/Mathe\_für\_Nicht-Freaks:\_Junktor\#Anker:Konjunktion}
{Konjunktion}, die man als „und“ lesen kann.

Den Beweis so aufzuschreiben ist aber aufwendig und macht keinen Spaß. Meist zeigen wir diese Aussagen wie die Beweisskizze oben. Wir wenden einfach die Grenzwertsätze an, obwohl wir nicht wissen, ob die Folgen konvergieren. Wir müssen aber im Nachhinein anmerken, dass wir die Grenzwertsätze anwenden durften. Das gilt, weil am Ende alles konvergiert. Weil bei den letzten Schritten alles funktioniert, durften wir die Schritte davor machen. Wenn wir den Beweis also durch direkte Anwendung der Grenzwertsätze zeigen wollen, müssen wir noch erklären, dass wir diese Sätze benutzen durften.
\clearpage
\section{Beweise der Grenzwertsätze}

\subsection{Die Betragsregel}

\begin{theorem*}[Grenzwertregel mit Absolutbetrag]
Sei $(a_{n})_{n\in \mathbb {N} }$ eine konvergente Folge mit dem Grenzwert $a$. Dann ist $\lim _{n\rightarrow \infty }|a_{n}|=|a|$.

\end{theorem*}

\begin{proof*}[Grenzwertregel mit Absolutbetrag]
Sei $\epsilon >0$ beliebig. Weil $(a_{n})_{n\in \mathbb {N} }$ gegen $a$ konvergiert, gibt es ein $N\in \mathbb {N} $ mit $|a_{n}-a|<\epsilon $ für alle $n\geq N$. Sei nun $n\geq N$ beliebig. Wegen der umgekehrten Dreiecksungleichung $||x|-|y||\leq |x-y|$ folgt

\begin{align*}
||a_{n}|-|a||\leq |a_{n}-a|<\epsilon 
\end{align*}

\end{proof*}

\subsection{Die Summenregel}

\begin{theorem*}[Grenzwertsatz für Summen]
Sei $(a_{n})_{n\in \mathbb {N} }$ eine konvergente Folge mit Grenzwert $a$ und $(b_{n})_{n\in \mathbb {N} }$ eine konvergente Folge mit Grenzwert $b$. Dann konvergiert auch die Folge $(a_{n}+b_{n})_{n\in \mathbb {N} }$ mit $\lim _{n\rightarrow \infty }a_{n}+b_{n}=a+b$.

\end{theorem*}

\begin{solutionprocess*}[Grenzwertsatz für Summen]
Wir müssen zeigen, dass der Betrag $|(a_{n}+b_{n})-(a+b)|$ beliebig klein wird. Wir können verwenden, dass die Beträge $|a_{n}-a|$ und $|b_{n}-b|$ beliebig klein werden. Deswegen sollten wir eine Abschätzung von $|(a_{n}+b_{n})-(a+b)|$ nach oben finden, bei der die Beträge $|a_{n}-a|$ oder $|b_{n}-b|$ vorkommen. Hier gibt es einen Trick: Wir schreiben den Term $(a_{n}+b_{n})-(a+b)$ geschickt um und verwenden dann die Dreiecksungleichung

\begin{align*}
|(a_{n}+b_{n})-(a+b)|&=|(a_{n}-a)+(b_{n}-b)|\\&\leq |a_{n}-a|+|b_{n}-b|
\end{align*}

Weil $|a_{n}-a|$ und $|b_{n}-b|$ beliebig klein werden, sollte auch ihre Summe beliebig klein werden. Somit sollte unsere Abschätzung ausreichen. Jedoch müssen wir noch einen Epsilon-Beweis für unsere Vermutung formulieren. Auch hier können wir einen Trick verwenden: In der Summe haben wir zwei Beträge und jeden schätzen wir gegen ${\tfrac {\epsilon }{2}}$ ab. Wenn nämlich $|a_{n}-a|<{\tfrac {\epsilon }{2}}$ und $|b_{n}-b|<{\tfrac {\epsilon }{2}}$ ist, dann ist

\begin{align*}
|a_{n}-a|+|b_{n}-b|<{\tfrac {\epsilon }{2}}+{\tfrac {\epsilon }{2}}=\epsilon 
\end{align*}

Wir wissen, dass es ein $N_{1}$ mit $|a_{n}-a|<{\tfrac {\epsilon }{2}}$ für alle $n\geq N_{1}$ gibt. Analog existiert ein $N_{2}$ mit $|b_{n}-b|<{\tfrac {\epsilon }{2}}$ für alle $n\geq N_{2}$. Für unseren Beweis brauchen wir gleichzeitig $|a_{n}-a|<{\tfrac {\epsilon }{2}}$ und $|b_{n}-b|<{\tfrac {\epsilon }{2}}$. Also sollte gleichzeitig $n\geq N_{1}$ und $n\geq N_{2}$ gelten. Unser Ziel ist es, ein $N\in \mathbb {N} $ zu finden, sodass aus $n\geq N$ sowohl $n\geq N_{1}$ als auch $n\geq N_{2}$ folgt. Eine Möglichkeit ist, $N=\max\{N_{1},\,N_{2}\}$ zu wählen. Aus $n\geq \max\{N_{1},\,N_{2}\}$ folgt nämlich $n\geq N_{1}$ und $n\geq N_{2}$.

\end{solutionprocess*}

\begin{proof*}[Grenzwertsatz für Summen]
Sei $\epsilon >0$ beliebig. Es gibt ein $N_{1}$ mit $|a_{n}-a|<{\tfrac {\epsilon }{2}}$ für alle $n\geq N_{1}$, weil $\lim _{n\rightarrow \infty }a_{n}=a$ ist. Außerdem gibt es wegen $\lim _{n\to \infty }b_{n}=b$ ein $N_{2}$ mit $|b_{n}-b|<{\tfrac {\epsilon }{2}}$ für alle $n\geq N_{2}$. Wir wählen $N=\max\{N_{1},\,N_{2}\}$. Sei $n\geq N$ beliebig. Es ist

\begin{align*}
|(a_{n}+b_{n})-(a+b)|&=|(a_{n}-a)+(b_{n}-b)|\\[0.5em]&\quad {\color {OliveGreen}\left\downarrow \ {\text{Dreiecksungleichung}}\right.}\\[0.5em]&\leq |a_{n}-a|+|b_{n}-b|\\&<{\tfrac {\epsilon }{2}}+{\tfrac {\epsilon }{2}}\\&=\epsilon 
\end{align*}

\end{proof*}

\subsection{Die Faktorregel}

\begin{theorem*}[Faktorregel für Grenzwerte]
Sei $\lambda \in \mathbb {R} $ beliebig und $(a_{n})_{n\in \mathbb {N} }$ eine konvergente Folge mit Grenzwert $a$. Dann konvergiert auch die Folge $(\lambda a_{n})_{n\in \mathbb {N} }$ mit $\lim _{n\rightarrow \infty }\lambda a_{n}=\lambda a$.

\end{theorem*}

\begin{solutionprocess*}[Faktorregel für Grenzwerte]
Um $\lim _{n\rightarrow \infty }\lambda a_{n}=\lambda a$ zu beweisen, müssen wir $|\lambda a_{n}-\lambda a|<\epsilon $ für fast alle $n\in \mathbb {N} $ zeigen. Formen wir diese Ungleichung um:

\begin{align*}
{\begin{array}{lrl}&|\lambda a_{n}-\lambda a|&<\epsilon \\\iff {}&|\lambda \cdot (a_{n}-a)|&<\epsilon \\\iff {}&|\lambda |\cdot |a_{n}-a|&<\epsilon \end{array}}
\end{align*}

Wir können nicht pauschal durch $|\lambda |$ teilen, weil $\lambda $ auch Null sein könnte. Jedoch ist der Fall $\lambda =0$ einfach zu zeigen. Hier müssen wir beweisen, dass $\lim _{n\to \infty }\lambda a_{n}=\lambda a=0\cdot a=0$ ist. Da $\lambda a_{n}=0\cdot a_{n}=0$ ist, folgt $\lim _{n\to \infty }\lambda a_{n}=\lim _{n\to \infty }0=0$, was zu zeigen war. Schauen wir uns den Fall $\lambda \neq 0$ an:

\begin{align*}
{\begin{array}{lrl}&|\lambda |\cdot |a_{n}-a|&<\epsilon \\\iff {}&|a_{n}-a|&<{\frac {\epsilon }{|\lambda |}}\end{array}}
\end{align*}

Weil $|a_{n}-a|$ gegen $0$ konvergiert, gibt es ein $N\in \mathbb {N} $, sodass $|a_{n}-a|<{\tfrac {\epsilon }{|\lambda |}}$ für alle $n\geq N$ ist.

\end{solutionprocess*}

\subsection{Die Produktregel}

\begin{theorem*}[Produktregel für Grenzwerte]
Sei $(a_{n})_{n\in \mathbb {N} }$ eine konvergente Folge mit Grenzwert $a$ und $(b_{n})_{n\in \mathbb {N} }$ eine konvergente Folge mit Grenzwert $b$. Dann konvergiert auch die Folge $(a_{n}\cdot b_{n})_{n\in \mathbb {N} }$ mit $\lim _{n\rightarrow \infty }a_{n}\cdot b_{n}=a\cdot b$.

\end{theorem*}

\begin{proof*}[Produktregel für Grenzwerte]
Sei $\epsilon >0$ beliebig.

Wir müssen beweisen, dass $|a_{n}b_{n}-ab|<\epsilon $ für alle $n\geq N$ gilt, wobei wir $N$ in Abhängigkeit von $\epsilon $ geschickt wählen müssen. Dabei können wir verwenden, dass $|a_{n}-a|$ und $|b_{n}-b|$ beliebig klein werden, weil die Folgen $(a_{n})_{n\in \mathbb {N} }$ gegen $a$ und $(b_{n})_{n\in \mathbb {N} }$ gegen $b$ konvergieren. Um dies nutzen zu können, müssen wir $|a_{n}b_{n}-ab|$ geschickt umformen und so nach oben abschätzen, dass wir die Beträge $|a_{n}-a|$ und $|b_{n}-b|$ erhalten. Hierzu verwenden wir einen Trick, der für diese Art von Beweis typisch ist. Wir addieren den Term $ab_{n}-ab_{n}$, welcher gleich Null ist:

\begin{align*}
&|a_{n}b_{n}-ab|\\[0.3em]&\quad {\color {OliveGreen}\left\downarrow \ ab_{n}-ab_{n}=0\right.}\\[0.3em]=\ &|a_{n}b_{n}-ab+ab_{n}-ab_{n}|\\[0.3em]&\quad {\color {OliveGreen}\left\downarrow \ {\text{Umstellen}}\right.}\\[0.3em]=\ &|a_{n}b_{n}-ab_{n}+ab_{n}-ab|\\[0.3em]&\quad {\color {OliveGreen}\left\downarrow \ {\text{Ausklammern}}\right.}\\[0.3em]=\ &|b_{n}(a_{n}-a)+a(b_{n}-b)|\\[0.3em]&\quad {\color {OliveGreen}\left\downarrow \ {\text{Dreiecksungleichung}}\right.}\\[0.3em]\leq \ &|b_{n}(a_{n}-a)|+|a(b_{n}-b)|\\[0.3em]\leq \ &|b_{n}|\cdot |a_{n}-a|+|a|\cdot |b_{n}-b|
\end{align*}

Wenn wir also für alle $n\geq N$ zeigen können, dass beide Summanden kleiner als ${\tfrac {\epsilon }{2}}$ sind, dann sind wir fertig.

\textbf{Abschätzung des zweiten Summanden}

Beim zweiten Summanden ist das leicht: Die Folge $(b_{n})_{n\in \mathbb {N} }$ konvergiert gegen $b$ und nach der Faktorregel mit $\lambda =a$ gilt $\lim _{n\to \infty }a\cdot b_{n}=a\cdot b$. Damit gilt $\lim _{n\to \infty }a\cdot (b_{n}-b)=0$ nach der Summenregel, d.h. es gibt ein $N_{1}\in \mathbb {N} $ so, dass für alle $n\geq N_{1}$ gilt $|a|\cdot |b_{n}-b|=|a(b_{n}-b)|<{\tfrac {\epsilon }{2}}$.

\textbf{Abschätzung des ersten Summanden}

Auch beim ersten Summanden wäre es schön, wenn wir die Faktorregel anwenden können. Das Problem ist nur, dass $b_{n}$ von $n$ abhängt und folglich $b_{n}$ kein Kandidat für das $\lambda $ aus der Faktorregel ist.

Wir haben in einem vorherigen Kapitel bewiesen, dass konvergente Folgen beschränkt sind. Diesen Satz können wir hier auf die Folge $b_{n}$ anwenden: Sei $M\in \mathbb {R} _{0}^{+}$ so dass $|b_{n}|\leq M$ für alle $n\in \mathbb {N} $.

Dann gilt für alle $n\in \mathbb {N} $, dass $|b_{n}|\cdot |a_{n}-a|\leq M\cdot |a_{n}-a|$ und genauso wie für den zweiten Summanden liefert uns die Faktorregel mit $\lambda =M$ (beachte, dass $M$ im Gegensatz zu $b_{n}$ nicht von $n$ abhängt) ein $N_{2}\in \mathbb {N} $ mit $M\cdot |a_{n}-a|<{\tfrac {\epsilon }{2}}$ für alle $n\geq N_{2}$. Also gilt für alle $n\geq N_{2}$ die folgende Ungleichung: $|b_{n}|\cdot |a_{n}-a|<{\tfrac {\epsilon }{2}}$.

\textbf{Zusammenfassung}

Wir brauchen nur noch ein passend gewähltes $N$. Für alle $n\geq N$ muss die Bedingung $n\geq N_{1}$ und $n\geq N_{2}$ erfüllt sein, damit beide Abschätzungen gültig sind. Daher wählen wir $N:=\max\{N_{1},N_{2}\}$. Dieses hängt nur von $\epsilon $ ab, da $N_{1}$ und $N_{2}$ nur von $\epsilon $ abhängen.

Für alle $n\geq N$ gilt nun

\begin{align*}
|a_{n}b_{n}-ab|\leq \ |b_{n}|\cdot |a_{n}-a|+|a|\cdot |b_{n}-b|<{\tfrac {\epsilon }{2}}+{\tfrac {\epsilon }{2}}=\epsilon 
\end{align*}

\end{proof*}

\subsection{Die Quotientenregel}

\begin{theorem*}[Quotientenregel für Grenzwerte]
Sei $(a_{n})_{n\in \mathbb {N} }$ eine konvergente Folge mit Grenzwert $a$ und sei $(b_{n})_{n\in \mathbb {N} }$ eine konvergente Folge mit Grenzwert $b\neq 0$ sowie $b_{n}\neq 0$ für alle $n\in \mathbb {N} $. Dann konvergiert die Folge $({\tfrac {a_{n}}{b_{n}}})_{n\in \mathbb {N} }$ mit $\lim _{n\rightarrow \infty }{\tfrac {a_{n}}{b_{n}}}={\tfrac {a}{b}}$.

\end{theorem*}

\begin{solutionprocess*}[Quotientenregel für Grenzwerte]
Es genügt zu zeigen, dass $\lim _{n\to \infty }{\tfrac {1}{b_{n}}}={\tfrac {1}{b}}$ ist, denn aus der Produktregel folgt

\begin{align*}
\lim _{n\to \infty }{\tfrac {a_{n}}{b_{n}}}=\lim _{n\to \infty }a_{n}\cdot {\tfrac {1}{b_{n}}}=\lim _{n\to \infty }a_{n}\cdot \lim _{n\to \infty }{\tfrac {1}{b_{n}}}=a\cdot {\tfrac {1}{b}}={\tfrac {a}{b}}
\end{align*}

Für den Beweis $\lim _{n\to \infty }{\tfrac {1}{b_{n}}}={\tfrac {1}{b}}$ müssen wir zeigen, dass $|{\tfrac {1}{b_{n}}}-{\tfrac {1}{b}}|$ beliebig klein wird. Dabei können wir verwenden, dass $|b_{n}-b|$ beliebig klein wird, weil $(b_{n})_{n\in \mathbb {N} }$ gegen $b$ konvergiert. Dazu formen wir $|{\tfrac {1}{b_{n}}}-{\tfrac {1}{b}}|$ geschickt um:

\begin{align*}
\left|{\frac {1}{b_{n}}}-{\frac {1}{b}}\right|&=\left|{\frac {b}{b_{n}b}}-{\frac {b_{n}}{bb_{n}}}\right|\\[0.3em]&=\left|{\frac {b-b_{n}}{b_{n}b}}\right|\\[0.3em]&={\frac {|b-b_{n}|}{|b_{n}||b|}}
\end{align*}

Nun können wir $|b_{n}-b|$ kontrollieren, d.h. beliebig klein machen. Das $|b|$ im Nenner stört uns nicht weiter, da es konstant ist. Wir müssen uns also nur noch um $|b_{n}|$ im Nenner kümmern. Da wir $|b_{n}-b|$ beliebig klein machen können, reicht es, wenn wir ${\tfrac {1}{|b_{n}|}}$ nach \emph{oben} durch eine Konstante abschätzen. Dazu müssen wir $|b_{n}|$ nach \emph{unten} abschätzen.

Um $|b_{n}|$ nach unten abzuschätzen, verwenden wir nun die Voraussetzung, dass $\lim _{n\to \infty }b_{n}=b\neq 0$ ist. Daher gibt es ein $N_{1}\in \mathbb {N} $, so dass ab diesem Index alle Folgenglieder von $(b_{n})_{n\in \mathbb {N} }$ die Ungleichung $|b_{n}-b|<{\tfrac {|b|}{2}}$ erfüllen. Also gilt $|b_{n}|\geq |b|-|b_{n}-b|\geq |b|-{\tfrac {|b|}{2}}={\tfrac {|b|}{2}}$ für alle $n\geq N_{1}$. Für den gesamten Ausdruck erhalten wir damit

\begin{align*}
\left|{\frac {1}{b_{n}}}-{\frac {1}{b}}\right|&={\frac {|b-b_{n}|}{|b_{n}||b|}}\\[0.5em]&\leq {\frac {|b-b_{n}|}{{\tfrac {|b|}{2}}|b|}}\\[0.5em]&={\frac {2}{|b|^{2}}}|b-b_{n}|
\end{align*}

Diesen Ausdruck bekommen wir beliebig klein, da wir $|b-b_{n}|$ beliebig klein kriegen, und der Vorfaktor konstant ist. Hierzu wählen wir zu einem beliebigem $\epsilon >0$ den Index $N_{2}\in \mathbb {N} $ so groß, dass für alle $n\geq N_{2}$ gilt

\begin{align*}
{\frac {2}{|b|^{2}}}|b-b_{n}|<\epsilon \iff |b_{n}-b|<{\frac {\epsilon |b|^{2}}{2}}
\end{align*}

Dann erhalten wir insgesamt für alle $n\geq \max\{N_{1},N_{2}\}$:

\begin{align*}
\left|{\frac {1}{b_{n}}}-{\frac {1}{b}}\right|&\leq {\frac {2}{|b|^{2}}}|b-b_{n}|\\[0.5em]&<{\frac {2}{|b|^{2}}}\cdot {\frac {\epsilon |b|^{2}}{2}}\\[0.5em]&=\epsilon 
\end{align*}

Diese Beweisskizze müssen wir nun in einen formalen Beweis gießen, um $\lim _{n\to \infty }{\tfrac {1}{b_{n}}}={\tfrac {1}{b}}$ zu zeigen.

\end{solutionprocess*}

\subsection{Die Monotonieregel}

\begin{theorem*}[Monotonieregel für Grenzwerte]
Seien $(a_{n})_{n\in \mathbb {N} }$ und $(b_{n})_{n\in \mathbb {N} }$ Folgen mit Grenzwerten $a$ und $b$. Es gelte außerdem $a_{n}\leq b_{n}$ für fast alle $n\in \mathbb {N} $. Dann gilt $a\leq b$.

\end{theorem*}

\begin{proofsummary*}[Monotonieregel für Grenzwerte]
Diese Regel zeigen wir durch einen \emph{Widerspruchsbeweis}. Wir nehmen an, dass unter den Voraussetzungen des Satzes $a>b$ wäre, und leiten daraus eine widersprüchliche Aussage her.

\end{proofsummary*}

\begin{proof*}[Monotonieregel für Grenzwerte]
Angenommen $a>b$. Wegen $\lim _{n\to \infty }a_{n}=a$ und $\lim _{n\to \infty }b_{n}=b$ gibt es zu $\epsilon ={\tfrac {a-b}{2}}>0$ Indizes $N_{1},N_{2}\in \mathbb {N} $ mit $|a_{n}-a|<{\tfrac {a-b}{2}}$ für alle $n\geq N_{1}$ und $|b_{n}-b|<{\tfrac {a-b}{2}}$ für alle $n\geq N_{2}$. Daraus folgt für alle $n\geq \max\{N_{1},N_{2}\}$:

\begin{align*}
b_{n}-a_{n}&=b_{n}-a_{n}+0+0\\&=b_{n}-a_{n}+b-a+a-b\\&=b_{n}-b+(b-a)+a-a_{n}\\&\leq |b_{n}-b|+(b-a)+|a_{n}-a|\\&<{\tfrac {a-b}{2}}+(b-a)+{\tfrac {a-b}{2}}\\&=a-b+b-a=0
\end{align*}

Also $b_{n}<a_{n}$ für alle $n\geq \max\{N_{1},N_{2}\}$. Dies ist ein Widerspruch zu $a_{n}\leq b_{n}$ für fast alle $n$. Daher muss $a\leq b$ gelten.

\end{proof*}

\chapter{Der Sandwichsatz}

Der \emph{Sandwichsatz} ist ein mächtiges Werkzeug, um den Grenzwert einer Folge zu bestimmen. Dieser Satz ist insbesondere hilfreich bei Folgen mit einer komplexen Bildungsvorschrift, bei denen Grenzwertsätze nicht angewandt werden können und bei denen die Epsilon-Definiton der Konvergenz schwer nachgewiesen werden kann. In der Literatur gibt es für den Satz zahlreiche weitere Bezeichnungen, wie \emph{Sandwich-Theorem}, \emph{Sandwich Lemma}, \emph{Einschnürungssatz} oder \emph{Einschließungsregel}.

\section{Motivation}

Die Aussage des Satzes ist recht einfach. Wir wollen die Konvergenz einer Folge $(a_{n})_{n\in \mathbb {N} }$ untersuchen. Dies können wir allerdings nicht immer direkt machen, da sie eine komplizierte Bauart haben kann. Oft ist es jedoch möglich zwei einfacher strukturierte Folgen $(b_{n})_{n\in \mathbb {N} }$ und $(c_{n})_{n\in \mathbb {N} }$ zu finden, die $(a_{n})_{n\in \mathbb {N} }$ von unten bzw. oben einschließen, d.h. es gilt $b_{n}\leq a_{n}\leq c_{n}$ für alle $n\in \mathbb {N} $. Konvergieren diese beiden Folgen nun gegen denselben Grenzwert $a$, so besagt des Sandwichsatz, dass auch unsere eingeschlossene Folge $(a_{n})_{n\in \mathbb {N} }$ gegen $a$ konvergiert.

\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Sandwich_lemma.svg}{\textbf{Sandwich\allowbreak\_lemma.svg}} by Who2010 \textit{(CC BY-SA 4.0)}}\begin{center}
\adjincludegraphics[max width=0.5\textwidth,max height=0.2\textheight]{file58sandwich95lemma95ac3cb3f4725d7c1a51fae50fcd0a31c5b2056562}\end{center}

Aus der Funktionsweise erklärt sich der Name des Satzes von selbst: Die Folgen $(b_{n})_{n\in \mathbb {N} }$ und $(c_{n})_{n\in \mathbb {N} }$ schließen wie die Brötchen eines Sandwiches den Inhalt die Folge $(a_{n})_{n\in \mathbb {N} }$ ein. Wenn nun $(b_{n})_{n\in \mathbb {N} }$ und $(c_{n})_{n\in \mathbb {N} }$ sich immer näher kommen und gegen einen Punkt konvergieren, dann muss auch die eingeschlossene Folge $(a_{n})_{n\in \mathbb {N} }$ gegen diesen Punkt konvergieren.
\clearpage
\section{Der Sandwichsatz}

Der Satz lautet:

\begin{theorem*}[Sandwichsatz]
Sei $(a_{n})_{n\in \mathbb {N} }$ eine beliebige Folge. Wenn es zwei Folgen $(b_{n})_{n\in \mathbb {N} }$ und $(c_{n})_{n\in \mathbb {N} }$ gibt, so dass $b_{n}\leq a_{n}\leq c_{n}$ für alle $n\in \mathbb {N} $ und $\lim _{n\rightarrow \infty }b_{n}=\lim _{n\rightarrow \infty }c_{n}=a$ für ein $a\in \mathbb {R} $, dann konvergiert auch $(a_{n})_{n\in \mathbb {N} }$ gegen $a$.

\end{theorem*}

\begin{proof*}[Sandwichsatz]
Seien $(b_{n})_{n\in \mathbb {N} }$ und $(c_{n})_{n\in \mathbb {N} }$, sodass $b_{n}\leq a_{n}\leq c_{n}$ und $\lim _{n\rightarrow \infty }b_{n}=\lim _{n\rightarrow \infty }c_{n}=a$ für ein $a\in \mathbb {R} $.

Wir müssen $\lim _{n\to \infty }a_{n}=a$ zeigen, d.h. zu jedem $\epsilon >0$ gibt es ein $N\in \mathbb {N} $, so dass $|a_{n}-a|<\epsilon $ für alle $n\geq N$ gilt. Sei $\epsilon >0$ beliebig. Nach Voraussetzung ist zum einen $\lim _{n\to \infty }b_{n}=a$ und $\lim _{n\to \infty }c_{n}=a$.

Daher gibt es $N_{1}\in \mathbb {N} $ und $N_{2}\in \mathbb {N} $ mit $|b_{n}-a|<\epsilon $ für alle $n\geq N_{1}$ und $|c_{n}-a|<\epsilon $ für alle $n\geq N_{2}$. Zum anderen gilt $b_{n}\leq a_{n}\leq c_{n}$ für alle $n\in \mathbb {N} $. Für jedes Folgenglied von $(a_{n})_{n\in \mathbb {N} }$ kann nun $a_{n}\geq a$ oder $a_{n}\leq a$ gelten.

Für $a_{n}\geq a$ folgt

\begin{align*}
|\underbrace {a_{n}-a} _{\geq 0}|=a_{n}-a{\overset {a_{n}\leq c_{n}}{\leq }}\underbrace {c_{n}-a} _{\geq 0}=|c_{n}-a|
\end{align*}

Andererseits folgt für $a_{n}\leq a$

\begin{align*}
|\underbrace {a_{n}-a} _{\leq 0}|=-(a_{n}-a)=a-a_{n}{\overset {a_{n}\geq b_{n}}{\leq }}a-b_{n}=-(\underbrace {b_{n}-a} _{\leq 0})=|b_{n}-a|
\end{align*}

Wir wählen nun $N=\max\{N_{1},N_{2}\}$. Sei $n\geq N$ beliebig, daraus folgt sowohl $n\geq N_{1}$ als auch $n\geq N_{2}$. Wenn $a_{n}\geq a$ ist, gilt somit

\begin{align*}
|a_{n}-a|\leq |c_{n}-a|<\epsilon 
\end{align*}

Im Fall $a_{n}\leq a$ ist

\begin{align*}
|a_{n}-a|\leq |b_{n}-a|<\epsilon 
\end{align*}

Also ist stets $|a_{n}-a|<\epsilon $ für alle $n\geq N$.

\end{proof*}

\begin{hint*}
Im Sandwichsatz muss die Ungleichung $b_{n}\leq a_{n}\leq c_{n}$ nicht notwendigerweise für alle $n\in \mathbb {N} $ erfüllt sein. Es genügt, wenn sie bis auf endlich viele Folgenglieder gilt, d.h. wenn es ein $n_{0}\in \mathbb {N} $ gibt, so dass $b_{n}\leq a_{n}\leq c_{n}$ für alle $n\geq n_{0}$ gilt.

\end{hint*}

\begin{example*}
Wir wollen zeigen, dass $\lim _{n\to \infty }{\sqrt[{n}]{c}}=1$ für eine beliebige, aber feste Konstante $c>0$.

Hierzu wollen wir den Sandwichsatz verwenden. Wir unterscheiden zwei Fälle:

\proofcase{1}{$c\geq 1$}
\begin{indentblock}
Wir wählen $n_{0}\in \mathbb {N} $, so dass $n_{0}\geq c$. Dann gilt für alle $n\in \mathbb {N} $ mit $n\geq n_{0}$ wegen $c\leq n$, dass ${\sqrt[{n}]{c}}\leq {\sqrt[{n}]{n}}$. Andererseits gilt wegen $1\leq c$, dass $1={\sqrt[{n}]{1}}\leq {\sqrt[{n}]{c}}$ für alle $n\in \mathbb {N} $.

Da, wie wir bereits in einem vorherigen Kapitel gezeigt haben, $\lim _{n\to \infty }1=1=\lim _{n\to \infty }{\sqrt[{n}]{n}}$, gilt nach dem Sandwichsatz $\lim _{n\to \infty }{\sqrt[{n}]{c}}=1$.

\end{indentblock}

\proofcase{2}{$c<1$}
\begin{indentblock}
In diesem Fall gilt ${\tfrac {1}{c}}\geq 1$, also können wir den ersten Fall auf ${\tfrac {1}{c}}$ anwenden. Also gilt $\lim _{n\to \infty }{\sqrt[{n}]{\tfrac {1}{c}}}=1$ und mit der Quotientenregel folgt:

\begin{align*}
\lim _{n\to \infty }{\sqrt[{n}]{c}}=\lim _{n\to \infty }{\frac {1}{\sqrt[{n}]{\frac {1}{c}}}}={\frac {1}{\lim _{n\to \infty }{\sqrt[{n}]{c}}}}={\frac {1}{1}}=1
\end{align*}

\end{indentblock}

\end{example*}

\chapter{Monotoniekriterium}

In diesem Kapitel werden wir beweisen, dass monotone und beschränkte Folgen konvergieren. Wenn du also zeigen kannst, dass eine Folge beschränkt und monoton ist, dann muss diese konvergieren. Das Schöne dabei: Für diesen Beweis musst du den Grenzwert der Folge nicht kennen!

So ist dieser Satz bei Konvergenzbeweisen für rekursiv definierte Folgen hilfreich, denn bei rekursiv definierten Folgen ist eine Abschätzung des Abstands zwischen dem Grenzwert und dem Folgenglied oft schwierig.

\section{Konvergenz monotoner und beschränkter Folgen}

\begin{theorem*}[Monotoniekriterium für Folgen]
Jede monotone und beschränkte Folge reeller Zahlen konvergiert.

\end{theorem*}

\begin{explanation*}[Monotoniekriterium für Folgen]
Diesen Satz kannst du folgendermaßen nachvollziehen: Versuche, auf der Zahlengerade eine divergente, monotone und beschränkte Folge einzuzeichnen. Zur Erinnerung: Eine Folge ist monoton, wenn entweder immer $a_{n+1}\geq a_{n}$ oder immer $a_{n+1}\leq a_{n}$ gilt. Du wirst schnell merken, dass dies nicht möglich ist. Es macht also Sinn, dass dieser Satz gilt. Wie lässt er sich beweisen?

\end{explanation*}

\begin{proof*}[Monotoniekriterium für Folgen]
Beschränken wir uns zunächst auf monoton wachsende Folgen. Für monoton fallende Folgen ist der Beweis analog. Sei also $(a_{n})_{n\in \mathbb {N} }$ eine monoton steigende und beschränkte Folge. Zunächst müssen wir den Grenzwert dieser Folge bestimmen, um mit Hilfe der Epsilon-Definition des Grenzwerts die Konvergenz zu beweisen. Schauen wir uns hierzu die Folge $a_{n}={\tfrac {n}{n+1}}$ als Beispiel an:

\begin{align*}
a_{n}={\frac {n}{n+1}}={\frac {1}{1+{\frac {1}{n}}}}{\xrightarrow {n\to \infty }}1
\end{align*}

Der Grenzwert ist also 1. In welcher Relation steht aber 1 zur Folge? Die Folgenglieder steigen und nähern sich dabei immer mehr dem Grenzwert an. Der Grenzwert sollte also gleich dem Supremum aller Folgenglieder sein. Und tatsächlich: 1 ist gleich dem Supremum aller Folgenglieder $\left\{{\tfrac {n}{n+1}}:n\in \mathbb {N} \right\}$.

Diese Überlegung lässt sich auf beliebige monoton steigende Folgen verallgemeinern. Generell sollte das Supremum aller Folgenglieder gleich dem gesuchten Grenzwert sein. Setzen wir also

\begin{align*}
a=\sup\{a_{n}:n\in \mathbb {N} \}
\end{align*}

Dieses Supremum existiert, weil $(a_{n})_{n\in \mathbb {N} }$ beschränkt und damit insbesondere die Menge der Folgenglieder nach oben beschränkt ist.

Führen wir nun den Grenzwertbeweis durch: Sei $\epsilon >0$ beliebig. In Abhängigkeit zum gegebenen $\epsilon $ müssen wir ein $N\in \mathbb {N} $ finden, so dass $|a_{n}-a|<\epsilon $ für alle $n\geq N$ ist.

Wir wissen, dass es ein $a_{N}$ geben muss, so dass $a_{N}$ größer als $a-\epsilon $ ist. $a-\epsilon $ kann nämlich keine obere Schranke der Menge der Folgenglieder sein ($a$ ist als Supremum die kleinste obere Schranke). Weil $a-\epsilon $ keine obere Schranke der Folgenglieder ist, muss es mindestens ein größeres Folgenglied $a_{N}$ als $a-\epsilon $ geben. Außerdem ist $a\geq a_{N}$, da $a$ als Supremum eine obere Schranke aller Folgenglieder ist. Wir haben somit

\begin{align*}
a\geq a_{N}>a-\epsilon 
\end{align*}

Hieraus folgt

\begin{align*}
|a-a_{N}|<\epsilon 
\end{align*}

Da unsere Folge monoton wächst, müssen alle Folgenglieder nach $a_{N}$ größer gleich $a_{N}$ sein. Es ist also $a_{n}\geq a_{N}$ für alle $n\geq N$. Außerdem ist $a\geq a_{n}$, weil $a$ eine obere Schranke der Folgenglieder ist. Für $n\geq N$ haben wir

\begin{align*}
a\geq a_{n}\geq a_{N}>a-\epsilon 
\end{align*}

und damit

\begin{align*}
|a-a_{n}|\leq |a-a_{N}|<\epsilon 
\end{align*}

Dies zeigt, dass $a$ Grenzwert der Folge ist und somit $(a_{n})_{n\in \mathbb {N} }$ konvergiert. Der Beweis für monoton fallende Folgen ist analog. Hier muss man entsprechend das Infimum wählen.

\end{proof*}

\chapter{Konvergenzbeweise rekursiver Folgen}

In diesem Kapitel werde ich dir zeigen, wie du Konvergenzbeweise für rekursive Folgen führen kannst. Hier werden wir eine gute Anwendung des Monotoniekriteriums kennenlernen.

\section{Problemstellung}

Nehme folgende Aufgabe:

\begin{importantparagraph*}
Sei $(a_{n})_{n\in \mathbb {N} }$ eine durch $a_{0}=0$ und durch $a_{n+1}={\tfrac {1}{2}}a_{n}+{\tfrac {1}{2}}$ rekursiv definierte Folge. Konvergiert diese Folge? Wenn ja, wie lautet der Grenzwert?

\end{importantparagraph*}

Die Anwendung der Epsilon-Definition der Konvergenz ist in dieser Aufgabe schwierig. Weil die Folge $(a_{n})_{n\in \mathbb {N} }$ rekursiv definiert ist, können wir ihren Grenzwert nicht direkt ablesen. Auch sind im Allgemeinen Abschätzungen für den Term $|a-a_{n}|$ mit einer reellen Zahl schwierig, weil wir keine explizite Form des Folgenglieds $a_{n}$ kennen.

\section{Lösungsstrategien}

In diesem Kapitel werde ich dir die folgenden zwei Lösungswege präsentieren, um die Konvergenz einer Folge zu zeigen:

\begin{enumerate}
\item \emph{Explizite Bildungsvorschriften:} Man kann versuchen, eine explizite Bildungsvorschrift der gegebenen Folge zu bestimmen, um mit dieser das Konvergenzverhalten der Folge weiter zu untersuchen.
\item \emph{Monotoniekriterium verwenden:} Wenn die rekursiv gegebene Folge konvergieren sollte, kann man versuchen, das Monotoniekriterium anzuwenden. Nachdem die Konvergenz der Folge bewiesen wurde, kann man dieses Wissen nutzen, um den Grenzwert der Folge zu bestimmen.
\end{enumerate}

\section{Lösungsweg: Explizite Bildungsvorschrift finden}

Eine mögliche Lösungsstrategie ist die, eine explizite Bildungsvorschrift von $(a_{n})_{n\in \mathbb {N} }$ zu bestimmen.

Hier bietet es sich an, die ersten Folgenglieder rekursiv auszurechnen. Dies ist generell empfehlenswert, um ein Gefühl für das Konvergenzverhalten der Folge zu bekommen. Die ersten Folgeglieder lauten:

\begin{align*}
a_{0}&=0\\[0.3em]a_{1}&={\frac {1}{2}}a_{0}+{\frac {1}{2}}={\frac {1}{2}}\cdot 0+{\frac {1}{2}}={\frac {1}{2}}\\[0.3em]a_{2}&={\frac {1}{2}}a_{1}+{\frac {1}{2}}={\frac {1}{2}}\cdot {\frac {1}{2}}+{\frac {1}{2}}={\frac {3}{4}}\\[0.3em]a_{3}&={\frac {1}{2}}a_{2}+{\frac {1}{2}}={\frac {1}{2}}\cdot {\frac {3}{4}}+{\frac {1}{2}}={\frac {7}{8}}\\[0.3em]a_{4}&={\frac {1}{2}}a_{3}+{\frac {1}{2}}={\frac {1}{2}}\cdot {\frac {7}{8}}+{\frac {1}{2}}={\frac {15}{16}}\\[0.3em]&\vdots 
\end{align*}

Aufgrund dieser Liste der ersten Folgenglieder können wir vermuten, dass die Folge gegen $1$ konvergiert. Für die explizite Bildungsvorschrift müssen wir eine Zuordnungsvorschrift $n\mapsto a_{n}$ finden. Wir suchen also einen Term $f(x)$ für den gilt:

\begin{align*}
f(0)&=0\\[0.3em]f(1)&={\frac {1}{2}}\\[0.3em]f(2)&={\frac {3}{4}}\\[0.3em]f(3)&={\frac {7}{8}}\\[0.3em]&\vdots 
\end{align*}

Wir sehen, dass der Nenner jeweils eine Potenz von $2$ ist. Außerdem ist der Zähler jeweils die Vorgängerzahl des Nenners. Es ist also

\begin{align*}
{\begin{array}{rll}f(0)&=0&={\frac {2^{0}-1}{2^{0}}}\\[0.3em]f(1)&={\frac {1}{2}}&={\frac {2^{1}-1}{2^{1}}}\\[0.3em]f(2)&={\frac {3}{4}}&={\frac {2^{2}-1}{2^{2}}}\\[0.3em]f(3)&={\frac {7}{8}}&={\frac {2^{3}-1}{2^{3}}}\\[0.3em]&\vdots \end{array}}
\end{align*}

Anhand dieser Beispiele liegt die Vermutung nahe, dass $f(n)={\frac {2^{n}-1}{2^{n}}}$ wählen können. Es sollte also gelten

\begin{align*}
a_{n}=f(n)={\frac {2^{n}-1}{2^{n}}}=1-{\frac {1}{2^{n}}}=1-\left({\frac {1}{2}}\right)^{n}
\end{align*}

Diese Vermutung müssen wir noch beweisen. Hier bietet sich wie in vielen anderen Beispielen die vollständige Induktion an. Im Induktionsanfang haben wir:

\begin{align*}
a_{0}=0=1-1=1-\left({\frac {1}{2}}\right)^{0}
\end{align*}

Den Induktionsschritt von $n$ nach $n+1$ können wir nach Anwendung des Rekursionsschritts führen:

\begin{align*}
a_{n+1}&={\frac {1}{2}}{\color {Teal}a_{n}}+{\frac {1}{2}}\\[0.3em]&{\color {OliveGreen}\left\downarrow \ {\text{Induktionsvoraussetzung}}\right.}\\[0.3em]&={\frac {1}{2}}\cdot {\color {Teal}\left(1-\left({\frac {1}{2}}\right)^{n}\right)}+{\frac {1}{2}}\\[0.3em]&={\frac {1}{2}}-\left({\frac {1}{2}}\right)^{n+1}+{\frac {1}{2}}\\[0.3em]&=1-\left({\frac {1}{2}}\right)^{n+1}
\end{align*}

Damit haben wir $a_{n}=1-\left({\frac {1}{2}}\right)^{n}$ gezeigt. Nun können wir das Konvergenzverhalten mit den Mitteln untersuchen, die wir bereits kennengelernt haben. Beispielsweise haben wir bereits $\lim _{n\to \infty }\left({\frac {1}{2}}\right)^{n}=0$ bewiesen und damit können wir die Grenzwertsätze anwenden, um die Konvergenz der Folge zu beweisen:

\begin{align*}
\lim _{n\to \infty }a_{n}&=\lim _{n\to \infty }1-\left({\frac {1}{2}}\right)^{n}\\[0.3em]&{\color {OliveGreen}\left\downarrow \ {\text{Grenzwertsatz: }}\lim _{n\to \infty }(a_{n}+b_{n})=\lim _{n\to \infty }a_{n}+\lim _{n\to \infty }b_{n}\right.}\\[0.3em]&=\lim _{n\to \infty }1-\lim _{n\to \infty }\left({\frac {1}{2}}\right)^{n}\\[0.3em]&=1-0=1
\end{align*}
\clearpage
\section{Lösungsweg: Monotoniekriterium anwenden}

\subsection{Schritt 1: Beweis der Konvergenz}

Was machen wir, wenn wir keine explizite Bildungsvorschrift der Folge finden? In manchen Fällen kann das Monotoniekriterium helfen. Dieses Kriterium lautet:

\begin{importantparagraph*}
Jede beschränkte und monotone Folge konvergiert.

\end{importantparagraph*}

Dementsprechend reicht es aus, wenn wir die Beschränktheit und die Monotonie der Folge zeigen. Schauen wir uns zunächst die ersten Folgeglieder an, um eine Vermutung über die Eigenschaften der Folge zu bekommen:

\begin{align*}
a_{0}&=0\\[0.3em]a_{1}&={\frac {1}{2}}a_{0}+{\frac {1}{2}}={\frac {1}{2}}\cdot 0+{\frac {1}{2}}={\frac {1}{2}}\\[0.3em]a_{2}&={\frac {1}{2}}a_{1}+{\frac {1}{2}}={\frac {1}{2}}\cdot {\frac {1}{2}}+{\frac {1}{2}}={\frac {3}{4}}\\[0.3em]a_{3}&={\frac {1}{2}}a_{2}+{\frac {1}{2}}={\frac {1}{2}}\cdot {\frac {3}{4}}+{\frac {1}{2}}={\frac {7}{8}}\\[0.3em]a_{4}&={\frac {1}{2}}a_{3}+{\frac {1}{2}}={\frac {1}{2}}\cdot {\frac {7}{8}}+{\frac {1}{2}}={\frac {15}{16}}\\[0.3em]&\vdots 
\end{align*}

Die Folge scheint monoton zu steigen. Außerdem sieht es so aus, als ob die Folge nach oben durch $1$ beschränkt ist.

Die Monotonie können wir induktiv beweisen. Hier zeigen wir zunächst im Induktionsanfang, dass $a_{1}\geq a_{0}$ ist:

\begin{align*}
a_{1}={\frac {1}{2}}a_{0}+{\frac {1}{2}}={\frac {1}{2}}\cdot 0+{\frac {1}{2}}={\frac {1}{2}}\geq 0=a_{0}
\end{align*}

Der Induktionsschritt von $n$ nach $n+1$ ist:

\begin{align*}
a_{n+1}&={\frac {1}{2}}{\color {Teal}a_{n}}+{\frac {1}{2}}\\[0.3em]&{\color {OliveGreen}\left\downarrow \ {\text{Induktionsvoraussetzung: }}a_{n}\geq a_{n-1}\right.}\\[0.3em]&\geq {\frac {1}{2}}{\color {Teal}a_{n-1}}+{\frac {1}{2}}=a_{n}
\end{align*}

Damit ist das monotone Wachstum der Folge bewiesen. Auch die Beschränktheit der Folge nach oben durch $1$ kann mit Hilfe vollständiger Induktion gezeigt werden. Hier ist wegen $a_{0}=0$ der Induktionsanfang $a_{0}\leq 1$ direkt gegeben. Im Induktionsschritt haben wir:

\begin{align*}
a_{n+1}&={\frac {1}{2}}{\color {Teal}a_{n}}+{\frac {1}{2}}\\[0.3em]&{\color {OliveGreen}\left\downarrow \ {\text{Induktionsvoraussetzung: }}a_{n}\leq 1\right.}\\[0.3em]&\leq {\frac {1}{2}}{\color {Teal}1}+{\frac {1}{2}}=1
\end{align*}

Damit ist die Folge $(a_{n})_{n\in \mathbb {N} }$ nach oben durch $1$ beschränkt. Jetzt können wir auch die Konvergenz zeigen: Weil die Folge monoton steigt und nach oben beschränkt ist, konvergiert die Folge nach dem Monotoniekriterium.

\subsection{Schritt 2: Grenzwert bestimmen}

Um den Grenzwert der rekursiven Folge zu bestimmen, können wir die gerade bewiesene Konvergenz ausnutzen. Wir wissen so nämlich, dass es ein $a\in \mathbb {R} $ mit $\lim _{n\to \infty }a_{n}=a$ gibt, wobei uns der exakte Wert von $a$ noch unbekannt ist.

Um $a$ zu bestimmen, betrachten wir den Limes $\lim _{n\to \infty }a_{n+1}$. Auch dieser Grenzwert ist $a$ und somit erhalten wir

\begin{align*}
a&=\lim _{n\to \infty }a_{n+1}\\[0.5em]&{\color {OliveGreen}\left\downarrow \ {\text{Rekursionsschritt: }}a_{n+1}={\frac {1}{2}}a_{n}+{\frac {1}{2}}\right.}\\[0.5em]&=\lim _{n\to \infty }{\frac {1}{2}}a_{n}+{\frac {1}{2}}\\[0.5em]&{\color {OliveGreen}\left\downarrow \ {\text{Grenzwertsätze}}\right.}\\[0.5em]&=\left(\lim _{n\to \infty }{\frac {1}{2}}\right)\cdot \left(\lim _{n\to \infty }a_{n}\right)+\lim _{n\to \infty }{\frac {1}{2}}\\[0.5em]&={\frac {1}{2}}a+{\frac {1}{2}}
\end{align*}

Der Wert $a$ muss damit die Gleichung $a={\tfrac {1}{2}}a+{\tfrac {1}{2}}$ erfüllen. Aufgrund dieser Bedingung können wir den exakten Wert von $a$ bestimmen:

\begin{align*}
a={\frac {1}{2}}a+{\frac {1}{2}}\iff {\frac {1}{2}}a={\frac {1}{2}}\iff a=1
\end{align*}

Damit ist $a=1$ der Grenzwert der Folge $(a_{n})_{n\in \mathbb {N} }$.

\part[Häufungspunkte und Cauchy-Folgen]{\changefontsizes[59pt]{40pt}\color{white} Häufungspunkte und Cauchy-Folgen}

\addxcontentsline{lof}{part}[\arabic{part}]{Häufungspunkte und Cauchy-Folgen}\begin{authors}
Stephan Kulla, Who2010, Autorenkollektiv „Auswahlaxiom“ (Charlotte Dietze, Matthias Paulsen, Anne Reif), Akram Chawki, Christoph Kehle, Ekin Köksal, Mingliaozi, Phoible, 0-Brane, Meitnerium266, PhilippHanemann, Menuja J. (MJ Studies), Jenny Kilian, Katharina Kircher, Werner Fröhlich, Nevsor, SerloBot, Matthias Greger, Letsluk, CommonsDelinker, Pierpao, Botho cc, Peter Gröbner, Richtefee, Digamma, Alexander Sedlmayr, Paolo Martinoni\end{authors}

\chapter{Teilfolgen}

\section{Einführendes Beispiel}

Manchmal ist es notwendig, nur über eine Unterfolge einer Folge zu sprechen. Solche Unterfolgen werden in der Mathematik \emph{Teilfolge} genannt. Dieser Name ist ganz intuitiv: Teilfolgen bezeichnen einen \emph{Teil} einer Folge. Eine Teilfolge entsteht dadurch, dass in einer gegebenen Folge beliebige Folgenglieder entfernt werden. Beim Streichen der Folgenglieder müssen aber unendlich viele Folgenglieder übrig bleiben. Die übrig geliebenen Folgenglieder bilden dann eine Teilfolge der ursprünglichen Folge. Nehmen wir zum Beispiel die Folge $a_{n}=(-1)^{n}$:

\begin{align*}
-1,\ 1,\ -1,\ 1,\ -1,\ \ldots 
\end{align*}

Wir interessieren uns nun für die Teilfolge jedes zweiten Folgenglieds. Diese entsteht, indem wir alle Folgenglieder mit ungeradem Index streichen:

\begin{align*}
{\begin{array}{rccccccc}{\text{Originalfolge}}\colon &{\cancel {-1}},\ &{\color {OliveGreen}1},\ &{\cancel {-1}},\ &{\color {OliveGreen}1},&{\cancel {-1}},\ &{\color {OliveGreen}1},\ &\ldots \\{\text{Teilfolge}}\colon &&{\color {OliveGreen}1},\ &&{\color {OliveGreen}1},&&{\color {OliveGreen}1},\ &\ldots \end{array}}
\end{align*}

So entsteht eine Teilfolge, die konstant $1$ ist.

\section{Mathematische Beschreibung}

Wie können Teilfolgen notiert werden? Schauen wir uns zunächst die Indizes der Folgenglieder an, die in der Teilfolge enthalten sein sollen:

\begin{align*}
{\begin{array}{rccccccccccccc}{\text{Index }}n\colon &{\cancel {1}}&\ &{\color {RedOrange}2}&\ &{\cancel {3}}&\ &{\color {RedOrange}4}&\ &{\cancel {5}}&\ &{\color {RedOrange}6}&\ &\ldots \\&\downarrow &&\downarrow &&\downarrow &&\downarrow &&\downarrow &&\downarrow &&\\{\text{Folgenglied }}a_{n}\colon &{\cancel {-1}}&\ &{\color {OliveGreen}1}&\ &{\cancel {-1}}&\ &{\color {OliveGreen}1}&\ &{\cancel {-1}}&\ &{\color {OliveGreen}1}&\ &\ldots \end{array}}
\end{align*}

Jetzt suchen wir eine Folge $\left(n_{k}\right)_{k\in \mathbb {N} }$, die diese Indizes beschreibt. Im obigen Beispiel betrachten wir alle geraden Indizes. Also ist $n_{k}=2k$:

\begin{align*}
{\begin{array}{rccccccccccccc}{\text{Neuer Index }}k\colon &1&\ &2&\ &3&\ &4&\ &\ldots \\[0.5em]&\downarrow &&\downarrow &&\downarrow &&\downarrow &&\\[0.5em]{\text{Indexfolge }}n_{k}\colon &{\color {RedOrange}2}&\ &{\color {RedOrange}4}&\ &{\color {RedOrange}6}&\ &{\color {RedOrange}8}&\ &\ldots \end{array}}
\end{align*}

Diese Folge setzen wir in $\left(a_{n}\right)_{n\in \mathbb {N} }$ ein. Dadurch entsteht die Teilfolge $\left(a_{n_{k}}\right)_{k\in \mathbb {N} }$:

\begin{align*}
{\begin{array}{rccccccccccccc}{\text{Neuer Index }}k\colon &1&&2&&3&&4&&\ldots \\[0.5em]&\downarrow &&\downarrow &&\downarrow &&\downarrow &&\\[0.5em]{\text{Indexfolge }}n_{k}\colon &{\color {RedOrange}2}&&{\color {RedOrange}4}&&{\color {RedOrange}6}&&{\color {RedOrange}8}&&\ldots \\[0.5em]&\downarrow &&\downarrow &&\downarrow &&\downarrow &&\\[0.5em]{\text{Teilfolge }}a_{n_{k}}\colon &{\color {OliveGreen}(-1)^{2}=1}&&{\color {OliveGreen}(-1)^{4}=1}&&{\color {OliveGreen}(-1)^{6}=1}&&{\color {OliveGreen}(-1)^{8}=1}&&\ldots \end{array}}
\end{align*}

Zunächst bilden wir also die Folge $\left(n_{k}\right)_{k\in \mathbb {N} }$ der relevanten Indizes einer Teilfolge. Diese Teilfolge setzen wir dann in die Originalfolge $\left(a_{n}\right)_{n\in \mathbb {N} }$ für $n$ ein, sodass wir die Teilfolge $\left(a_{n_{k}}\right)_{k\in \mathbb {N} }$ erhalten.

In unserem Beispiel ist ${\color {RedOrange}n_{k}=2k}$. Wir setzen also ${\color {RedOrange}2k}$ für $n$ in $a_{n}=(-1)^{n}$ ein. So erhalten wir die Teilfolge ${\color {OliveGreen}a_{2k}=(-1)^{2k}=1}$.

\section{Definition}

\begin{definition*}[Teilfolge]
Sei $\left(a_{n}\right)_{n\in \mathbb {N} }$ eine beliebige Folge. Jede Folge $\left(a_{n_{k}}\right)_{k\in \mathbb {N} }$ heißt Teilfolge von $\left(a_{n}\right)_{n\in \mathbb {N} }$, wenn $\left(n_{k}\right)_{k\in \mathbb {N} }$ eine streng monoton steigende Folge natürlicher Zahlen ist.

\end{definition*}

Dieser Begriff ist wichtig für die Analysis, weil durch ihn Häufungspunkte charakterisiert werden können. Was Häufungspunkte genau sind, werden wir im nächsten Kapitel näher untersuchen.

\begin{hint*}
Jede Folge ist eine Teilfolge von sich selbst. Wenn man nämlich $n_{k}=k$ wählt, dann ist $\left(a_{n_{k}}\right)_{k\in \mathbb {N} }=\left(a_{k}\right)_{k\in \mathbb {N} }=\left(a_{n}\right)_{n\in \mathbb {N} }$. Für $n_{k}=k$ ist also die Teilfolge $\left(a_{n_{k}}\right)_{k\in \mathbb {N} }$ mit der ursprünglichen Folge $\left(a_{n}\right)_{n\in \mathbb {N} }$ identisch. Das zeigt, dass jede Folge eine Teilfolge von sich selbst ist.

\end{hint*}

\begin{exercise*}[Teilfolgen]
Gib fünf unterschiedliche Teilfolgen der Folge $\left({\tfrac {1}{n}}\right)_{n\in \mathbb {N} }$ an.

\end{exercise*}

\begin{solution*}[Teilfolgen]
Die Folge $\left({\tfrac {1}{n}}\right)_{n\in \mathbb {N} }$ besitzt unendlich viele Teilfolgen. Fünf davon sind

\begin{align*}
\left({\tfrac {1}{2k}}\right)_{k\in \mathbb {N} }&=\left({\tfrac {1}{2}},{\tfrac {1}{4}},{\tfrac {1}{6}},{\tfrac {1}{8}},{\tfrac {1}{10}},\ldots \right)\\\left({\tfrac {1}{2k-1}}\right)_{k\in \mathbb {N} }&=\left({\tfrac {1}{1}},{\tfrac {1}{3}},{\tfrac {1}{5}},{\tfrac {1}{7}},{\tfrac {1}{9}},\ldots \right)\\\left({\tfrac {1}{k^{2}}}\right)_{k\in \mathbb {N} }&=\left({\tfrac {1}{1}},{\tfrac {1}{4}},{\tfrac {1}{9}},{\tfrac {1}{16}},{\tfrac {1}{25}},\ldots \right)\\\left({\tfrac {1}{k+2}}\right)_{k\in \mathbb {N} }&=\left({\tfrac {1}{3}},{\tfrac {1}{4}},{\tfrac {1}{5}},{\tfrac {1}{6}},{\tfrac {1}{7}},\ldots \right)\\\left({\tfrac {1}{p(k)}}\right)_{k\in \mathbb {N} }&=\left({\tfrac {1}{2}},{\tfrac {1}{3}},{\tfrac {1}{5}},{\tfrac {1}{7}},{\tfrac {1}{11}},\ldots \right),{\text{ wobei }}p(k){\text{ die }}k{\text{-te Primzahl ist}}
\end{align*}

\end{solution*}

\section{Konvergenz von Teilfolgen}

Für Teilfolgen gibt es den folgenden wichtigen Satz:

\begin{theorem*}[Konvergenz von Teilfolgen]
Sei $(a_{n})_{n\in \mathbb {N} }$ eine Folge. $(a_{n})_{n\in \mathbb {N} }$ konvergiert genau dann, wenn jede Teilfolge konvergiert. Der Grenzwert der Folge stimmt mit den Grenzwerten ihrer Teilfolgen überein.

\end{theorem*}

\begin{proof*}[Konvergenz von Teilfolgen]
Um die Äquivalenz

\begin{align*}
{\begin{array}{c}(a_{n})_{n\,\in \,\mathbb {N} }{\text{ konvergiert gegen }}a\\[1ex]\Leftrightarrow \\[1ex]{\text{Jede Teilfolge von }}(a_{n})_{n\in \mathbb {N} }{\text{ konvergiert gegen }}a\end{array}}
\end{align*}

zu beweisen, beweisen wir die zwei Implikationen:

\begin{enumerate}
\item Wenn $(a_{n})_{n\in \mathbb {N} }$ gegen $a$ konvergiert, konvergiert auch jede Teilfolge von $(a_{n})_{n\in \mathbb {N} }$ gegen $a$.
\item Wenn jede Teilfolge von $(a_{n})_{n\in \mathbb {N} }$ gegen $a$ konvergiert, konvergiert $(a_{n})_{n\in \mathbb {N} }$ gegen $a$.
\end{enumerate}

Wir können den Beweis so führen, weil die Aussage $A\Leftrightarrow B$ äquivalent zu $(A\Rightarrow B)\land (B\Rightarrow A)$ ist.

\proofstep{Beweis der ersten Implikation „$\Rightarrow $“:}
 \begin{indentblock}
Sei $(a_{n})_{n\in \mathbb {N} }$ eine gegen $a$ konvergente Folge. Wir müssen beweisen, dass alle Teilfolgen von $(a_{n})_{n\in \mathbb {N} }$ auch gegen $a$ konvergieren.

Sei also $\left(a_{n_{k}}\right)_{k\in \mathbb {N} }$ eine Teilfolge von $(a_{n})_{n\in \mathbb {N} }$. Wir wollen nun zeigen, dass $\left(a_{n_{k}}\right)_{k\in \mathbb {N} }$ auch gegen $a$ konvergiert. Sei $\epsilon >0$ gegeben. Da $a$ der Grenzwert von $(a_{n})_{n\in \mathbb {N} }$ ist, existiert ein Index $N\in \mathbb {N} $, sodass für alle $n\geq N$ die Ungleichung $|a_{n}-a|<\epsilon $ erfüllt ist.

Da nach Definition die Folge $(n_{k})_{k\in \mathbb {N} }$ eine streng monoton steigende Folge ist, ist $n_{k}\geq k$ für alle $k\in \mathbb {N} $. Damit ist $n_{k}\geq N$ für alle $k\geq N$, denn aus $n_{k}\geq k$ und $k\geq N$ folgt $n_{k}\geq k\geq N$. Somit ist $\left|a_{n_{k}}-a\right|<\epsilon $ für alle $k\geq N$.

Insgesamt haben wir so bewiesen, dass es für alle $\epsilon >0$ ein $N\in \mathbb {N} $ mit $\left|a_{n_{k}}-a\right|<\epsilon $ für alle $k\geq N$ gibt. Nach Definition des Grenzwertes besitzt die Teilfolge $(a_{n_{k}})_{k\in \mathbb {N} }$ den Grenzwert $a$ und konvergiert somit. Da die Teilfolge $\left(a_{n_{k}}\right)_{k\in \mathbb {N} }$ beliebig gewählt war, gilt dieser Beweisschritt für alle Teilfolgen von $(a_{n})_{n\in \mathbb {N} }$.

\end{indentblock}

\proofstep{Beweis der zweiten Implikation „$\Leftarrow $“:}
 \begin{indentblock}
Wir wissen, dass alle Teilfolgen von $(a_{n})_{n\in \mathbb {N} }$ gegen $a$ konvergieren. Nun ist aber die Folge $(a_{n})_{n\in \mathbb {N} }$ eine Teilfolge von sich selbst. Also muss auch diese gegen $a$ konvergieren.

\end{indentblock}

\end{proof*}

\begin{example*}[Konvergenz von Teilfolgen]
Da $(a_{n})_{n\in \mathbb {N} }=\left({\tfrac {1}{n}}\right)_{n\in \mathbb {N} }$ eine Nullfolge ist, gilt auch für die beiden Grenzwerte

\begin{align*}
\lim _{k\to \infty }a_{2k}&=\lim _{k\to \infty }{\tfrac {1}{2k}}=0\\\lim _{k\to \infty }a_{2k-1}&=\lim _{k\to \infty }{\tfrac {1}{2k-1}}=0
\end{align*}

\end{example*}

\begin{hint*}
Aus obigem Satz folgt unmittelbar, dass eine konvergente Folge $(a_{n})_{n\in \mathbb {N} }$ ihren Grenzwert nicht ändert, wenn man endlich viele Folgeglieder streicht. Durch Streichen von endlich vielen Folgegliedern entsteht nämlich eine Teilfolge $(a_{n_{k}})_{k\in \mathbb {N} }$ von $(a_{n})_{n\in \mathbb {N} }$. Diese Teilfolge hat nach dem eben bewiesenen Satz denselben Grenzwert.

\end{hint*}

Aus dem obigen Satz folgt direkt:

\begin{theorem*}[Divergenz bei Divergenz einer Teilfolge]
Wenn eine Teilfolge divergiert, muss auch die ursprüngliche Folge divergieren.

\end{theorem*}

\begin{example*}[Divergenz von Teilfolgen]
Betrachten wir die Folge $(a_{n})_{n\in \mathbb {N} }$ mit $a_{n}=(-1)^{n}n$. Diese hat die Folge $(a_{2k})_{k\in \mathbb {N} }=(2k)_{k\in \mathbb {N} }$ als Teilfolge. Da $(2k)_{k\in \mathbb {N} }$ eine unbeschränkte Folge ist, divergiert diese Teilfolge. Nach dem gerade bewiesenen Satz divergiert dann auch $(a_{n})_{n\in \mathbb {N} }$.

\end{example*}

\chapter{Häufungspunkte von Folgen}

\section{Achtung: Verschiedene Arten von Häufungspunkten}

Beim Begriff „Häufungspunkt“ müssen wir aufpassen. Es gibt nämlich zwei verschiedene Arten von Häufungspunkten in der Mathematik: Häufungspunkte von \emph{Folgen} und Häufungspunkte von \emph{Mengen}. Obwohl beide Begriffe eng miteinander verwandt sind, müssen wir zwischen ihnen unterscheiden. In Vorlesungen und Übungen sollte man sich immer klar machen, um welche Art von Häufungspunkt es gerade geht.

In diesem Kapitel werden Häufungspunkte von Folgen vorgestellt. Wenn wir also im Folgenden das Wort „Häufungspunkt“ benutzen, dann ist damit der Häufungspunkt einer Folge gemeint.

\section{Einleitendes Beispiel}

Wir stoßen auf den Begriff des Häufungspunkts, wenn wir uns das Grenzwertverhalten bestimmter Folgen anschauen. Nehmen wir die Folge $a_{n}=(-1)^{n}{\tfrac {n}{n+1}}$. Sie hat die Folgenglieder

\begin{align*}
\left((-1)^{n}{\tfrac {n}{n+1}}\right)_{n\in \mathbb {N} }=\left(-{\tfrac {1}{2}},\,{\tfrac {2}{3}},\,-{\tfrac {3}{4}},\,{\tfrac {4}{5}},\,-{\tfrac {5}{6}},\,{\tfrac {6}{7}},\,\ldots \right)
\end{align*}

Im Diagramm sieht die Folge so aus:

\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:First_elements_of_sequence_with_two_accumulation_points.svg}{\textbf{First\allowbreak\_elements\allowbreak\_of\allowbreak\_sequence\allowbreak\_with\allowbreak\_two\allowbreak\_accumulation\allowbreak\_points.svg}} by Stephan Kulla \textit{(CC0)}}\begin{center}
\adjincludegraphics[max width=0.5\textwidth,max height=0.2\textheight]{file58first95elements95of95sequence95with95two95accumulation95points95f91ffc5e0baedd4c55ac2f9ae4b862bb0133ee15}\end{center}

Wir sehen zunächst, dass die Folge keinen Grenzwert besitzt. Es gibt nämlich keinen eindeutigen Wert, gegen den sie strebt. Dennoch können wir ein gewisses Grenzwertverhalten ausmachen: Ein Teil der Folge scheint gegen $1$ und der andere Teil gegen $-1$ zu streben.

Für dieses „Streben eines Teils der Folge“ gibt es den Begriff des Häufungspunkts. Wir werden sehen, dass $1$ und $-1$ die beiden Häufungspunkte der Folge $a_{n}=(-1)^{n}{\tfrac {n}{n+1}}$ sind. Fassen wir also zusammen:

\begin{importantparagraph*}
Häufungspunkte sind Werte, gegen die ein Teil einer Folge strebt.

\end{importantparagraph*}

Diese intuitive Beschreibung müssen wir noch in eine mathematisch exakte Definition umformulieren.

\section{Definition des Häufungspunkts}

Wie kann man die Intuition „Streben eines Teils einer Folge“ allein durch mathematische Begriffe ausdrücken? Im letzten Kapitel haben wir das Konzept einer Teilfolge kennengelernt. Es liegt nahe, die Umschreibung „Teil einer Folge“ durch den Begriff „Teilfolge“ zu ersetzen. Genauso können wir „Streben eines Teils einer Folge“ durch {''}Streben einer Teilfolge{''} ersetzen.

Wir müssen noch konkretisieren, was {''}Streben einer Teilfolge{''} gegen einen Wert sein soll. Die Konvergenz einer Folge beschreibt die intuitive Idee, dass eine Folge gegen einen Grenzwert strebt. Wir können also die Umschreibung

\begin{importantparagraph*}
„…ein Teil einer Folge strebt gegen einen Wert“

\end{importantparagraph*}

durch folgende Formulierung ersetzen:

\begin{importantparagraph*}
„…eine Teilfolge konvergiert gegen einen Wert“

\end{importantparagraph*}

So erhalten wir folgende Definition des Häufungspunkts:

\begin{definition*}[Häufungspunkt einer Folge]
Eine Zahl $a$ ist Häufungspunkt einer Folge $(a_{n})_{n\in \mathbb {N} }$, wenn es eine Teilfolge $\left(a_{n_{k}}\right)_{k\in \mathbb {N} }$ der Folge $(a_{n})_{n\in \mathbb {N} }$ gibt, die gegen diese Zahl $a$ konvergiert.

\end{definition*}

\section{Beispiele}

Im einführenden Beispiel hatten wir intuitiv festgestellt, dass die Folge $a_{n}=(-1)^{n}{\tfrac {n}{n+1}}$ die Häufungspunkte $1$ und $-1$ besitzt. Können wir dies auch mit unserer Definition des Häufungspunkts nachweisen? Finden wir also zwei Teilfolgen von $a_{n}=(-1)^{n}{\tfrac {n}{n+1}}$, die einmal gegen $1$ und einmal gegen $-1$ konvergieren? Zur Wiederholung: Die Folge $a_{n}=(-1)^{n}{\tfrac {n}{n+1}}$ besitzt die Folgenglieder:

\begin{align*}
-{\tfrac {1}{2}},\,{\tfrac {2}{3}},\,-{\tfrac {3}{4}},\,{\tfrac {4}{5}},\,-{\tfrac {5}{6}},\,{\tfrac {6}{7}},\,\ldots 
\end{align*}

Im Diagramm ergibt sich folgendes Bild der Folge:

\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Sequence_with_two_accumulation_points_(subsequences_colored_differently).svg}{\textbf{Sequence\allowbreak\_with\allowbreak\_two\allowbreak\_accumulation\allowbreak\_points\allowbreak\_(subsequences\allowbreak\_colored\allowbreak\_differently).svg}} by Stephan Kulla \textit{(CC0)}}\begin{center}
\adjincludegraphics[max width=0.5\textwidth,max height=0.2\textheight]{file58sequence95with95two95accumulation95points9540subsequences95colored95differently4195c5bc6a947b0139d01d8e83dd37be573c2f248b7a}\end{center}

Anscheinend strebt die Teilfolge der geraden Folgenglieder gegen $1$. Schauen wir uns diese Teilfolge an:

\begin{align*}
{\cancel {-{\tfrac {1}{2}}}},\,{\color {OliveGreen}{\tfrac {2}{3}}},\,{\cancel {-{\tfrac {3}{4}}}},\,{\color {OliveGreen}{\tfrac {4}{5}}},\,{\cancel {-{\tfrac {5}{6}}}},\,{\color {OliveGreen}{\tfrac {6}{7}}},\,\ldots 
\end{align*}

Diese Teilfolge ${\color {OliveGreen}(c_{k})_{k\in \mathbb {N} }}$ besitzt die explizite Bildungsvorschrift

\begin{align*}
c_{k}=a_{2k}=(-1)^{2k}{\tfrac {2k}{2k+1}}={\tfrac {2k}{2k+1}}
\end{align*}

Und tatsächlich: Mit Hilfe der Grenzwertsätze können wir nachweisen, dass diese Folge gegen $1$ konvergiert:

\begin{align*}
\lim _{k\rightarrow \infty }c_{k}&=\lim _{k\rightarrow \infty }{\frac {2k}{2k+1}}=\lim _{k\rightarrow \infty }{\frac {2k}{2k\left(1+{\frac {1}{2k}}\right)}}=\lim _{k\rightarrow \infty }{\frac {1}{1+{\frac {1}{2k}}}}\\[1em]&{\color {OliveGreen}\left\downarrow \ {\text{Grenzwertsätze}}\right.}\\[1em]&={\frac {\lim _{k\rightarrow \infty }1}{\lim _{k\rightarrow \infty }1+\lim _{k\rightarrow \infty }{\frac {1}{2k}}}}={\frac {1}{1+0}}=1
\end{align*}

$1$ ist also ein Häufungspunkt der Folge $a_{n}=(-1)^{n}{\tfrac {n}{n+1}}$, weil es mit $(c_{k})_{k\in \mathbb {N} }$ eine gegen $1$ konvergente Teilfolge gibt. Um nachzuweisen, dass $-1$ ein Häufungspunkt der Folge ist, können wir uns die Teilfolge der ungeraden Folgenglieder anschauen:

\begin{align*}
{\color {Blue}-{\tfrac {1}{2}}},\,{\cancel {\tfrac {2}{3}}},\,{\color {Blue}-{\tfrac {3}{4}}},\,{\cancel {\tfrac {4}{5}}},\,{\color {Blue}-{\tfrac {5}{6}}},\,{\cancel {\tfrac {6}{7}}},\,\ldots 
\end{align*}

Auch hier können wir mit den Grenzwertsätzen nachweisen, dass diese Teilfolge gegen $-1$ konvergiert. Dass $1$ und $-1$ die beiden Häufungspunkte der Folge sind, erkennt man auch, wenn man alle Folgenglieder auf der Zahlengeraden einzeichnet. Hier sind die Häufungspunkte die Zahlen, bei denen sich die Glieder der Folge „häufen“:

\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Rational_sequence_with_2_accumulation_points_svg.svg}{\textbf{Rational\allowbreak\_sequence\allowbreak\_with\allowbreak\_2\allowbreak\_accumulation\allowbreak\_points\allowbreak\_svg.svg}} by Jochen Burghardt \textit{(CC BY-SA 3.0)}}\begin{center}
\adjincludegraphics[max width=0.5\textwidth,max height=0.2\textheight]{file58rational95sequence95with95295accumulation95points95svg95e09da08887cb9102a99cad00984b3d69196266a6}\end{center}

\section{Alternative Definition des Häufungspunkts}

\subsection{Umgebungsdefinition des Häufungspunkts}

Den Grenzwert einer Folge hatten wir dadurch charakterisiert, dass in jeder Umgebung des Grenzwerts fast alle Folgenglieder liegen. Für Häufungspunkte gibt es eine ähnliche Charakterisierung: Eine Zahl ist Häufungspunkt einer Folge, wenn in jeder Umgebung um den Punkt unendlich viele Folgenglieder liegen. Der Unterschied zum Grenzwert liegt darin, dass sich in jeder Umgebung um den Häufungspunkt nur unendlich viele und nicht fast alle Folgenglieder befinden müssen.

Zur Erinnerung: Eine $\epsilon $-Umgebung einer Zahl $h$ ist ein offenes Intervall $(h-\epsilon ,h+\epsilon )$ mit $\epsilon >0$.

Damit $h$ ein Häufungspunkt der Folge sein kann, müssen sich also unendlich viele Folgenglieder in jedem Intervall der Form $(h-\epsilon ,h+\epsilon )$ befinden. Nun liegt ein Folgenglied $a_{n}$ genau dann im offenen Intervall $(h-\epsilon ,h+\epsilon )$, wenn $|a_{n}-h|<\epsilon $ ist. Also muss es unendlich viele Indizes $n\in \mathbb {N} $ mit $|a_{n}-h|<\epsilon $ geben. Die alternative Definition des Häufungspunkts lautet:

\begin{definition*}[Umgebungsdefinition des Häufungspunkts]
Eine Folge $(a_{n})_{n\in \mathbb {N} }$ besitzt den Häufungspunkt $h\in \mathbb {R} $, wenn sich in jeder Umgebung von $h$ unendlich viele Folgenglieder von $(a_{n})_{n\in \mathbb {N} }$ befinden. Für alle $\epsilon >0$ muss es also unendlich viele Indizes $n\in \mathbb {N} $ mit $|a_{n}-h|<\epsilon $ geben.

\end{definition*}

Diese Definition zeigt, dass der Häufungspunktbegriff eine Abschwächung des Grenzwertbegriffs ist. Bei Grenzwerten müssen in jeder $\epsilon $-Umgebung des Grenzwerts fast alle Folgenglieder liegen. Nur endlich viele Folgenglieder dürfen sich außerhalb befinden. Demgegenüber müssen bei Häufungspunkten nur unendlich viele Folgenglieder in jeder $\epsilon $-Umgebung sein. Es können also auch unendlich viele Folgenglieder außerhalb der $\epsilon $-Umgebung liegen.

\begin{example*}[Alternative Definition des Häufungspunkts]
Als Beispiel betrachten wir die Folge $(a_{n})_{n\in \mathbb {N} }=((-1)^{n})_{n\in \mathbb {N} }$. Diese hat $1$ als Häufungspunkt. Für jedes noch so kleine $\epsilon >0$ liegen nämlich alle Folgenglieder mit geradem Index $a_{2k}$, also unendlich viele, in der Umgebung $(1-\epsilon ,1+\epsilon )$. Formal schreiben wir dafür $a_{2k}=(-1)^{2k}=1\in (1-\epsilon ,1+\epsilon )$ für alle $k\in \mathbb {N} $ und alle $\epsilon >0$.

Allerdings liegen \emph{nicht} fast alle Folgenglieder für alle $\epsilon >0$ in $(1-\epsilon ,1+\epsilon )$. Für $\epsilon ={\tfrac {1}{2}}$ zum Beispiel liegen auch unendlich viele Folgenglieder außerhalb dieser Umgebung, nämlich alle mit ungeradem Index $a_{2k-1}$. Formal: $a_{2k-1}=(-1)^{2k-1}=-1\notin ({\tfrac {1}{2}},{\tfrac {3}{2}})=(1-\epsilon ,1+\epsilon )$ für alle $k\in \mathbb {N} $.

\end{example*}

\subsection{Beweis der Äquivalenz}

Nun müssen wir beweisen, dass beide Definitionen äquivalent sind. Genau dann, wenn eine Zahl nach der Teilfolgen-Definition ein Häufungspunkt ist, muss sie auch nach der Umgebungsdefinition Häufungspunkt der Folge sein (und umgekehrt). Wir müssen also folgenden Satz beweisen:

\begin{theorem*}[Äquivalenz der beiden Häufungspunkt-Definitionen]
Sei $(a_{n})_{n\in \mathbb {N} }$ eine Folge. Für eine Zahl $h\in \mathbb {R} $ sind folgende Definitionen äquivalent:

\begin{itemize}
\item $(a_{n})_{n\in \mathbb {N} }$ besitzt eine gegen $h$ konvergente Teilfolge.
\item In jeder Umgebung von $h$ liegen unendlich viele Folgenglieder von $(a_{n})_{n\in \mathbb {N} }$.
\end{itemize}

\end{theorem*}

\begin{proof*}[Äquivalenz der beiden Häufungspunkt-Definitionen]
\proofstep{Beweisschritt 1:}
 Wenn es eine gegen $h$ konvergente Teilfolge gibt, dann liegen in jeder $\epsilon $-Umgebung von $h$ unendlich viele Folgenglieder.\begin{indentblock}
Wir wissen, dass eine Teilfolge der Folge gegen $h$ konvergiert. Sei nun $U$ eine beliebige Umgebung von $h$. Aus der Definition der Konvergenz folgt, dass fast alle Folgenglieder der Teilfolge in der Umgebung liegen müssen. Nun sind aber fast alle Folgenglieder der Teilfolge unendlich viele Folgenglieder der ursprünglichen Folge. Jedes Folgenglied der Teilfolge ist nämlich auch ein Folgenglied der ursprünglichen Folge. Damit haben wir gezeigt, dass in $U$ unendlich viele Folgenglieder liegen.

\end{indentblock}

\proofstep{Beweisschritt 2:}
 Wenn in jeder Umgebung von $h$ unendlich viele Folgenglieder liegen, dann gibt es eine gegen $h$ konvergente Teilfolge.\begin{indentblock}
Wir wissen, dass in jeder Umgebung von $h$ unendlich viele Folgenglieder liegen. Diesen Umstand müssen wir ausnutzen, um eine Teilfolge zu finden, die gegen $h$ konvergiert. Hierfür wählen wir immer kleiner werdende Umgebungen, zum Beispiel die offenen Intervalle

\begin{align*}
U_{n}=\left(h-{\tfrac {1}{n}},h+{\tfrac {1}{n}}\right)
\end{align*}

Wir haben also folgende Folge von Intervallen:

\begin{align*}
U_{1}&=\left(h-1,h+1\right)\\U_{2}&=\left(h-{\tfrac {1}{2}},h+{\tfrac {1}{2}}\right)\\U_{3}&=\left(h-{\tfrac {1}{3}},h+{\tfrac {1}{3}}\right)\\U_{4}&=\left(h-{\tfrac {1}{4}},h+{\tfrac {1}{4}}\right)\\&\ \vdots 
\end{align*}

Wir wissen, dass jedes dieser offenen Intervalle unendlich viele Folgenglieder besitzt. Dies müssen wir ausnutzen, um eine gegen $h$ konvergente Teilfolge $(t_{n})_{n\in \mathbb {N} }$ zu finden. Diese Teilfolge können wir folgendermaßen rekursiv definieren:

\begin{itemize}
\item $t_{1}$ ist irgendein Folgenglied in $U_{1}$.
\item Seien $t_{1},t_{2},\ldots ,t_{n-1}$ bereits bestimmt. Als nächstes Folgenglied $t_{n}$ der Teilfolge wählen wir irgendein Folgenglied aus $U_{n}$, sodass der Index von $t_{n}$ in der ursprünglichen Folge $(a_{n})_{n\in \mathbb {N} }$ größer als alle Indizes von $t_{1},t_{2},\ldots ,t_{n-1}$ ist. (Dies ist notwendig, damit $(t_{n})_{n\in \mathbb {N} }$ auch wirklich eine Teilfolge ist.)
\end{itemize}

Wegen $t_{n}\in U_{n}$ ist $|h-t_{n}|<{\tfrac {1}{n}}$, da für alle $x\in U_{n}$ die Ungleichung $|h-x|<{\tfrac {1}{n}}$ gilt. Dies beweist, dass $\lim _{n\to \infty }t_{n}=h$ ist.

\end{indentblock}

\end{proof*}

\section{Zusammenhang Grenzwert – Häufungspunkte}

\subsection{Häufungspunkte als Verallgemeinerung von Grenzwerten}

Aus beiden Definitionen des Häufungspunkts folgt direkt, dass jeder Grenzwert auch Häufungspunkt ist. Nach Definition konvergiert eine Folge genau dann, wenn in jeder Umgebung um den Häufungspunkt fast alle Folgenglieder liegen. Damit liegen aber auch in jeder Umgebung unendlich viele Folgenglieder, denn „fast alle Folgenglieder“ bedeutet „alle Folgenglieder bis auf endlich viele Ausnahmen“ und dies impliziert „unendlich viele Folgenglieder“. Dies zeigt, dass jeder Grenzwert auch Häufungspunkt ist:

\begin{theorem*}[Jeder Grenzwert ist Häufungspunkt]
Bei konvergenten Folgen ist der Grenzwert der Folge auch ein Häufungspunkt.

\end{theorem*}

\begin{mdframed}[style=semanticbox,frametitleaboveskip=3pt,innerbottommargin=3pt,frametitle=Frage]
Warum ist jeder Grenzwert auch nach der Teilfolgen-Definition ein Häufungspunkt?

\end{mdframed}
\pagebreak
\begin{answer*}
Wir wissen: Wenn eine Folge konvergiert, muss jede Teilfolge der Folge gegen den Grenzwert konvergieren. Damit gibt es aber eine Teilfolge der Folge, die gegen den Grenzwert konvergiert. Wir können schlicht eine beliebige wählen. Nach der Teilfolgen-Definition des Häufungspunkts ist damit jeder Grenzwert auch Häufungspunkt.

\end{answer*}

Umgekehrt ist aber nicht jeder Häufungspunkt ein Grenzwert. Im einführenden Beispiel hatten wir eine Folge mit zwei Häufungspunkten kennengelernt. Jedoch wissen wir, dass es höchstens einen Grenzwert pro Folge geben kann. Der Grenzwert ist eindeutig. Die Häufungspunkte im einführenden Beispiel sind demnach keine Grenzwerte.

Halten wir fest: Jeder Grenzwert ist Häufungspunkt, aber nicht jeder Häufungspunkt ist Grenzwert. Damit ist der Begriff des Häufungspunkts eine Verallgemeinerung des Grenzwertbegriffs. Außerdem können wir festhalten: Da jeder Grenzwert ein Häufungspunkt ist und eine konvergente Folge genau einen Grenzwert besitzt, müssen alle Folgen divergieren, die keinen oder mehr als einen Häufungspunkt besitzen:

\begin{theorem*}
Jede Folge mit mehr als einem oder mit keinem Häufungspunkt divergiert.

\end{theorem*}

\begin{mdframed}[style=semanticbox,frametitleaboveskip=3pt,innerbottommargin=3pt,frametitle=Frage]
Konvergiert jede Folge mit genau einem Häufungspunkt? Beweise deine Aussage!

\end{mdframed}

\begin{answer*}
Nein, dem ist nicht so. Nehmen wir zum Beispiel die Folge

\begin{align*}
a_{n}={\begin{cases}n,&n{\text{ ist gerade}}\\0,&n{\text{ ist ungerade}}\end{cases}}
\end{align*}

Diese Folge divergiert, weil sie unbeschränkt ist. Jedoch besitzt sie 0 als einzigen Häufungspunkt.

\end{answer*}

\pagebreak
\subsection{Vergleich Häufungspunkt – Grenzwert}
\renewcommand{\arraystretch}{1.5}

\begin{longtabu} to \linewidth {X[l]X[l]} \\ \toprule 
Grenzwert & Häufungspunkt \\ 
\midrule
\textbf{Jede} Teilfolge konvergiert gegen den Grenzwert. & \textbf{Mindestens eine} Teilfolge konvergiert gegen den Häufungspunkt. \\ 
In jeder $\epsilon $-Umgebung liegen \textbf{fast alle} Folgenglieder. & In jeder $\epsilon $-Umgebung liegen \textbf{unendlich viele} Folgenglieder. \\ 
Eine Folge hat \textbf{höchstens einen} Grenzwert. & Es kann \textbf{beliebig viele} Häufungspunkte geben. \\ 
Jeder Grenzwert ist Häufungspunkt. & Es gibt Häufungspunkte, die keine Grenzwerte sind. \\ 
\bottomrule
\end{longtabu}
\renewcommand{\arraystretch}{1.0}

\chapter{Satz von Bolzano-Weierstraß}

In diesem Kapitel besprechen wir einen Satz, der für viele Beweise hilfreich ist: Der \emph{Satz von Bolzano-Weierstraß}, welcher nach \href{https://de.wikipedia.org/wiki/Bernard\%20Bolzano}
{Bernard Bolzano} und \href{https://de.wikipedia.org/wiki/Karl\%20Weierstraß}
{Karl Weierstraß} benannt ist.

Dieser Satz garantiert die Existenz von Häufungspunkten bei beschränkten Folgen und wird oft verwendet, um die Existenz von Grenzwerten oder Häufungspunkten zu zeigen. Zwar könnte zum Nachweis dieser Existenz auch das Intervallschachtelungsprinzip herangezogen werden, der Weg über den Satz von Bolzano-Weierstraß ist aber oftmals einfacher.

So wird in einigen Lehrbüchern mit Hilfe des Satzes von Bolzano-Weierstraß das Monotoniekriterium für Folgen und Reihen gezeigt. Auch kann mit ihm das Theorem bewiesen werden, dass stetige Funktionen auf abgeschlossenen Intervallen der Form $[a,b]$ mit $a,b\in \mathbb {R} $ beschränkt sind und ihr Maximum und Minimum annehmen.

\section{Der Satz von Bolzano-Weierstraß}

Der Satz von Bolzano-Weierstraß lautet:

\begin{theorem*}[Satz von Bolzano-Weierstraß]
Jede beschränkte Folge $(x_{n})_{n\in \mathbb {N} }$ von reellen Zahlen besitzt mindestens einen Häufungspunkt. Es gibt also eine reelle Zahl $x$, so dass mindestens eine Teilfolge $\left(x_{n_{k}}\right)_{k\in \mathbb {N} }$ von $(x_{n})_{n\in \mathbb {N} }$ gegen $x$ konvergiert.

\end{theorem*}

Diesen Satz kannst du so nachvollziehen: Eine Folge ist genau dann beschränkt, wenn es ein Intervall $[s,S]$ gibt, so dass alle Folgenglieder in diesem Intervall liegen. Nun hat eine Folge unendlich viele Glieder. Wenn man sie alle in das endliche Intervall $[s,S]$ sperrt, gibt es ein ziemliches Gedränge und die Folgenglieder müssen sich zwangsweise zum Teil sehr nah kommen. Nun sagt der Satz von Bolzano-Weierstraß, dass es mindestens eine reelle Zahl $x$ gibt, der die Glieder einer Teilfolge beliebig nah kommen. Diese Zahl $x$ ist Häufungspunkt der Folge. Beachte, dass $x$ selbst kein Glied der Folge sein muss. Auch könnte es insgesamt mehr als einen Häufungspunkt geben.

Wie bereits erwähnt, wird der Satz von Bolzano-Weierstraß bei Existenzbeweisen genutzt. So kann mit diesem Satz ein gesuchter Grenzwert oder ein gesuchter Häufungspunkt gefunden werden. Dabei ist die Anwendung dieses Satzes oft einfacher als die Benutzung des Supremumaxioms oder des Intervallschachtelungsprinzips. Der Grund liegt darin, dass die Beschränktheit einer Folge oft leicht nachgewiesen werden kann. Demgegenüber lässt sich das Supremumaxiom nur dann gut anwenden, wenn die gesuchte Zahl Supremum einer gegebenen Menge ist und beim Intervallschachtelungsprinzip muss man eine geeignete Intervallschachtelung konstruieren bzw. finden, die die gesuchte Zahl beliebig genau approximiert.

\begin{hint*}
In der Literatur wird manchmal der Satz von Bolzano-Weierstraß in der Form „Jede reelle beschränkte Folge besitzt eine konvergente Teilfolge“ formuliert. Der Satz „Jede beschränkte Folge reeller Zahlen besitzt einen Häufungspunkt“ wird dann als \emph{Satz vom Häufungspunkt} bezeichnet.

\end{hint*}

\section{Notwendigkeit des Vollständigkeitsaxioms}

Für den Satz von Bolzano-Weierstraß ist die Vollständigkeit der reellen Zahlen eine notwendige Eigenschaft. Um dies zu sehen, nehmen wir als Grundmenge die rationalen Zahlen. Hier betrachten wir eine Folge rationaler Zahlen, die gegen eine irrationale Zahl konvergieren würde. Weil diese Folge konvergiert, muss sie auch beschränkt sein (wir hatten bereits nachgewiesen, dass jede konvergente Folge beschränkt ist). Außerdem ist der irrationale Grenzwert der einzige Häufungspunkt der Folge (jeder Grenzwert ist alleiniger Häufungspunkt einer Folge).

Nun besitzt die gewählte Folge keinen Häufungspunkt mehr, weil wir beim Wechsel von $\mathbb {R} $ nach $\mathbb {Q} $ den einzigen Häufungspunkt der Folge entfernt haben. Damit kann für die Grundmenge der rationalen Zahlen der Satz von Bolzano-Weierstraß nicht gelten. Schließlich kann es beschränkte rationale Folgen geben, die keine rationalen Häufungspunkte besitzen. Dieses Beispiel zeigt, wie wichtig die Grundmenge für den Satz ist und dass wir zum Beweis das Vollständigkeitsaxiom der reellen Zahlen benötigen werden. Schließlich ist die Vollständigkeit die wesentliche Eigenschaft, die die reellen von den rationalen Zahlen unterscheidet.
\clearpage
\section{Beweis (mit Intervallschachtelung)}

\begin{proof*}[Satz von Bolzano-Weierstraß]
Sei $(x_{n})_{n\in \mathbb {N} }$ eine beschränkte Folge. Wir wissen, dass es ein $s$ und ein $S$ gibt, so dass alle Folgenglieder in $[s,S]$ liegen (die Folge ist beschränkt):

\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Bolzano–Weierstrass_theorem_-_step_1.svg}{\textbf{Bolzano–Weierstrass\allowbreak\_theorem\allowbreak\_\allowbreak-\allowbreak\_step\allowbreak\_1.svg}} by Stephan Kulla \textit{(CC BY-SA 4.0)}}\begin{center}
\adjincludegraphics[max width=0.5\textwidth,max height=0.2\textheight]{file58bolzano8211weierstrass95theorem954595step9519537be80fa29a24561c2b8b4ae53989c8c459c0ad3}\end{center}

Nun müssen wir einen Häufungspunkt der Folge finden. Wir wissen bereits, dass wir hierfür die Vollständigkeit von $\mathbb {R} $ benötigen, welche wir über das Intervallschachtelungsprinzip eingeführt haben. Zur Erinnerung: Eine Intervallschachtelung ist eine Möglichkeit, eine Zahl durch eine Folge von immer kleiner werdenden Intervallen $I_{n}=[a_{n},b_{n}]$ zu approximieren. Die gesuchte Zahl $x$ ist dabei Element jedes Intervalls $[a_{n},b_{n}]$ der Intervallschachtelung. Dabei ist $a_{n}$ eine Abschätzung nach unten und $b_{n}$ eine Abschätzung nach oben von $x$, also $a_{n}\leq x\leq b_{n}$.

Wenn die Breite der Intervalle gegen $0$ konvergiert (also die Approximation beliebig genau wird), dann garantiert das Intervallschachtelungsprinzip, dass durch die Intervallschachtelung genau eine reelle Zahl beschrieben wird. Es gibt also genau eine reelle Zahl, die in allen Intervallen $[a_{n},b_{n}]$ liegt.

Fazit: Um den gesuchten Häufungspunkt $x$ von $(x_{n})_{n\in \mathbb {N} }$ zu finden, können wir eine Intervallschachtelung konstruieren, bei der jedes Intervall $x$ approximiert. Das Intervallschachtelungsprinzip garantiert uns dann die Existenz von $x$. Wie gehen wir bei der Konstruktion der Intervallschachtelung vor?

Zunächst setzen wir $[s,S]$ als erstes Intervall $I_{1}$. Wir wissen ja bereits, dass in diesem Intervall alle Folgenglieder liegen und damit sich auch der gesuchte Häufungspunkt in diesem Intervall befinden muss:

\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Bolzano–Weierstrass_theorem_-_step_2.svg}{\textbf{Bolzano–Weierstrass\allowbreak\_theorem\allowbreak\_\allowbreak-\allowbreak\_step\allowbreak\_2.svg}} by Stephan Kulla \textit{(CC BY-SA 4.0)}}\begin{center}
\adjincludegraphics[max width=0.5\textwidth,max height=0.2\textheight]{file58bolzano8211weierstrass95theorem954595step95295c9ded11530a7bfbe671e183204edfcf83a4ffb86}\end{center}

Jetzt teilen wir das Intervall in zwei gleich große Intervalle:

\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Bolzano–Weierstrass_theorem_-_step_3.svg}{\textbf{Bolzano–Weierstrass\allowbreak\_theorem\allowbreak\_\allowbreak-\allowbreak\_step\allowbreak\_3.svg}} by Stephan Kulla \textit{(CC BY-SA 4.0)}}\begin{center}
\adjincludegraphics[max width=0.5\textwidth,max height=0.2\textheight]{file58bolzano8211weierstrass95theorem954595step95395320fed91197bd4d8dc7a516b3d5b8cb68b3cb1a2}\end{center}

Da die Folge $(x_{n})_{n\in \mathbb {N} }$ unendlich viele Folgenglieder besitzt, müssen in mindestens einem der beiden Intervalle unendlich viele Folgenglieder von $(x_{n})_{n\in \mathbb {N} }$ liegen. Dieses Intervall setzen wir als das zweite Intervall $I_{2}$ der Intervallschachtelung. Da $I_{2}$ unendlich viele Folgenglieder der ursprünglichen Folge besitzt, sollte sich in diesem Intervall ein Häufungspunkt der ursprünglichen Folge befinden:

\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Bolzano–Weierstrass_theorem_-_step_4.svg}{\textbf{Bolzano–Weierstrass\allowbreak\_theorem\allowbreak\_\allowbreak-\allowbreak\_step\allowbreak\_4.svg}} by Stephan Kulla \textit{(CC BY-SA 4.0)}}\begin{center}
\adjincludegraphics[max width=0.5\textwidth,max height=0.2\textheight]{file58bolzano8211weierstrass95theorem954595step95495fe5ac6a50303e75de90885e6372fed214cf17266}\end{center}

Nun können wir wiederum $I_{2}$ in zwei gleich große Teilintervalle aufteilen:

\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Bolzano–Weierstrass_theorem_-_step_5.svg}{\textbf{Bolzano–Weierstrass\allowbreak\_theorem\allowbreak\_\allowbreak-\allowbreak\_step\allowbreak\_5.svg}} by Stephan Kulla \textit{(CC BY-SA 4.0)}}\begin{center}
\adjincludegraphics[max width=0.5\textwidth,max height=0.2\textheight]{file58bolzano8211weierstrass95theorem954595step95595e5f8bae1035c11f90252946e9da8913c983a7d2b}\end{center}

Als nächstes Intervall $I_{3}$ wählen wir das Teilintervall von $I_{2}$, welches wiederum unendlich viele Folgenglieder der ursprünglichen Folge besitzt. Wir wissen, dass $I_{3}$ existiert, weil sich bereits in $I_{2}$ unendlich viele Folgenglieder der ursprünglichen Folge befinden und damit auch in mindestens einem der zwei Teilintervalle unendlich Folgenglieder von $(x_{n})_{n\in \mathbb {N} }$ liegen müssen:

\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Bolzano–Weierstrass_theorem_-_step_6.svg}{\textbf{Bolzano–Weierstrass\allowbreak\_theorem\allowbreak\_\allowbreak-\allowbreak\_step\allowbreak\_6.svg}} by Stephan Kulla \textit{(CC BY-SA 4.0)}}\begin{center}
\adjincludegraphics[max width=0.5\textwidth,max height=0.2\textheight]{file58bolzano8211weierstrass95theorem954595step9569564e0603891c2eafba16f06c9bac10f498ee4211f}\end{center}

Dieses Verfahren wiederholen wir beliebig oft, so dass wir am Ende eine Intervallschachtelung $(I_{n})_{n\in \mathbb {N} }$ erhalten.

Formal können wir die Intervallschachtelung wie folgt beschreiben: Setze $I_{1}=[s,S]$. Ist $I_{k}=[a_{k},b_{k}]$ gegeben, so setze $M={\tfrac {1}{2}}(a_{k}+b_{k})$ und

\begin{align*}
I_{k+1}=[a_{k+1},b_{k+1}]={\begin{cases}\;[a_{k},M]&{\text{in }}[a_{k},M]{\text{ liegen unendlich viele }}a_{n}\\\;[M,b_{k}]&{\text{sonst}}\end{cases}}
\end{align*}

Die Breite der Intervalle $I_{n}$ halbiert sich bei jedem Schritt. Es ist

\begin{align*}
|I_{1}|&=|S-s|\\|I_{2}|&={\frac {1}{2}}|I_{1}|={\frac {1}{2}}|S-s|\\|I_{3}|&={\frac {1}{2}}|I_{2}|={\frac {1}{2^{2}}}|S-s|\\|I_{4}|&={\frac {1}{2}}|I_{3}|={\frac {1}{2^{3}}}|S-s|\\&\vdots \\|I_{n}|&={\frac {1}{2}}|I_{n-1}|={\frac {1}{2^{n-1}}}|S-s|
\end{align*}

Dabei ist $|I_{n}|$ die Breite des $n$-ten Intervalls. Es folgt

\begin{align*}
|I_{n}|={\frac {1}{2^{n-1}}}|S-s|{\xrightarrow {n\to \infty }}0
\end{align*}

Die Breite der Intervalle konvergiert gegen $0$ und damit gibt es nach dem Intervallschachtelungsprinzip genau einen Punkt $x$, der in allen Intervallen $I_{n}$ liegt. Dieser Punkt sollte Häufungspunkt der Folge sein. Das müssen wir aber noch beweisen.

Betrachten wir hierzu für ein beliebiges $\epsilon >0$ die $\epsilon $-Umgebung $(x-\epsilon ,x+\epsilon )$ von $x$. Weil $\lim _{n\to \infty }|I_{n}|=0$ ist, gibt es ein $N\in \mathbb {N} $ mit $|I_{N}|<\epsilon $. Wegen $x\in I_{N}$ ist damit aber $I_{N}\subseteq (x-\epsilon ,x+\epsilon )$. Nun hatten wir die Intervalle so konstruiert, dass in jedem Intervall unendlich viele Folgenglieder von $(x_{n})_{n\in \mathbb {N} }$ liegen. Also liegen auch in $I_{N}$ unendlich viele Folgenglieder der ursprünglichen Folge. Wegen $I_{N}\subseteq (x-\epsilon ,x+\epsilon )$ sind alle Elemente von $I_{N}$ auch Elemente von $(x-\epsilon ,x+\epsilon )$. Damit müssen aber auch in der $\epsilon $-Umgebung unendlich viele Folgenglieder von $(x_{n})_{n\in \mathbb {N} }$ liegen (nämlich mindestens diejenigen, die bereits in $I_{N}$ liegen).

Weil $\epsilon >0$ beliebig gewählt wurde, liegen in jeder $\epsilon $-Umgebung von $x$ unendlich viele Folgenglieder von $(x_{n})_{n\in \mathbb {N} }$. Dies beweist, dass $x$ Häufungspunkt von $(x_{n})_{n\in \mathbb {N} }$ ist, womit diese Folge mindestens einen Häufungspunkt besitzt.

\end{proof*}

\chapter{Bestimmte Divergenz}

Bisher haben wir vor allem die Konvergenz von Folgen untersucht. In diesem Kapitel werden wir uns mit divergenten Folgen beschäftigen. Hier können nämlich zwei Arten der Divergenz unterschieden werden: Bestimmte und unbestimmte Divergenz.

\section{Motivation}

Wenn wir uns divergente Folgen anschauen, dann gibt es Folgen wie $a_{n}=n$, $b_{n}=2^{n}$ und $c_{n}=-n+2\cdot (-1)^{n}$, die ein eindeutiges Streben gegen $+\infty $ oder $-\infty $ aufweisen:

\begin{tabularx}{\linewidth}{XX}
\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Arithmetische Folge.svg}{\textbf{Arithmetische Folge.svg}} by Stephan Kulla \textit{(CC BY-SA 3.0)}}\begin{minipage}[t]{\linewidth}
\begin{figure}[H]
\begin{minipage}[t][0.2\textheight][c]{\linewidth}
\centering
\adjincludegraphics[max width=1.\linewidth, max height=0.2\textheight]{file58arithmetische32folge959c2fb61091f31d0261b5e024d92a585034d20ac6}
\end{minipage}
\caption*{Die Folge $a_{n}=n$ strebt eindeutig gegen $+\infty $. (\arabic{imagelabel})}
\end{figure}

\end{minipage}
&
\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:First elements of sequence -n+2*(-1)^n.svg}{\textbf{First elements of sequence \allowbreak-n+2*(\allowbreak-1)\textasciicircum{}n.svg}} by Stephan Kulla \textit{(CC0)}}\begin{minipage}[t]{\linewidth}
\begin{figure}[H]
\begin{minipage}[t][0.2\textheight][c]{\linewidth}
\centering
\adjincludegraphics[max width=1.\linewidth, max height=0.2\textheight]{file58first32elements32of32sequence3245n43242404514194n95d9b543359d4a5fe5f14798cf57adf46fd6a26d90}
\end{minipage}
\caption*{Die Folge $c_{n}=-n+2\cdot (-1)^{n}$ strebt eindeutig gegen $-\infty $. (\arabic{imagelabel})}
\end{figure}

\end{minipage}
\end{tabularx}

Bei solchen Folgen werden wir sagen, dass sie bestimmt gegen $+\infty $ beziehungsweise gegen $-\infty $ divergieren. Demgegenüber gibt es bei Folgen wie $d_{n}=(-1)^{n}$ oder $e_{n}=(-1)^{n}\cdot n$ kein solches eindeutiges Streben. Die Folge $d_{n}=(-1)^{n}$ ist beschränkt und kann deswegen weder gegen $+\infty $ noch gegen $-\infty $ divergieren.

Die Folge $e_{n}=(-1)^{n}\cdot n$ ist zwar unbeschränkt, ihr Streben ist aber nicht eindeutig. Diese Folge besitzt nämlich Teilfolgen, die gegen $+\infty $ streben, und andere Teilfolgen, die gegen $-\infty $ streben:

\begin{tabularx}{\linewidth}{XX}
\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Alternierende Folge.svg}{\textbf{Alternierende Folge.svg}} by Stephan Kulla \textit{(CC BY-SA 3.0)}}\begin{minipage}[t]{\linewidth}
\begin{figure}[H]
\begin{minipage}[t][0.2\textheight][c]{\linewidth}
\centering
\adjincludegraphics[max width=1.\linewidth, max height=0.2\textheight]{file58alternierende32folge95115b34e53f591be661e9622f4f1b7bd17c2172c4}
\end{minipage}
\caption*{Die alternierende Folge $d_{n}=(-1)^{n}$ ist beschränkt und kann deswegen nicht gegen unendlich streben. (\arabic{imagelabel})}
\end{figure}

\end{minipage}
&
\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:First elements of sequence -n*(-1)^n.svg}{\textbf{First elements of sequence \allowbreak-n*(\allowbreak-1)\textasciicircum{}n.svg}} by Stephan Kulla \textit{(CC0)}}\begin{minipage}[t]{\linewidth}
\begin{figure}[H]
\begin{minipage}[t][0.2\textheight][c]{\linewidth}
\centering
\adjincludegraphics[max width=1.\linewidth, max height=0.2\textheight]{file58first32elements32of32sequence3245n42404514194n9568cd08cc9c4ee17d2aa67643eb4a1aca913f0c8b}
\end{minipage}
\caption*{Die Folge $e_{n}=(-1)^{n}\cdot n$ ist zwar unbeschränkt, besitzt aber auch kein eindeutiges Streben gegen $+\infty $ oder $-\infty $. (\arabic{imagelabel})}
\end{figure}

\end{minipage}
\end{tabularx}

\section{Definition}

Wir haben gesehen, dass die bestimmte Divergenz das eindeutige Streben einer Folgen gegen $+\infty $ oder gegen $-\infty $ ist. Wie kann dies mathematisch formuliert werden?

Beginnen wir mit der bestimmten Divergenz gegen $+\infty $: Wenn eine Folge gegen $+\infty $ strebt, dann wird diese Folge größer als jede Zahl – egal wie groß sie ist. Mehr noch: Egal wie groß man eine Zahl $S$ annimmt, fast alle Folgenglieder dieser Folge liegen über dieser Zahl. Es existiert also ein Index $N$, ab dem alle folgenden Folgenglieder größer gleich $S$ sind. Damit wird die Zahl $S$ ab dem Index $N$ nicht mehr unterschritten. Dies ist dann auch die Definition der bestimmten Divergenz gegen $+\infty $:

\begin{definition*}[Bestimmte Divergenz gegen $+\infty $]
Eine Folge $(a_{n})_{n\in \mathbb {N} }$ divergiert bestimmt gegen $+\infty $, wenn für jede Zahl $S\in \mathbb {R} $ fast alle Folgenglieder größer oder gleich $S$ sind. Für alle reellen Zahlen $S$ gibt es also einen Index $N$, sodass $a_{n}\geq S$ für alle $n\geq N$ ist:

\begin{align*}
\forall S\in \mathbb {R} \,\exists N\in \mathbb {N} \,\forall n\geq N:a_{n}\geq S
\end{align*}

\end{definition*}

Wir können die Aussageform der bestimmten Divergenz gegen unendlich so übersetzen:

\begin{align*}
\underbrace {{\underset {}{}}\forall S\in \mathbb {R} } _{{\text{Für jede reelle Zahl }}S}\ \underbrace {{\underset {}{}}\exists N\in \mathbb {N} } _{{\text{ existiert ein Mindestindex }}N,}\ \underbrace {{\underset {}{}}\forall n\geq N} _{{\text{sodass für alle Indizes }}n\geq N}\ \underbrace {{\underset {}{}}a_{n}\geq S} _{{\text{ das Folgenglied }}a_{n}{\text{ größer gleich }}S{\text{ ist}}}
\end{align*}

Analog können wir die bestimmte Divergenz gegen $-\infty $ definieren:

\begin{definition*}[Bestimmte Divergenz gegen $-\infty $]
Eine Folge $(a_{n})_{n\in \mathbb {N} }$ divergiert bestimmt gegen $-\infty $, wenn für jede Zahl $s\in \mathbb {R} $ fast alle Folgenglieder kleiner oder gleich $s$ sind. Für alle reellen Zahlen $s$ gibt es also einen Index $N$, sodass $a_{n}\leq s$ für alle $n\geq N$ ist:

\begin{align*}
\forall s\in \mathbb {R} \,\exists N\in \mathbb {N} \,\forall n\geq N:a_{n}\leq s
\end{align*}

\end{definition*}

\section{Schreibweise}

Wenn eine Folge $(a_{n})_{n\in \mathbb {N} }$ gegen $+\infty $ bestimmt divergiert, dann schreiben wir

\begin{align*}
\lim _{n\to \infty }a_{n}=+\infty 
\end{align*}

Analog benutzen wir folgende Schreibweise, wenn eine Folge $(a_{n})_{n\in \mathbb {N} }$ gegen $-\infty $ bestimmt divergiert:

\begin{align*}
\lim _{n\to \infty }a_{n}=-\infty 
\end{align*}

\section{Bestimmte Divergenz als uneigentliche Konvergenz}

Die Schreibweise $\lim _{n\to \infty }a_{n}=+\infty $ suggeriert, dass die Folge $(a_{n})_{n\in \mathbb {N} }$ gegen unendlich konvergiert. Hier liegt aber eine Divergenz und keine Konvergenz vor! Das Symbol $+\infty $ ist nämlich keine reelle Zahl. Konvergente Folgen dürfen per Definition aber nur reelle Zahlen als Grenzwerte besitzen. Es gibt allerdings Parallelen zwischen der Konvergenz und der bestimmten Divergenz:


\renewcommand{\arraystretch}{1.5}

\begin{longtabu} to \linewidth {X[l]X[l]} \\ \toprule 
Konvergenz & Bestimmte Divergenz \\ 
\midrule
In jeder $\epsilon $-Umgebung liegen fast alle Folgenglieder. & In jedem Intervall $[S,\infty )$ liegen fast alle Folgenglieder. \\ 
Alle Teilfolgen konvergieren gegen denselben Grenzwert. & Auch alle Teilfolgen divergieren bestimmt gegen $\pm \infty $. \\ 
Jede konvergente Folge ist beschränkt. & Jede bestimmt divergente Folge ist unbeschränkt. \\ 
\bottomrule
\end{longtabu}
\renewcommand{\arraystretch}{1.0}
Dementsprechend gibt es für bestimmte Divergenz auch den Begriff der \emph{uneigentlichen Konvergenz}. Das Wort „uneigentliche Konvergenz“ deutet darauf hin, dass die bestimmte Divergenz gewisse Ähnlichkeiten zur Konvergenz aufweist. Sie ist aber in ihrem Wesen eine Divergenz.

\begin{warning*}
Es ist wichtig, dass wir uns merken, dass die bestimmte Divergenz eine Art der Divergenz ist, obwohl sie der Konvergenz ähnelt und wir sie als uneigentliche Konvergenz bezeichnen. Wir dürfen also auf bestimmt divergente Folgen keine Rechenregeln anwenden, die nur für konvergente Folgen gelten. Ein Beispiel ist die Produktregel $\lim _{n\to \infty }(a_{n}\cdot b_{n})=\lim _{n\to \infty }a_{n}\cdot \lim _{n\to \infty }b_{n}$. Diese Regel gilt für bestimmt divergente Folgen nicht, wie die folgende Umformung zeigt:

\begin{align*}
1=\lim _{n\to \infty }\left(n\cdot {\frac {1}{n}}\right)=\lim _{n\to \infty }n\cdot \lim _{n\to \infty }{\frac {1}{n}}=\infty\cdot 0 =0
\end{align*}

Man erhält also die falsche Aussage $1=0$, wenn man die Produktregel auf bestimmt divergente Folgen anwendet. Wir müssen also vorsichtig sein, welche Rechenregeln wir auf bestimmt divergente Folgen anwenden. 

\end{warning*}

\chapter{Lim sup und Lim inf}

Der Limes superior und der Limes inferior ist der größte und der kleinste Häufungspunkt einer Folge. Diese dienen als partiellen Ersatz für den Grenzwert, wenn dieser nicht existiert.

\section{Motivation}

Der Grenzwert einer Folge ist diejenige Zahl, gegen die eine Folge im Unendlichen strebt. In jeder Umgebung um den Grenzwert liegen fast alle Folgenglieder (blaue Punkte in der folgenden Grafik) und damit befinden sich nur endlich viele Folgenglieder außerhalb einer beliebigen Umgebung (rote Punkte):

\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Epsilonschlauch2.svg}{\textbf{Epsilonschlauch2.svg}} by Autorenkollektiv „Auswahlaxiom“ (Charlotte Dietze, Matthias Paulsen, Anne Reif), Stephan Kulla \textit{(CC BY-SA 4.0)}}\begin{center}
\adjincludegraphics[max width=0.5\textwidth,max height=0.2\textheight]{file58epsilonschlauch2951c1661dd3fd063c56ca5fffe145305eeda4778be}\end{center}

Auch, wenn nicht jede Folge einen Grenzwert besitzt, kann man sowohl bei konvergenten als auch bei divergenten Folgen einiges über ihr Verhalten im Unendlichen aussagen. Im Kapitel \href{https://de.wikibooks.org/wiki/Mathe\_für\_Nicht-Freaks:\_Häufungspunkt\_einer\_Folge}
{„Häufungspunkt einer Folge“} haben wir bereits das Konzept des Häufungspunkts als Verallgemeinerung des Grenzwerts kennengelernt. Der Häufungspunkt ist eine Zahl, gegen die ein Teil der Folge strebt und um die sich deswegen die Folgenglieder „häufen“. Damit können sie benutzt werden, um das Verhalten einer Folge im Unendlichen zu beschreiben.

Durch Angabe des größten und des kleinsten Häufungspunkts können wir den Bereich einschränken, wo sich diese Häufungspunkte befinden. Der größte Häufungspunkt wird dabei \emph{Limes superior} und der kleinste \emph{Limes inferior} genannt. Dabei verwenden wir für den größten Häufungspunkt einer Folge $(a_{n})_{n\in \mathbb {N} }$ den Ausdruck $\limsup _{n\to \infty }a_{n}$ und für den kleinsten Häufungspunkt $\liminf _{n\to \infty }a_{n}$.

Das abgeschlossene Intervall $\left[\liminf _{n\to \infty }a_{n},\limsup _{n\to \infty }a_{n}\right]$ zwischen dem kleinsten und größten Häufungspunkt ist eine Art „verallgemeinertes Grenzwertintervall“. Wir können nämlich zeigen, dass sich bei beschränkten Folgen in jeder Umgebung um dieses Intervall fast alle Folgenglieder befinden. Außerhalb einer solchen Umgebung befinden sich nur endlich viele Folgenglieder. In der folgenden Abbildung ist dies für eine Epsilon-Umgebung $\left[\liminf _{n\to \infty }a_{n}-\epsilon ,\limsup _{n\to \infty }a_{n}+\epsilon \right]$ um dieses Intervall illustriert. Außerhalb dieses {''}Schlauchs{''} befinden sich nur endlich viele Folgenglieder (rote Punkte) und innerhalb fast alle (blaue Punkte):

\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Illustration_of_Limes_superior_and_Limes_Inferior.svg}{\textbf{Illustration\allowbreak\_of\allowbreak\_Limes\allowbreak\_superior\allowbreak\_and\allowbreak\_Limes\allowbreak\_Inferior.svg}} by Ekin Köksal \textit{(CC BY-SA 4.0)}}\begin{center}
\adjincludegraphics[max width=0.5\textwidth,max height=0.2\textheight]{file58illustration95of95limes95superior95and95limes95inferior955f111207d2a5b399481065f3c1b8d456190447d4}\end{center}

\section{Definition}

Wir wollen eine Folge genauer durch die Bestimmung des kleinsten und größten Häufungspunkts beschreiben. Dadurch schwächen wir den Grenzwertbegriff ab und gewinnen einen anderen Blick auf die Folge. Der größte Häufungspunkt wird \emph{Limes superior} genannt und wird bei einer Folge $(a_{n})_{n\in \mathbb {N} }$ mit $\limsup a_{n}$ bezeichnet. Der kleinste Häufungspunkt ist der \emph{Limes inferior} und wird als $\liminf a_{n}$ beschrieben:

\begin{definition*}[Limes superior]
Der Limes superior einer Folge $(a_{n})_{n\in \mathbb {N} }$ ist bei nach oben beschränkten Folgen der \emph{größte Häufungspunkt} dieser Folge und wird mit $\limsup a_{n}$ bezeichnet. Bei nach oben unbeschränkten Folgen schreiben wir $\limsup _{n\to \infty }a_{n}=\infty $.

\end{definition*}

\begin{definition*}[Limes inferior]
Der Limes inferior einer Folge $(a_{n})_{n\in \mathbb {N} }$ ist bei nach unten beschränkten Folgen der \emph{kleinste Häufungspunkt} dieser Folge und wird mit $\liminf a_{n}$ bezeichnet. Bei nach unten unbeschränkten Folgen setzen wir $\liminf _{n\to \infty }a_{n}=-\infty $.

\end{definition*}
\clearpage
\section{Beispiele}

\begin{example*}
Wir zerlegen die Folge $(a_{n})_{n\in \mathbb {N} }=(-1)^{n}\;n$ in ihre eigenen Bestandteile, indem wir dazu die Folgen $(b_{n})_{n\in \mathbb {N} }=(-1)^{n}$ und $(c_{n})_{n\in \mathbb {N} }=n$ definieren.

Die Teilfolgen der \emph{divergenten} Folge $b_{n}$ haben die Häufungspunkte $\lim \limits _{k\to \infty }b_{2k-1}=-1$ und $\lim \limits _{k\to \infty }b_{2k}=1$.

Die Folge $c_{n}$ ist dahingegen \emph{bestimmt divergent} mit $\lim \limits _{n\to \infty }c_{n}=\infty $.

Wir fügen nun die {''}Grenzwerte{''} von $b_{n}=(-1)^{n}$ und $c_{n}=n$ zusammen. Aufgrund von $\lim \limits _{k\to \infty }b_{2k-1}=-1$ ergibt sich die nach unten unbeschränkte Folge $\lim _{k\to \infty }a_{2k-1}=\lim _{k\to \infty }-2k+1=-\infty $. Da $\lim \limits _{k\to \infty }b_{2k}=1$ gilt, erhalten wir die nach oben unbeschränkte Folge $\lim _{k\to \infty }a_{2k}=\lim _{k\to \infty }2k=\infty $.

Somit gilt $\limsup _{n\to \infty }a_{n}=\infty $ und $\liminf _{n\to \infty }a_{n}=-\infty $.

\end{example*}

\begin{example*}
Ist $(a_{n})_{n\in \mathbb {N} }={\tfrac {(-1)^{n}}{n}}$, so ist $\lim _{n\to \infty }a_{n}=0$. Da die Folge $(a_{n})$ konvergiert, hat sie nur den Häufungspunkt $0$. Daher ist

\begin{align*}
\limsup _{n\to \infty }a_{n}=\liminf _{n\to \infty }a_{n}=0
\end{align*}

\end{example*}

\begin{example*}
Ist $(a_{n})_{n\in \mathbb {N} }=n$, so ist $\lim _{n\to \infty }a_{n}=\infty $. Also ist $(a_{n})$ nach oben unbeschränkt. Somit ist $\limsup _{n\to \infty }a_{n}=\infty $. Desweiteren ist $(a_{n})$ nach unten (durch $1$) beschränkt und besitzt keine Häufungspunkte, da $\forall S\in \mathbb {R} $ fast alle $a_{n}\geq S$ sind. Daher ist auch $\liminf _{n\to \infty }a_{n}=\infty $.

\end{example*}

\section{Zusammenhang mit Grenzwert}

Eine Folge konvergiert genau dann, wenn der Limes superior und der Limes inferior existieren und übereinstimmen:

\begin{theorem*}[Limes superior/inferior und Konvergenz]
Eine Folge $a_{n}$ konvergiert genau dann, wenn gilt:

\begin{align*}
-\infty <\limsup _{n\to \infty }a_{n}=\liminf _{n\to \infty }a_{n}<\infty 
\end{align*}

\end{theorem*}

\begin{hint*}
Der Satz lässt sich auch auf bestimmt divergente Folgen übertragen. Es gilt

\begin{align*}
\lim _{n\to \infty }a_{n}=\pm \infty \iff \liminf _{n\to \infty }a_{n}=\limsup _{n\to \infty }a_{n}=\pm \infty 
\end{align*}

\end{hint*}

\section{Alternative Charakterisierung von Limes Superior und Limes Inferior}

Ist $(a_{n})_{n\in \mathbb {N} }$ beschränkt, so lassen sich $\limsup _{n\to \infty }a_{n}$ und $\liminf _{n\to \infty }a_{n}$ auch wie folgt charakterisieren:

\begin{theorem*}[Alternative Definition von lim sup und lim inf]
Ist $(a_{n})_{n\in \mathbb {N} }$ eine beschränkte reelle Folge, so gilt

\begin{align*}
\limsup _{n\to \infty }a_{n}=\lim _{k\to \infty }\sup\{a_{n}:n\geq k\}=\inf _{k\in \mathbb {N} }\sup\{a_{n}:n\geq k\}\\\liminf _{n\to \infty }a_{n}=\lim _{k\to \infty }\inf\{a_{n}:n\geq k\}=\sup _{k\in \mathbb {N} }\inf\{a_{n}:n\geq k\}
\end{align*}

\end{theorem*}

\subsection{Beispiele}

Da diese Charakterisierungen von $\limsup $ und $\liminf $ am Anfang etwas abstrakt wirken, wollen wir sie zunächst an zwei Beispielen veranschaulichen:

\begin{example*}
Sei zunächst $(a_{n})_{n\in \mathbb {N} }=((-1)^{n})_{n\in \mathbb {N} }=(-1,1,-1,1,-1,1,\ldots )$. Entscheidend ist es nun, $(b_{k})_{k\in \mathbb {N} }=(\sup\{a_{n}:n\geq k\})_{k\in \mathbb {N} }$ und $({\tilde {b}}_{k})_{k\in \mathbb {N} }=(\inf\{a_{n}:n\geq k\})_{k\in \mathbb {N} }$ zu bestimmen. Dies ist hier aber nicht allzu schwer. Da die Folge $(a_{n})$ nur die Werte $\pm 1$ annimmt, und beide unendlich oft, ist $b_{k}=\sup\{a_{n}:n\geq k\}=1$ und ${\tilde {b}}_{k}=\inf\{a_{n}:n\geq k\}=-1$ für alle $k\in \mathbb {N} $. Damit ergibt sich

\begin{align*}
\limsup _{n\to \infty }a_{n}=\lim _{k\to \infty }b_{k}=\lim _{k\to \infty }1=1
\end{align*}

und

\begin{align*}
\liminf _{n\to \infty }a_{n}=\lim _{k\to \infty }{\tilde {b}}_{k}=\lim _{k\to \infty }-1=-1
\end{align*}

\end{example*}

\begin{example*}
Ist $(a_{n})_{n\in \mathbb {N} }=({\tfrac {(-1)^{n}}{n}})_{n\in \mathbb {N} }=(-1,{\tfrac {1}{2}},-{\tfrac {1}{3}},{\tfrac {1}{4}},-{\tfrac {1}{5}},{\tfrac {1}{6}},\ldots )$, so ist

\begin{align*}
(b_{k})_{k\in \mathbb {N} }=(\sup\{a_{n}:n\geq k\})_{k\in \mathbb {N} }=({\tfrac {1}{2}},{\tfrac {1}{2}},{\tfrac {1}{4}},{\tfrac {1}{4}},{\tfrac {1}{6}},{\tfrac {1}{6}},\ldots )
\end{align*}

Daraus folgt dann

\begin{align*}
\limsup _{n\to \infty }a_{n}=\lim _{k\to \infty }b_{k}=0
\end{align*}

\end{example*}

\begin{mdframed}[style=semanticbox,frametitleaboveskip=3pt,innerbottommargin=3pt,frametitle={Verständnisaufgabe}]
Bestimme analog $\liminf _{n\to \infty }a_{n}$.

\end{mdframed}

\begin{answer*}
Es gilt

\begin{align*}
({\tilde {b}}_{k})_{k\in \mathbb {N} }=(\inf\{a_{n}:n\geq k\})_{k\in \mathbb {N} }=(-1,-{\tfrac {1}{3}},-{\tfrac {1}{3}},-{\tfrac {1}{5}},-{\tfrac {1}{5}},-{\tfrac {1}{7}},-{\tfrac {1}{7}},\ldots )
\end{align*}

Und daher ist ebenfalls

\begin{align*}
\liminf _{n\to \infty }a_{n}=\lim _{k\to \infty }{\tilde {b}}_{k}=0
\end{align*}

\end{answer*}

\section{Rechenregeln für Limes Superior und Limes Inferior}

\begin{theorem*}[Monotonieregel]
Seien $(a_{n})_{n\in \mathbb {N} }$ und $(b_{n})_{n\in \mathbb {N} }$ beschränkte reelle Folgen mit $a_{n}\leq b_{n}$ für alle $n\in \mathbb {N} $. Dann gilt

\begin{align*}
\liminf _{n\to \infty }a_{n}\leq \liminf _{n\to \infty }b_{n}
\end{align*}

 und \begin{align*}
\limsup _{n\to \infty }a_{n}\leq \limsup _{n\to \infty }b_{n}
\end{align*}

\end{theorem*}

\begin{theorem*}[Zusammenhang limsup und liminf]
Sei $(a_{n})_{n\in \mathbb {N} }$ eine beschränkte reelle Folge. Dann gilt

\begin{align*}
-\limsup _{n\to \infty }a_{n}=\liminf _{n\to \infty }(-a_{n})
\end{align*}

\end{theorem*}

\begin{theorem*}[Summenregel]
Seien $(a_{n})_{n\in \mathbb {N} }$ und $(b_{n})_{n\in \mathbb {N} }$ reelle Folgen. Dann gilt

\begin{align*}
\limsup _{n\to \infty }a_{n}+\liminf _{n\to \infty }b_{n}\leq \limsup _{n\to \infty }(a_{n}+b_{n})\leq \limsup _{n\to \infty }a_{n}+\limsup _{n\to \infty }b_{n}
\end{align*}

\end{theorem*}

\begin{exercise*}[Beispiel zur Summenregel]
Finde konkrete Folgen $(a_{n})$ und $(b_{n})$ mit

\begin{align*}
\limsup _{n\to \infty }a_{n}+\liminf _{n\to \infty }b_{n}<\limsup _{n\to \infty }(a_{n}+b_{n})<\limsup _{n\to \infty }a_{n}+\limsup _{n\to \infty }b_{n}
\end{align*}

\end{exercise*}

\begin{proof*}[Beispiel zur Summenregel]
Beispielsweise können wir

\begin{align*}
a_{n}={\begin{cases}(-1)^{k}&{\text{ falls }}n=2k\\{\frac {1}{2}}&{\text{falls }}n=2k+1\end{cases}}
\end{align*}

 und \begin{align*}
b_{n}={\begin{cases}(-1)^{k+1}&{\text{ falls }}n=2k\\{\frac {1}{2}}&{\text{falls }}n=2k+1\end{cases}}
\end{align*}

wählen. Dann ist

\begin{align*}
(a_{n})=({\tfrac {1}{2}},-1,{\tfrac {1}{2}},1,{\tfrac {1}{2}},-1,{\tfrac {1}{2}},1,\ldots )
\end{align*}

 und \begin{align*}
(b_{n})=({\tfrac {1}{2}},1,{\tfrac {1}{2}},-1,{\tfrac {1}{2}},1,{\tfrac {1}{2}},-1,\ldots )
\end{align*}

Also haben $(a_{n})$ und $(b_{n})$ jeweils die Häufungspunkte $-1,{\tfrac {1}{2}}$ und $1$. Außerdem ist

\begin{align*}
a_{n}+b_{n}={\begin{cases}0&{\text{ falls }}n=2k\\1&{\text{falls }}n=2k+1\end{cases}}
\end{align*}

Somit hat $(a_{n}+b_{n})$ die Häufungspunkte $0$ und $1$. Damit gilt

\begin{align*}
\underbrace {\limsup _{n\to \infty }a_{n}+\liminf _{n\to \infty }b_{n}} _{=1+(-1)=0}<\underbrace {\limsup _{n\to \infty }(a_{n}+b_{n})} _{=1}<\underbrace {\limsup _{n\to \infty }a_{n}+\limsup _{n\to \infty }b_{n}} _{=1+1=2}
\end{align*}

\end{proof*}

\chapter{Cauchy-Folgen}

\section{Motivation}

In dem letzten Kapitel haben wir den Begriff des Grenzwerts einer Folge kennengelernt. Du hast auch gesehen, wie man die Konvergenz einer Folge mit Hilfe der Epsilon-Definition des Grenzwerts beweisen kann. Für den Konvergenzbeweis mit der Epsilon-Definition ist es aber notwendig, den Grenzwert der Folge zu kennen bzw. eine Vermutung zu haben, was der Grenzwert der Folge sein könnte.

Die Epsilon-Definition des Grenzwerts lautet nämlich: Zu jedem $\epsilon >0$ gibt es ein $N\in \mathbb {N} $, so dass die Ungleichung $|a_{n}-a|<\epsilon $ für $n\geq N$ erfüllt ist. Dabei ist $a$ der Grenzwert der Folge $(a_{n})_{n\in \mathbb {N} }$. Du siehst: In der Epsilon-Definition muss man den Grenzwert $a$ kennen. Doch wie können wir die Konvergenz einer Folge zeigen, wenn es sehr schwer oder sogar unmöglich ist, den Grenzwert der Folge zu bestimmen? Deshalb steht in diesem Kapitel folgende Frage im Vordergrund:

\begin{importantparagraph*}
Wie kann man beweisen, dass eine Folge konvergiert, ohne den Grenzwert dieser Folge zu kennen?

\end{importantparagraph*}

Wie würdest du dieses Problem lösen? Ein erster Ansatz ist folgende Hypothese:

\begin{importantparagraph*}
Eine Folge konvergiert genau dann, wenn der Abstand zwischen benachbarten Folgengliedern beliebig klein wird.

\end{importantparagraph*}

Diese Hypothese ist plausibel. Ist aber dieses Kriterium ausreichend? Leider nicht! Nimm zum Beispiel die Folge

\begin{align*}
(x_{n})_{n\in \mathbb {N} }=\left(1,\,2,\,2{\tfrac {1}{2}},\,3,\,3{\tfrac {1}{3}},\,3{\tfrac {2}{3}},\,4,\,4{\tfrac {1}{4}},\,4{\tfrac {2}{4}},\,\ldots \right)
\end{align*}

Die Folge wird beliebig groß und divergiert damit. Der Abstand benachbarter Folgenglieder wird aber beliebig klein. Hier siehst du, dass wir ein stärkeres Kriterium als unsere obige Hypothese benötigen. Cauchy-Folgen erfüllen genau dieses stärkere Kriterium.

\section{Herleitung von Cauchy-Folgen}

Nehmen wir die Epsilon-Eigenschaft des Grenzwerts und „spielen“ ein wenig damit herum. Wenn eine Folge $(a_{n})_{n\in \mathbb {N} }$ gegen $a$ konvergiert, dann wissen wir aus der Epsilon-Definition der Konvergenz:

\begin{align*}
\forall \epsilon >0\,\exists N\in \mathbb {N} \,\forall n\geq N:|a-a_{n}|<\epsilon 
\end{align*}

Fixieren wir ein $\epsilon >0$. Es gibt dann einen von $\epsilon $ abhängigen Index $N_{\epsilon }\in \mathbb {N} $, so dass $|a-a_{n}|<\epsilon $ für alle $n\geq N_{\epsilon }$ ist. Seien nun $n,m\geq N_{\epsilon }$. Es ist dann

\begin{align*}
|a-a_{n}|&<\epsilon \\|a-a_{m}|&<\epsilon 
\end{align*}

Insgesamt erhalten wir mit Hilfe der Dreiecksungleichung folgende Abschätzung für $|a_{n}-a_{m}|$:

\begin{align*}
|a_{n}-a_{m}|&=|(a_{n}-a)+(a-a_{m})|\\[0.5em]&{\color {OliveGreen}\left\downarrow \ {\text{Dreiecksungleichung: }}|x+y|\leq |x|+|y|\right.}\\[0.5em]&\leq \underbrace {|a_{n}-a|} _{<\ \epsilon }+\underbrace {|a-a_{m}|} _{<\ \epsilon }\\&<2\epsilon 
\end{align*}

Folgenglieder nach $a_{N_{\epsilon }}$ müssen also alle untereinander einen Abstand kleiner als $2\epsilon $ besitzen. Dies kann auch aus folgender Überlegung geschlussfolgert werden: Alle Folgenglieder nach $a_{N_{\epsilon }}$ müssen in der $\epsilon $-Umgebung $(a-\epsilon ,a+\epsilon )$ liegen:

\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Epsilon_Umgebung.svg}{\textbf{Epsilon\allowbreak\_Umgebung.svg}} by Stephan Kulla \textit{(CC-BY-SA-3.0)}}\begin{center}
\adjincludegraphics[max width=0.5\textwidth,max height=0.2\textheight]{file58epsilon95umgebung956834d136113839a7291e90f72f3206e5bad33c28}\end{center}

Obige Epsilon-Umgebung besitzt die Breite $2\epsilon $. Da alle Folgenglieder nach $a_{N_{\epsilon }}$ in dieser Umgebung liegen, muss ihr Abstand untereinander kleiner als $2\epsilon $ sein. Insgesamt haben wir also für die konvergente Folge $(a_{n})_{n\in \mathbb {N} }$ folgendes gezeigt:

\begin{align*}
\forall \epsilon >0\,\exists N\in \mathbb {N} \,\forall n,m\geq N:|a_{n}-a_{m}|<2\epsilon 
\end{align*}

Diesen Ausdruck können wir nun schöner schreiben. Hierzu setzen wir ${\tilde {\epsilon }}=2\epsilon $. Wenn $\epsilon $ alle positiven Zahlen durchläuft, dann durchläuft auch ${\tilde {\epsilon }}$ alle positiven Zahlen. Die Abbildung $x\mapsto 2x$ bildet nämlich $\mathbb {R} ^{+}$ bijektiv auf $\mathbb {R} ^{+}$ ab (diese Abbildung nutzen wir, wenn wir ${\tilde {\epsilon }}=2\epsilon $ setzen). Damit ist

\begin{align*}
\forall {\tilde {\epsilon }}>0\,\exists N\in \mathbb {N} \,\forall n,m\geq N:|a_{n}-a_{m}|<{\tilde {\epsilon }}
\end{align*}

Folgen mit dieser Eigenschaft werden \emph{Cauchy-Folgen} genannt. Du siehst, dass diese Definition nicht auf den Grenzwert einer Folge zurückgreift. Später werden wir sehen, dass eine reelle Folge genau dann konvergiert, wenn sie eine Cauchy-Folge ist. So kann man die Konvergenz einer Folge beweisen, ohne den Grenzwert kennen zu müssen.

\begin{hint*}
In den folgenden Abschnitten werden wir bei Cauchy-Folgen $\epsilon $ anstelle von ${\tilde {\epsilon }}$ nutzen. In diesem Abschnitt hatten wir bereits früher $\epsilon $ verwendet, weswegen wir die neue Variable ${\tilde {\epsilon }}$ eingeführt hatten.

\end{hint*}

\section{Definition von Cauchy-Folgen}

Fassen wir das bisher Hergeleitete zusammen:

\begin{definition*}[Cauchy-Folge]
Eine Folge $(a_{n})_{n\in \mathbb {N} }$ heißt \emph{Cauchy-Folge}, wenn es für alle $\epsilon >0$ eine natürliche Zahl $N\in \mathbb {N} $ gibt, so dass $|a_{n}-a_{m}|<\epsilon $ für alle $n,m\geq N$ ist.

\end{definition*}

Intuitiv gesprochen ist eine Folge genau dann eine Cauchy-Folge, wenn die Abstände der Folgenglieder untereinander beliebig klein werden. Beachte, dass hier mehr als nur der Abstand direkt benachbarter Folgenglieder gemeint ist. Zur Illustration:

\begin{tabularx}{\linewidth}{XX}
\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Cauchy sequence illustration.svg}{\textbf{Cauchy sequence illustration.svg}} by Krishnavedala \textit{(CC0)}}\begin{minipage}[t]{\linewidth}
\begin{figure}[H]
\begin{minipage}[t][0.2\textheight][c]{\linewidth}
\centering
\adjincludegraphics[max width=1.\linewidth, max height=0.2\textheight]{file58cauchy32sequence32illustration95cb933a3068372e1cd2c54505c7ab2143412ebaad}
\end{minipage}
\caption*{Eine Cauchy-Folge: Der Abstand der Folgenglieder untereinander wird beliebig klein (\arabic{imagelabel})}
\end{figure}

\end{minipage}
&
\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Cauchy sequence illustration2.svg}{\textbf{Cauchy sequence illustration2.svg}} by Krishnavedala, Reubot \textit{(Public domain)}}\begin{minipage}[t]{\linewidth}
\begin{figure}[H]
\begin{minipage}[t][0.2\textheight][c]{\linewidth}
\centering
\adjincludegraphics[max width=1.\linewidth, max height=0.2\textheight]{file58cauchy32sequence32illustration295c531d3ea70d96af16085ecf228a727d0b5803cdc}
\end{minipage}
\caption*{Keine Cauchy-Folge: Der Abstand der Folgenglieder untereinander wird nicht beliebig klein (\arabic{imagelabel})}
\end{figure}

\end{minipage}
\end{tabularx}

\begin{example*}[Eine Cauchy-Folge]
Die Folge $(a_{n})_{n\in \mathbb {N} }$ mit $a_{n}={\tfrac {1}{n}}$ ist eine Cauchy-Folge. Dazu müssen wir zeigen, dass es zu jedem $\epsilon >0$ ein $N\in \mathbb {N} $ gibt, so dass für alle $n,m\geq N$ gilt

\begin{align*}
|a_{n}-a_{m}|=\left|{\frac {1}{n}}-{\frac {1}{m}}\right|=\left|{\frac {n-m}{nm}}\right|={\frac {|n-m|}{nm}}<\epsilon 
\end{align*}

Nehmen wir nun $n>m$ an. Der Fall $n<m$ geht analog, mit vertauschten Rollen von $n$ und $m$. Nun gilt

\begin{align*}
|a_{n}-a_{m}|={\frac {|n-m|}{nm}}<{\frac {n}{nm}}={\frac {1}{m}}
\end{align*}

Wählen wir nun $N$ so groß, dass $N>{\tfrac {1}{\epsilon }}$ ist (geht immer nach dem archimedischen Axiom). Dann gilt ${\tfrac {1}{N}}<\epsilon $ und damit auch ${\tfrac {1}{m}}<\epsilon $ für alle $m\geq N$. Insgesamt folgt nun für alle $n,m\in \mathbb {N} $ mit $n>m\geq N$:

\begin{align*}
|a_{m}-a_{n}|={\frac {|n-m|}{mn}}<{\frac {1}{m}}\leq {\frac {1}{N}}<\epsilon 
\end{align*}

Also ist $(a_{n})$ eine Cauchy-Folge.

\end{example*}

\begin{example*}[Keine Cauchy-Folge]
Die Folge $(a_{n})_{n\in \mathbb {N} }$ mit $a_{n}=n$ ist hingegen keine Cauchy-Folge. Zu jedem $\epsilon >0$ und $N\in \mathbb {N} $ können wir $n,m\geq N$ so weit auseinander wählen, dass garantiert $|a_{n}-a_{m}|\geq \epsilon $ ist. Wählen wir beispielsweise $\epsilon ={\tfrac {1}{2}}$ und nehmen ein beliebiges $N\in \mathbb {N} $. Nun können wir $n=N+1$ und $m=N+2$ definieren. Es ist $n,m\geq N$ und

\begin{align*}
|a_{m}-a_{n}|=|m-n|=|N+2-(N+1)|=1\geq {\tfrac {1}{2}}=\epsilon 
\end{align*}

Damit erfüllt $(a_{n})_{n\in \mathbb {N} }$ nicht die Cauchy-Definition für $\epsilon ={\tfrac {1}{2}}$ und ist damit keine Cauchy-Folge.

\end{example*}

\section{Jede konvergente Folge ist eine Cauchy-Folge}

Wir haben die Definition der Cauchy-Folge als Alternative zur Konvergenzdefinition hergeleitet. In den folgenden Abschnitten werden wir beweisen, dass bei reellwertigen Folgen jede Cauchy-Folge konvergiert und umgekehrt. Beginnen wir mit dem Beweis, dass konvergente Folgen Cauchy-Folgen sind:

\begin{theorem*}
Jede konvergente Folge ist eine Cauchy-Folge.

\end{theorem*}

\begin{proof*}
Sei $(a_{n})_{n\in \mathbb {N} }$ eine beliebige konvergente Folge. Sei $\epsilon >0$. Es gibt dann ein $N\in \mathbb {N} $ mit

\begin{align*}
|a-a_{n}|&<{\tfrac {\epsilon }{2}}\\|a-a_{m}|&<{\tfrac {\epsilon }{2}}
\end{align*}

für alle $n,m\geq N$. Sei $n,m\geq N$ nun beliebig. Es ist

\begin{align*}
|a_{n}-a_{m}|&=|(a_{n}-a)+(a-a_{m})|\\[0.5em]&{\color {OliveGreen}\left\downarrow \ {\text{Dreiecksungleichung: }}|x+y|\leq |x|+|y|\right.}\\[0.5em]&\leq \underbrace {|a_{n}-a|} _{<\ {\tfrac {\epsilon }{2}}}+\underbrace {|a-a_{m}|} _{<\ {\tfrac {\epsilon }{2}}}\\&<\epsilon 
\end{align*}

\end{proof*}

\section{Jede Cauchy-Folge ist beschränkt}

Genau wie bei konvergenten Folgen können wir folgenden Satz beweisen:

\begin{theorem*}[Cauchy-Folgen sind beschränkt]
Jede Cauchy-Folge ist beschränkt.

\end{theorem*}

\begin{explanation*}[Cauchy-Folgen sind beschränkt]
Dieser Satz verwundert nicht. Wir haben Cauchy-Folgen eingeführt, um mit ihnen die Konvergenz einer Folge zeigen zu können. Jede Cauchy-Folge soll nämlich konvergieren (wir werden sehen, dass dies für reelle Folgen tatsächlich gilt) und konvergente Folgen sind bekanntlich beschränkt. Der Beweis zum obigen Satz ist ähnlich wie der entsprechende Beweis bei konvergenten Folgen:

\end{explanation*}

\begin{proof*}[Cauchy-Folgen sind beschränkt]
Sei $(a_{n})_{n\in \mathbb {N} }$ eine Cauchy-Folge. Wir wissen, dass es für alle $\epsilon >0$ ein $N\in \mathbb {N} $ gibt, so dass $|a_{n}-a_{m}|<\epsilon $ für alle $n,m\geq N$ ist. Setzen wir wie beim Beschränktheitsbeweis konvergenter Folgen $\epsilon =1$. Wir erhalten ein $N\in \mathbb {N} $ mit $|a_{n}-a_{m}|<1$ für alle $n,m\geq N$. Setzen wir $m=N$, dann ist

\begin{align*}
|a_{n}-a_{N}|<1
\end{align*}

für alle $n\geq N$. Damit liegen alle Folgenglieder $a_{n}$ für $n\geq N$ im Intervall $(a_{N}-1,a_{N}+1)$. Somit sind alle Folgenglieder ab dem Index $N$ nach oben durch $a_{N}+1$ und nach unten durch $a_{N}-1$ beschränkt:

\begin{align*}
(a_{n})_{n\in \mathbb {N} }=(a_{1},\,a_{2},\,\ldots ,\,a_{N-1},\,{\color {OliveGreen}\underbrace {a_{N},\,a_{N+1},\,a_{N+2},\,a_{N+3},\,\ldots } _{{\text{Diese Folgenglieder sind }}\leq a_{N}+1{\text{ und }}\geq a_{N}-1}})
\end{align*}

Vor $a_{N}$ liegen nur endlich viele Folgenglieder $a_{1},\,a_{2},\,\ldots ,\,a_{N-1}$. Diese müssen deswegen zwangsweise beschränkt sein. So sind sie nach oben durch $S=\max\{a_{1},\,a_{2},\,\ldots ,\,a_{N-1}\}$ und nach unten durch $s=\min\{a_{1},\,a_{2},\,\ldots ,\,a_{N-1}\}$ beschränkt. Wir haben:

\begin{align*}
(a_{n})_{n\in \mathbb {N} }=({\color {Blue}\underbrace {a_{1},\,a_{2},\,\ldots ,\,a_{N-1}} _{{\text{Diese Folgenglieder sind }}\leq S{\text{ und }}\geq s}},{\color {OliveGreen}\underbrace {a_{N},\,a_{N+1},\,a_{N+2},\,a_{N+3},\,\ldots } _{{\text{Diese Folgenglieder sind }}\leq a_{N}+1{\text{ und }}\geq a_{N}-1}})
\end{align*}

Insgesamt ist die Folge damit nach oben durch $\max\{S,a_{N}+1\}$ und nach unten durch $\min\{s,a_{N}-1\}$ beschränkt.

\end{proof*}

\section{Cauchy-Folgen mit konvergenten Teilfolgen konvergieren}

Es folgt nun ein (Hilfs-)Satz, den wir später benötigen werden, um die Konvergenz einer (reellwertigen) Cauchy-Folge zu beweisen:

\begin{theorem*}[Cauchy-Folgen mit konvergenten Teilfolgen konvergieren]
Jede Cauchy-Folge $(a_{n})_{n\in \mathbb {N} }$, die eine gegen $a$ konvergente Teilfolge $\left(a_{n_{k}}\right)_{k\in \mathbb {N} }$ besitzt, konvergiert gegen den Grenzwert $a$ der konvergenten Teilfolge.

\end{theorem*}

\begin{solutionprocess*}[Cauchy-Folgen mit konvergenten Teilfolgen konvergieren]
Sei $(a_{n})_{n\in \mathbb {N} }$ eine Cauchy-Folge und $\left(a_{n_{k}}\right)_{k\in \mathbb {N} }$ eine konvergente Teilfolge der Cauchy-Folge mit dem Grenzwert $a$. Für die Cauchy-Folge $(a_{n})_{n\in \mathbb {N} }$ wissen wir aus der Definition, dass die Folgenglieder beliebig nah beieinander liegen. Außerdem wissen wir für die konvergente Teilfolge, dass ihre Folgenglieder beliebig nah an $a$ liegen. Diese beiden Eigenschaften müssen wir nun zu einem Beweis kombinieren.

Sei also $\epsilon >0$. Wir müssen nun ein $N\in \mathbb {N} $ finden, so dass $|a_{n}-a|<\epsilon $ für alle $n\geq N$ ist. Beginnen wir mit der Ungleichung

\begin{align*}
|a_{n}-a|<\epsilon 
\end{align*}

Wenn das Folgenglied $a_{n}$ in der Teilfolge $\left(a_{n_{k}}\right)_{k\in \mathbb {N} }$ liegt, ist obige Abschätzung kein Problem, da wir hier wissen, dass diese Ungleichung für ausreichend große Indizes erfüllt ist. Herausfordernd ist der Fall, dass $a_{n}$ kein Glied der Teilfolge ist. Aus der Cauchy-Eigenschaft folgt aber, dass es in einer beliebigen Umgebung von $a_{n}$ Glieder der Teilfolge geben muss und diese liegen bekanntlich beliebig nahe an dem Grenzwert. Wir können also den Abstand $|a_{n}-a|$ über den Umweg des Teilfolgenglieds abschätzen. Dies können wir durch die Dreiecksungleichung erreichen:

\begin{align*}
|a_{n}-a|&=\left|a_{n}-a_{n_{k}}+a_{n_{k}}-a\right|\\[0.5em]&{\color {OliveGreen}\left\downarrow \ {\text{Dreiecksungleichung}}\right.}\\[0.5em]&\leq \left|a_{n}-a_{n_{k}}\right|+\left|a_{n_{k}}-a\right|
\end{align*}

Beide Beträge können wir beliebig klein machen. Wenn beide Beträge kleiner als ${\tfrac {\epsilon }{2}}$ sind, dann ist $|a_{n}-a|$ garantiert kleiner als $\epsilon $. Beginnen wir zunächst mit $\left|a_{n_{k}}-a\right|$. Hier finden wir ein $N_{1}$ mit $\left|a_{n_{k}}-a\right|<{\tfrac {\epsilon }{2}}$ für alle $k\geq N_{1}$. Die Zahl $N_{1}$ existiert, weil die Teilfolge $\left(a_{n_{k}}\right)_{k\in \mathbb {N} }$ gegen $a$ konvergiert.

Nun zum zweiten Betrag: Es gibt ein $N_{2}$ mit $|a_{n}-a_{m}|<{\tfrac {\epsilon }{2}}$ für alle $n,m\geq N_{2}$. Anstelle von $a_{m}$ haben wir $a_{n_{k}}$. Wir müssen also garantieren, dass $n_{k}\geq N_{2}$ ist. Generell ist $n_{k}\geq k$, da $(n_{k})_{k\in \mathbb {N} }$ eine steigende Folge natürlicher Zahlen ist. Damit können wir $k\geq N_{2}$ wählen, weil dann $n_{k}\geq k\geq N_{2}$ ist. Da aber $k$ auch größer als $N_{1}$ sein sollte, wählen wir ein $k\geq \max\{N_{2},N_{1}\}$. Beachte, dass es egal ist, welches $k\geq \max\{N_{2},N_{1}\}$ wir hier wählen.

Die Variable $n$ trat bisher nur im Term $|a_{n}-a_{m}|$ auf. Hier hatten wir gefordert, dass $n,m\geq N_{2}$ ist. Dementsprechend haben wir für $n$ die einzige Anforderung, dass es größer gleich $N_{2}$ sein muss. Also wählen wir im Konvergenzbeweis $N=N_{2}$.

\end{solutionprocess*}

\begin{proof*}[Cauchy-Folgen mit konvergenten Teilfolgen konvergieren]
Sei $(a_{n})_{n\in \mathbb {N} }$ eine Cauchy-Folge und $\left(a_{n_{k}}\right)_{k\in \mathbb {N} }$ eine konvergente Teilfolge der Cauchy-Folge mit dem Grenzwert $a$. Sei $\epsilon >0$ beliebig. Aus der Cauchy-Eigenschaft folgt, dass es ein $N\in \mathbb {N} $ mit $|a_{n}-a_{m}|<{\tfrac {\epsilon }{2}}$ für alle $n,m\geq N$ gibt. Außerdem gibt es ein ${\tilde {N}}\in \mathbb {N} $ mit $\left|a_{n_{k}}-a\right|<{\tfrac {\epsilon }{2}}$ für alle $k\geq {\tilde {N}}$. Sei ${\tilde {k}}$ eine beliebige natürliche Zahl mit ${\tilde {k}}\geq \max\{N,{\tilde {N}}\}$. Sei $n\geq N$ beliebig. Es ist

\begin{align*}
|a_{n}-a|&=\left|a_{n}-a_{n_{\tilde {k}}}+a_{n_{\tilde {k}}}-a\right|\\[0.5em]&{\color {OliveGreen}\left\downarrow \ {\text{Dreiecksungleichung}}\right.}\\[0.5em]&\leq \left|a_{n}-a_{n_{\tilde {k}}}\right|+\left|a_{n_{\tilde {k}}}-a\right|\\[0.5em]&{\color {OliveGreen}\left\downarrow \ \left|a_{n_{\tilde {k}}}-a\right|<{\tfrac {\epsilon }{2}}\right.}\\[0.5em]&<\left|a_{n}-a_{n_{\tilde {k}}}\right|+{\tfrac {\epsilon }{2}}\\[0.5em]&{\color {OliveGreen}\left\downarrow \ n_{k}\geq {\tilde {k}}\geq N\Rightarrow \left|a_{n}-a_{n_{\tilde {k}}}\right|<{\tfrac {\epsilon }{2}}\right.}\\[0.5em]&<{\tfrac {\epsilon }{2}}+{\tfrac {\epsilon }{2}}=\epsilon 
\end{align*}

\end{proof*}
\clearpage
\section{Jede Cauchy-Folge konvergiert}

Kommen wir nun zum eigentlichen Grund, warum es sich lohnt, Cauchy-Folgen zu studieren. Wir können nämlich zeigen, dass jede Cauchy-Folge konvergiert. Es gilt der Satz:

\begin{theorem*}[Cauchy-Folgen konvergieren]
Sei $(a_{n})_{n\in \mathbb {N} }$ eine Cauchy-Folge reeller Zahlen. Dann gibt es eine reelle Zahl $a$, gegen die $(a_{n})_{n\in \mathbb {N} }$ konvergiert.

\end{theorem*}

\begin{proof*}[Cauchy-Folgen konvergieren]
Sei $(a_{n})_{n\in \mathbb {N} }$ eine Cauchyfolge. Wir hatten in diesem Kapitel bereits bewiesen, dass diese Folge beschränkt ist. Nach dem Satz von Bolzano-Weierstraß muss damit $(a_{n})_{n\in \mathbb {N} }$ eine konvergente Teilfolge besitzen. Eine Cauchyfolge mit einer konvergenten Teilfolge konvergiert nach dem obigen Satz.

\end{proof*}

\part{Reihen}

\addxcontentsline{lof}{part}[\arabic{part}]{Reihen}\begin{authors}
Who2010, Stephan Kulla, Autorenkollektiv „Auswahlaxiom“ (Charlotte Dietze, Matthias Paulsen, Anne Reif), Akram Chawki, Apfelmus, Phoible, 0-Brane, KaiJay, Menuja J. (MJ Studies), EulerschesPi, Werner Fröhlich, Agnes Pauer, Meitnerium266, Boehm, Florianwicher, SerloBot, Bno90, Letsluk, Tegel, Peter Gröbner, Jenny Kilian, Mitja, BR\textasciitilde{}dewikibooks, Dirk Hünniger, PhilippHanemann\end{authors}

\chapter{Begriff der Reihe}

In den vorigen Kapiteln haben wir uns mit Folgen und deren Grenzwerten auseinandergesetzt. Dieses Konzept wollen wir nun nutzen, um unendliche Summen mathematisch exakt zu beschreiben. Dabei werden wir auf den Begriff der Reihe stoßen, den wir in den nächsten Kapiteln untersuchen wollen.

\section{Motivation der Reihe}

Was ist $1+{\tfrac {1}{2}}+{\tfrac {1}{4}}+{\tfrac {1}{8}}+\ldots $? Hier kann man so vorgehen: Wir starten beim Quadrat mit der Seitenlänge $1$. Dessen Flächeninhalt ist $1^{2}=1$. Nun halbieren wir abwechselnd die horizontale und die vertikale Seite. Man erhält so das Rechteck mit dem Flächeninhalt ${\tfrac {1}{2}}\cdot 1={\tfrac {1}{2}}$, danach das Quadrat mit der Fläche ${\tfrac {1}{2}}\cdot {\tfrac {1}{2}}={\tfrac {1}{4}}$, dann das Rechteck mit der Fläche ${\tfrac {1}{4}}\cdot {\tfrac {1}{2}}={\tfrac {1}{8}}$ und so weiter. Diese Rechtecke können wir geschickt anordnen:

\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Geometrische_reihe.svg}{\textbf{Geometrische\allowbreak\_reihe.svg}} by Toby001 \textit{(CC BY-SA 3.0)}}\begin{center}
\adjincludegraphics[max width=0.5\textwidth,max height=0.2\textheight]{file58geometrische95reihe95a7e0ace97fe2fa41c6dd25ed083453a27e8688f8}\end{center}

Wenn wir alle Flächen zusammenaddieren, erhalten wir ein Rechteck mit den Maßen $2\times 1$ und dem Flächeninhalt $2\cdot 1=2$. Der Wert der unendlichen Summe $1+{\tfrac {1}{2}}+{\tfrac {1}{4}}+{\tfrac {1}{8}}+\ldots $ sollte also gleich $2$ sein. Wir kommen zum selben Ergebnis, wenn wir die Teilsummen der unendlichen Summe bestimmen:

\begin{align*}
{\begin{array}{rll}S_{1}&=1&=1\\[0.5em]S_{2}&=1+{\tfrac {1}{2}}&=1{,}5\\[0.5em]S_{3}&=1+{\tfrac {1}{2}}+{\tfrac {1}{4}}&=1{,}75\\[0.5em]S_{4}&=1+{\tfrac {1}{2}}+{\tfrac {1}{4}}+{\tfrac {1}{8}}&=1{,}875\\[0.5em]S_{5}&=1+{\tfrac {1}{2}}+{\tfrac {1}{4}}+{\tfrac {1}{8}}+{\tfrac {1}{16}}&=1{,}9375\\[0.5em]&\,\vdots \end{array}}
\end{align*}

Die Werte der Teilsummen scheinen gegen $2$ zu streben. Das unterstützt die These, dass $1+{\tfrac {1}{2}}+{\tfrac {1}{4}}+{\tfrac {1}{8}}+\ldots =2$ ist.

Wir haben gerade einer unendlichen Summe einen Wert zugeordnet. Doch jetzt stellt sich die Frage, wie wir das intuitive Konzept einer unendlichen Summe exakt definieren können. An dieser Stelle eröffnen sich einige Fragen:

\begin{itemize}
\item Wie können wir generell den Wert einer unendlichen Summe bestimmen?
\item Gibt es unendliche Summen, denen wir keinen Wert zuweisen können?
\item Wie unterscheidet man unendliche Summen, denen ein Wert beziehungsweise denen kein Wert zugewiesen werden kann?
\end{itemize}

In diesem Kapitel stellen wir mit dem Konzept der Reihe die formale Definition einer unendlichen Summe vor. Wir werden Reihen mit Hilfe von Partialsummen (= „Teilsummen“) definieren. Die Partialsummen bauen auf dem Begriff der endlichen Summe auf. In späteren Kapiteln beantworten wir die Frage, welchen unendlichen Summen wir einen Wert zuweisen können und welchen nicht.

\section{Partialsummen}

Nachdem wir wissen wie endliche Summen definiert sind, können wir uns der formalen Definition einer unendlichen Summe widmen. Hierzu starten wir mit der Form, die uns intuitiv plausibel erscheint:

\begin{align*}
a_{1}+a_{2}+a_{3}+a_{4}+a_{5}+\ldots 
\end{align*}

Wir betrachten zunächst die Folge der Teilsummen:

\begin{align*}
S_{1}&=a_{1}\\S_{2}&=a_{1}+a_{2}\\S_{3}&=a_{1}+a_{2}+a_{3}\\S_{4}&=a_{1}+a_{2}+a_{3}+a_{4}\\S_{5}&=a_{1}+a_{2}+a_{3}+a_{4}+a_{5}\\&\ \vdots \\S_{n}&=a_{1}+a_{2}+a_{3}+a_{4}+a_{5}+\ldots a_{n}\\&\ \vdots 
\end{align*}

Diese Folge werden wir später benutzen, um unendliche Summen zu definieren. $S_{n}$ ist die Summe der ersten $n$ Summanden und stellt eine endliche Summe dar:

\begin{align*}
S_{n}=a_{1}+a_{2}+a_{3}+a_{4}+a_{5}+\ldots +a_{n}=\sum _{k=1}^{n}a_{k}
\end{align*}

Diese Teilsummen werden in der Mathematik \emph{Partialsummen} (aus dem Lateinischen, von \href{https://de.wiktionary.org/wiki/pars}
{„pars“} = Teil) genannt. Sie sind ein endlicher Teil der unendlichen Summe. Die formale Definition lautet:

\begin{definition*}[Partialsumme]
Sei $(a_{m})_{m\in \mathbb {N} }$ eine Folge in $\mathbb {R} $. Dann heißt die folgende Summe die $n$-te Partialsumme:

\begin{align*}
S_{n}=a_{1}+a_{2}+a_{3}+a_{4}+a_{5}+\ldots +a_{n}=\sum _{k=1}^{n}a_{k}
\end{align*}

\end{definition*}

\section{Reihe}

Der Wert einer unendlichen Summe sollte dem Grenzwert ihrer Partialsummen entsprechen:

\begin{align*}
a_{1}+a_{2}+a_{3}+a_{4}+\ldots &=\lim _{n\to \infty }(a_{1}+a_{2}+a_{3}+a_{4}+\ldots +a_{n})\\&=\lim _{n\to \infty }\sum _{k=1}^{n}a_{k}\\&=\lim _{n\to \infty }S_{n}
\end{align*}

Wir können zuerst die Folge aller Partialsummen bilden und dann ihren Grenzwert betrachten. Wir definieren zunächst die Folge der Partialsummen als \emph{Reihe}. Für eine Reihe schreiben wir hier $\sum _{k=1}^{\infty }a_{k}$. Diese Schreibweise ist ähnlich zur $n$-ten Partialsumme $\sum _{k=1}^{n}a_{k}$. Der einzige Unterschied ist, dass wir als Endwert des Laufindex nicht $n$, sondern das Unendlichkeitssymbol $\infty $ verwenden. Wir definieren also:

\begin{definition*}[Reihe]
Für eine reelle Folge $(a_{k})_{k\in \mathbb {N} }$ ist die Reihe $\sum _{k=1}^{\infty }a_{k}$ die Folge aller Partialsummen $\left(\sum _{k=1}^{n}a_{k}\right)_{n\in \mathbb {N} }$:

\begin{align*}
\sum _{k=1}^{\infty }a_{k}&=\left(\sum _{k=1}^{n}a_{k}\right)_{n\in \mathbb {N} }\\[1em]&=\left(\sum _{k=1}^{1}a_{k},\ {\color {OliveGreen}\sum _{k=1}^{2}a_{k}},\ {\color {Blue}\sum _{k=1}^{3}a_{k}},\ {\color {Orange}\sum _{k=1}^{4}a_{k}},\ \ldots \right)\\[1em]&=\left(a_{1},\ {\color {OliveGreen}a_{1}+a_{2}},\ {\color {Blue}a_{1}+a_{2}+a_{3}},\ {\color {Orange}a_{1}+a_{2}+a_{3}+a_{4}},\ \ldots \right)
\end{align*}

\end{definition*}

Als Nächstes setzen wir den Grenzwert der unendlichen Summe mit dem Grenzwert der Partialsummenfolge gleich. Die Partialsummenfolge ist eine gewöhnliche Folge. Entweder sie besitzt einen Grenzwert oder sie divergiert. Divergiert die Partialsummenfolge, divergiert auch die unendliche Summe beziehungsweise die Reihe. Konvergiert die Partialsummenfolge, setzt man den Wert der unendlichen Summe mit dem Grenzwert der Partialsummenfolge gleich. Eine unendliche Summe ist also dasselbe wie der Grenzwert der dazugehörigen Folge von Partialsummen. Auch für diesen Grenzwert der Partialsummenfolge benutzen wir die Schreibweise $\sum _{k=1}^{\infty }a_{k}$:

\begin{definition*}[Grenzwert einer Reihe]
Der Grenzwert $\sum _{k=1}^{\infty }a_{k}$ einer Reihe ist der Limes der Partialsummenfolge:

\begin{align*}
\sum _{k=1}^{\infty }a_{k}=\lim _{n\to \infty }\sum _{k=1}^{n}a_{k}
\end{align*}

\end{definition*}

\section{Ist eine Reihe eine Zahl oder eine Folge?}

Wie wir bereits bemerkt haben, wird der Ausdruck $\sum _{k=1}^{\infty }a_{k}$ sowohl für die Folge der Partialsummen (= Reihe) als auch für den Grenzwert der Partialsummenfolge (= Wert der Reihe) verwendet. Das widerspricht grundlegenden Prinzipien der Mathematik, wo Schreibweisen eindeutig sein müssen. Der Ausdruck $\sum _{k=1}^{\infty }a_{k}$ sollte nicht gleichzeitig eine Folge und einen Grenzwert, also eine reelle Zahl, bezeichnen. So schreibt Otto Forster in seinem Buch zur „Analysis 1“:

\begin{displayquote}
„Das Symbol $\sum _{n=0}^{\infty }a_{n}$ bedeutet also zweierlei: \begin{enumerate}
\item Die Folge $\left(\sum _{n=0}^{m}a_{n}\right)_{m\in \mathbb {N} }$ der Partialsummen.
\item Im Falle der Konvergenz den Grenzwert $\lim _{m\to \infty }\sum _{n=0}^{m}a_{n}$.“
\end{enumerate}

 – Otto Forster in „Analysis 1“\end{displayquote}

Beim Ausdruck $\sum _{k=1}^{\infty }a_{k}$ müssen wir also darauf achten, ob damit die Partialsummenfolge oder ihr Grenzwert gemeint ist. In den meisten Fällen können wir das allerdings schnell aus dem Kontext schließen.
\clearpage
\section{Zusammenfassung}

Wir haben die Idee einer unendlichen Summe formal so definiert:

\begin{enumerate}
\item Wir haben die Summe der ersten $n$ Summanden als $n$-te Partialsumme definiert.
\item Wir haben die Folge der Partialsummen Reihe genannt. Der Grenzwert dieser Reihe entspricht dem Wert der unendlichen Summe.
\end{enumerate}

\section{Folge der Restglieder}

Wir haben gesehen, dass eine Reihe $\sum _{k=1}^{\infty }a_{k}$ dasselbe wie eine Partialsummenfolge $\left(\sum _{k=1}^{n}a_{k}\right)_{n\in \mathbb {N} }$ ist. Gehen wir nun davon aus, dass die Reihe $\sum _{k=1}^{\infty }a_{k}$ konvergiert. Der Grenzwert von $\lim _{n\to \infty }\sum _{k=1}^{n}a_{k}$ existiert also und entspricht dem Grenzwert $\sum _{k=1}^{\infty }a_{k}$. Damit ist $\lim _{n\to \infty }\left(\sum _{k=1}^{\infty }a_{k}-\sum _{k=1}^{n}a_{k}\right)=0$.

Betrachten wir nun den Unterschied zwischen den Partialsummen und dem Grenzwert der Reihe. Die Differenz zwischen der $n$-ten Partialsumme und dem Reihengrenzwert wird \emph{$n$-tes Restglied} $R_{n}$ genannt. Sie entspricht dem Fehler zwischen der $n$-ten Partialsumme und dem Reihengrenzwert.

Die formale Defintion des $n$-ten Restglieds lautet:

\begin{definition*}[$n$-tes Restglied einer Reihe]
Sei $\sum _{k=1}^{\infty }a_{k}$ eine Reihe. Dann nennen wir die folgende Reihe das $n$-te Restglied der Reihe:

\begin{align*}
R_{n}=\sum _{k=n+1}^{\infty }a_{k}=a_{n+1}+a_{n+2}+a_{n+3}+\ldots 
\end{align*}

\end{definition*}

Die Restglieder sehen so aus:

\begin{align*}
{\begin{array}{rclrcccccccc}S_{1}&=&a_{1}&R_{1}&=&a_{2}&+&a_{3}&+&a_{4}&+&\ldots \\S_{2}&=&a_{1}+a_{2}&R_{2}&=&&&a_{3}&+&a_{4}&+&\ldots \\S_{3}&=&a_{1}+a_{2}+a_{3}&R_{3}&=&&&&&a_{4}&+&\ldots \\&\vdots &&&\vdots &\\S_{n}&=&a_{1}+a_{2}+\ldots +a_{n}&R_{n}&=&a_{n+1}&+&a_{n+2}&+&a_{n+3}&+&\ldots \\&\vdots &&&\vdots &\end{array}}
\end{align*}

Nun betrachten wir die Folge der Restglieder $(R_{n})_{n\in \mathbb {N} }$. Wie verhält sich diese Folge? Wir haben oben schon erwähnt, dass es bei konvergenten Reihen Sinn ergibt, wenn $\lim _{n\to \infty }R_{n}=\lim _{n\to \infty }\left(\sum _{k=1}^{\infty }a_{k}-\sum _{k=1}^{n}a_{k}\right)=0$. Das werden wir im folgenden Satz beweisen:

\begin{theorem*}[Folge der Restglieder]
Sei $\sum _{k=1}^{\infty }a_{k}=\left(\sum _{k=1}^{n}a_{k}\right)_{n\in \mathbb {N} }=(S_{n})_{n\in \mathbb {N} }$ eine konvergente Reihe. Dann konvergiert die Folge der Restglieder $(R_{n})_{n\in \mathbb {N} }=\left(\sum _{k=n+1}^{\infty }a_{k}\right)_{n\in \mathbb {N} }$ gegen $0$.

\end{theorem*}

\begin{proof*}[Folge der Restglieder]
Da die Reihe konvergiert, existiert der Grenzwert $\lim _{n\to \infty }S_{n}=\lim _{n\to \infty }\sum _{k=1}^{n}a_{k}=\sum _{k=1}^{\infty }a_{k}=S<\infty $. Nun gilt

\begin{align*}
R_{n}&=\sum _{k=n+1}^{\infty }a_{k}\\&=\sum _{k=1}^{\infty }a_{k}-\sum _{k=1}^{n}a_{k}\\&=S-S_{n}
\end{align*}

Mit den Rechenregeln für Grenzwerte folgt daher

\begin{align*}
\lim _{n\to \infty }R_{n}&=\lim _{n\to \infty }(S-S_{n})\\&=S-\lim _{n\to \infty }S_{n}\\&=S-S\\&=0
\end{align*}

Also ist $(R_{n})_{n\in \mathbb {N} }$ eine Nullfolge.

\end{proof*}

\begin{hint*}
In der Praxis ist es normalerweise nicht möglich, eine explizite Darstellung für die Restgliederfolge $(R_{n})_{n\in \mathbb {N} }$ anzugeben. Jedoch können oft Abschätzungen gefunden werden. So werden wir bei alternierenden Reihen $\sum _{k=1}^{\infty }(-1)^{k+1}b_{k}$ mit Hilfe des \href{https://de.wikibooks.org/wiki/Mathe\_für\_Nicht-Freaks:\_Leibniz-Kriterium}
{Leibniz-Kriteriums} eine Fehlerabschätzung der Restglieder für solche Reihen herleiten. Ebenso können bei Taylorreihen Fehlerabschätzungen gefunden werden.

\end{hint*}

\chapter{Rechenregeln für Reihen}

Im letzten Kapitel haben wir Reihen, also unendliche Summen, definiert. Wie gehen wir aber jetzt mit diesen unendlichen Summen um? Bei endlichen Summen dürfen wir beliebig Klammern setzen und umordnen. Genauso dürfen wir zwei endliche Summen addieren und Skalare multiplizieren. Können wir das bei Reihen auch machen? Nein. Denn wir werden sehen, dass wir nur bei konvergenten Reihen addieren und multiplizieren dürfen. Außerdem ist Klammern setzen und umordnen der Summanden bei unendlichen Summen problematisch.

\section{Summenregel}

\subsection{Beweis}

\begin{theorem*}[Summenregel für Reihen]
Seien $\sum _{k=1}^{\infty }a_{k}$ und $\sum _{k=1}^{\infty }b_{k}$ zwei konvergente Reihen. Dann gilt

\begin{align*}
\sum _{k=1}^{\infty }(a_{k}+b_{k})=\sum _{k=1}^{\infty }a_{k}+\sum _{k=1}^{\infty }b_{k}
\end{align*}

\end{theorem*}

\begin{proof*}[Summenregel für Reihen]
Zunächst definieren wir die Partialsummen der einzelnen Reihen:

\begin{align*}
P_{n}=\sum _{k=1}^{n}a_{k}{\text{ und }}S_{n}=\sum _{k=1}^{n}b_{k}
\end{align*}

Wegen der Konvergenz der Reihen konvergieren auch die Folgen $(S_{n})_{n\in \mathbb {N} }$ und $(P_{n})_{n\in \mathbb {N} }$.

Nun wenden wir die Definition $\sum _{k=1}^{\infty }(a_{k}+b_{k})=\lim _{n\to \infty }\sum _{k=1}^{n}(a_{k}+b_{k})$ der Reihenkonvergenz an, danach die Linearität der Summe und schließlich können wir den Grenzwertsatz für die Summe konvergenter Folgen benutzen:

\begin{align*}
\sum _{k=1}^{\infty }(a_{k}+b_{k})&=\lim _{n\to \infty }\sum _{k=1}^{n}(a_{k}+b_{k})\\[0.5em]&=\lim _{n\to \infty }\left(\sum _{k=1}^{n}a_{k}+\sum _{k=1}^{n}b_{k}\right)\\[0.5em]&\ {\color {OliveGreen}\left\downarrow \ \lim _{n\to \infty }(P_{n}+S_{n})=\lim _{n\to \infty }P_{n}+\lim _{n\to \infty }S_{n}{\text{, weil die Folgen konvergieren.}}\right.}\\[0.5em]&=\lim _{n\to \infty }\sum _{k=1}^{n}a_{k}+\lim _{n\to \infty }\sum _{k=1}^{n}b_{k}\\[0.5em]&=\sum _{k=1}^{\infty }a_{k}+\sum _{k=1}^{\infty }b_{k}
\end{align*}

\end{proof*}

\section{Faktorregel}

\subsection{Beweis}

\begin{theorem*}[Faktorregel für Reihen]
Sei $\sum _{k=1}^{\infty }a_{k}$ eine konvergente Reihe und sei $\lambda \in \mathbb {R} $ eine beliebige reelle Zahl. Es ist dann:

\begin{align*}
\sum _{k=1}^{\infty }\lambda a_{k}=\lambda \cdot \sum _{k=1}^{\infty }a_{k}
\end{align*}

\end{theorem*}

\begin{proof*}[Faktorregel für Reihen]
Wir definieren zuerst: $P_{n}=\sum _{k=1}^{n}a_{k}$ für $n\in \mathbb {N} $. Da die Reihe konvergiert, gilt dies auch für die Folge der Partialsummen $(P_{n})_{n\in \mathbb {N} }$. Analog zum Beweis der Summenregel benutzen wir zunächst die Definition des Grenzwerts einer Reihe, danach die Linearität der Summe und schließlich den Grenzwertsatz für das Vielfache einer konvergenten Folge.

\begin{align*}
\sum _{k=1}^{\infty }\lambda a_{k}&=\lim _{n\to \infty }\sum _{k=1}^{n}\lambda a_{k}\\[0.5em]&=\lim _{n\to \infty }\left(\lambda \cdot \sum _{k=1}^{n}a_{k}\right)\\[0.5em]&\ {\color {OliveGreen}\left\downarrow \ \lim _{n\to \infty }\lambda \cdot P_{n}=\lambda \cdot \lim _{n\to \infty }P_{n}{\text{, weil die Folge konvergiert.}}\right.}\\[0.5em]&=\lambda \cdot \lim _{n\to \infty }\sum _{k=1}^{n}a_{k}\\[0.5em]&=\lambda \cdot \sum _{k=1}^{\infty }a_{k}
\end{align*}

\end{proof*}

\section{Aufteilungsregel}

\subsection{Beweis}

\begin{theorem*}[Aufteilungsregel für Reihen]
Sei $(a_{k})_{k\in \mathbb {N} }$ eine Folge. Wenn $\sum _{k=1}^{\infty }a_{2k}$ und $\sum _{k=1}^{\infty }a_{2k-1}$ konvergieren, dann ist auch $\sum _{k=1}^{\infty }a_{k}$ konvergent, und es gilt:

\begin{align*}
\sum _{k=1}^{\infty }a_{k}=\sum _{k=1}^{\infty }a_{2k}+\sum _{k=1}^{\infty }a_{2k-1}
\end{align*}

\end{theorem*}

\begin{proof*}[Aufteilungsregel für Reihen]
Diese Regel ist eine Folgerung aus der obigen \emph{Summenregel}. Zunächst schauen wir uns die beiden Reihen $\sum _{k=1}^{\infty }a_{2k}$ und $\sum _{k=1}^{\infty }a_{2k-1}$ an. Zu ihnen gehören die Partialsummenfolgen:

\begin{align*}
\left(\sum _{k=1}^{n}a_{2k-1}\right)_{n\in \mathbb {N} }&=(a_{1},a_{1}+a_{3},a_{1}+a_{3}+a_{5},\ldots )\\[0.5em]\left(\sum _{k=1}^{n}a_{2k}\right)_{n\in \mathbb {N} }&=(a_{2},a_{2}+a_{4},a_{2}+a_{4}+a_{6},\ldots )
\end{align*}

Zunächst bilden wir zwei neue Folgen $(b_{k})_{k\in \mathbb {N} }$ und $(c_{k})_{k\in \mathbb {N} }$, indem wir $(a_{2k-1})_{k\in \mathbb {N} }$ und $(a_{2k})_{k\in \mathbb {N} }$ geschickt mit Nullen auffüllen:

\begin{align*}
(b_{k})_{k\in \mathbb {N} }&=(a_{1},0,a_{3},0,a_{5},\ldots )\\[0.5em](c_{k})_{k\in \mathbb {N} }&=(0,a_{2},0,a_{4},0,\ldots )
\end{align*}

Zu diesen Folgen zugehörigen Partialsummenfolgen lauten damit:

\begin{align*}
\left(\sum _{k=1}^{n}b_{k}\right)_{n\in \mathbb {N} }&=(a_{1},a_{1}+0,a_{1}+0+a_{3},a_{1}+0+a_{3}+0,\ldots )\\[0.5em]\left(\sum _{k=1}^{n}c_{k}\right)_{n\in \mathbb {N} }&=(0,0+a_{2},0+a_{2}+0,0+a_{2}+0+a_{4},\ldots )
\end{align*}

Da $\sum _{k=1}^{\infty }a_{2k}$ und $\sum _{k=1}^{\infty }a_{2k-1}$ konvergieren, konvergieren auch $\sum _{k=1}^{\infty }b_{k}$ und $\sum _{k=1}^{\infty }c_{k}$, wobei für die Grenzwerte dieser Reihen gilt:

\begin{align*}
\sum _{k=1}^{\infty }b_{k}&=\sum _{k=1}^{\infty }a_{2k-1}\\[0.5em]\sum _{k=1}^{\infty }c_{k}&=\sum _{k=1}^{\infty }a_{2k}
\end{align*}

Aus der Summenregel folgt, dass $\sum _{k=1}^{\infty }(b_{k}+c_{k})$ konvergieren muss. Nun ist $a_{k}=b_{k}+c_{k}$ für alle $k\in \mathbb {N} $. Damit muss auch $\sum _{k=1}^{\infty }a_{k}$ konvergieren, wobei

\begin{align*}
\sum _{k=1}^{\infty }a_{k}=\sum _{k=1}^{\infty }(b_{k}+c_{k})=\sum _{k=1}^{\infty }b_{k}+\sum _{k=1}^{\infty }c_{k}=\sum _{k=1}^{\infty }a_{2k}+\sum _{k=1}^{\infty }a_{2k-1}
\end{align*}

\end{proof*}

\subsection{Beispielaufgabe Aufteilungsregel}

\begin{exercise*}[Aufteilungsregel für Reihen]
Sei $a_{n}={\begin{cases}({\frac {1}{3}})^{n}&{\text{ für }}n=2k,\\0&{\text{ für }}n=2k-1.\end{cases}}$ Berechne den Wert der Reihe $\sum _{n=0}^{\infty }a_{n}$.

\end{exercise*}

\begin{solution*}[Aufteilungsregel für Reihen]
Es gilt

\begin{align*}
\sum _{n=0}^{\infty }a_{n}&=\sum _{k=0}^{\infty }\left[a_{2k}+a_{2k+1}\right]\\[0.5em]&\ {\color {OliveGreen}\left\downarrow \ \sum _{k=0}^{\infty }a_{k}=\sum _{k=0}^{\infty }a_{2k}+\sum _{k=0}^{\infty }a_{2k-1}\right.}\\[0.5em]&=\sum _{k=0}^{\infty }\left({\frac {1}{3}}\right)^{2k}+\sum _{k=0}^{\infty }0\\[0.5em]&=\sum _{k=0}^{\infty }\left({\frac {1}{9}}\right)^{k}+0\\[0.5em]&\quad {\color {OliveGreen}\left\downarrow \ \sum _{k=0}^{\infty }\left({\frac {1}{9}}\right)^{k}={\frac {1}{1-{\frac {1}{9}}}}={\frac {9}{8}}\right.}\\[0.5em]&={\frac {9}{8}}
\end{align*}

Weil die Reihe konvergiert, durften wir die Rechenregeln anwenden.

\end{solution*}

\section{Neue Klammern in Reihen setzen}

Nehme die konvergente Reihe $\sum _{k=0}^{\infty }2^{-k}$. Diese Reihe stellte die unendliche Summe $1+{\tfrac {1}{2}}+{\tfrac {1}{4}}+{\tfrac {1}{8}}+\ldots $ dar, und zu ihr gehört die Partialsummenfolge

\begin{align*}
\left(\sum _{k=0}^{n}2^{-k}\right)_{n\in \mathbb {N} }=\left(1,\ 1+{\frac {1}{2}},\ 1+{\frac {1}{2}}+{\frac {1}{4}},\ \ldots \right)
\end{align*}

Was passiert, wenn wir neue Klammern in der unendlichen Folge einführen? Beispielsweise können wir die unendliche Summe auch über den Ausdruck $\left(1+{\tfrac {1}{2}}\right)+\left({\tfrac {1}{4}}+{\tfrac {1}{8}}\right)+\ldots $ notieren, bei der jeweils zwei benachbarte Summanden durch eine Klammer zusammengefasst sind. In der Reihenschreibweise ergibt sich so die Reihe $\sum _{k=0}^{\infty }\left(2^{-2k}+2^{-(2k+1)}\right)$. Zunächst werden jeweils die beiden Summanden $2^{-2k}$ und $2^{-(2k+1)}$ zusammengefasst, danach wird über alle so ausgerechneten Summen $2^{-2k}+2^{-(2k+1)}$ die Reihe gebildet. Damit erhalten wir folgende Partialsummenformel

\begin{align*}
\left(\sum _{k=0}^{n}\left(2^{-2k}+2^{-(2k+1)}\right)\right)_{n\in \mathbb {N} }=\left(1+{\frac {1}{2}},\ 1+{\frac {1}{2}}+{\frac {1}{4}}+{\frac {1}{8}},\ \ldots \right)
\end{align*}

Diese Partialsummenfolge ist eine Teilfolge der ursprünglichen Partialsummenfolge. Nun konvergiert die Reihe $\sum _{k=0}^{\infty }2^{-k}$ und damit auch die dazugehörige Partialsummenfolge. Da bei konvergenten Folgen auch jede Teilfolge gegen denselben Grenzwert konvergiert, muss auch die neu geklammerte Partialsummenfolge $\sum _{k=0}^{\infty }\left(2^{-2k}+2^{-(2k+1)}\right)$ gegen denselben Grenzwert wie die ursprüngliche Partialsummenfolge konvergieren.

Dies können wir auf beliebige Reihen verallgemeinern: Wenn wir in einer Reihe benachbarte Summanden durch neue Klammern zunächst zusammenfassen, bevor die Reihe gebildet wird, dann entsteht eine Teilfolge der ursprünglichen Partialsummenfolge. Wenn die ursprüngliche Reihe und damit ihre Partialsummenfolge konvergiert, dann muss auch die neu geklammerte Reihe gegen denselben Grenzwert konvergieren. Dies können wir im folgenden Satz zusammenfassen:

\begin{theorem*}[Klammersetzen in Reihen]
Konvergiert eine Reihe, so konvergiert auch jede Reihe gegen denselben Grenzwert, die durch neue Klammern aus der ursprünglichen Reihe entstanden ist. Konkret: Sei $\sum _{k=1}^{\infty }a_{k}$ eine konvergente Reihe. Sei außerdem $(j_{l})_{l\in \mathbb {N} }$ eine streng monoton steigende Folge natürlicher Zahlen mit $j_{1}=1$, wobei $j_{l}$ jeweils der erste Summand einer Teilsumme ist, die durch die Klammerung zusammengefasst wird. Es ist dann

\begin{align*}
\sum _{k=1}^{\infty }a_{k}=\sum _{l=1}^{\infty }\left(\sum _{k=j_{l}}^{j_{l+1}-1}a_{k}\right)
\end{align*}

\end{theorem*}

\begin{proof*}[Klammersetzen in Reihen]
Sei $\sum _{k=1}^{\infty }a_{k}$ eine konvergente Reihe. Durch Einführung neuer Klammern entsteht die Reihe $\sum _{l=1}^{\infty }\left(\sum _{k=j_{l}}^{j_{l+1}-1}a_{k}\right)$, wobei $(j_{l})_{l\in \mathbb {N} }$ eine streng monoton steigende Folge natürlicher Zahlen mit $j_{1}=1$. Die Zahl $j_{l}$ ist dabei jeweils der Index des ersten Summanden, welcher in einer Teilsumme durch eine Klammersetzung zusammengefasst wird. Die dazugehörige Partialsummenfolge lautet

\begin{align*}
\left(\sum _{l=1}^{n}\left(\sum _{k=j_{l}}^{j_{l+1}-1}a_{k}\right)\right)_{n\in \mathbb {N} }&=\left(\sum _{k=1}^{j_{2}-1}a_{k},\ \sum _{k=1}^{j_{2}-1}a_{k}+\sum _{k=j_{2}}^{j_{3}-1}a_{k},\ \ldots \right)\\[0.5em]&=\left(\sum _{k=1}^{j_{2}-1}a_{k},\ \sum _{k=1}^{j_{3}-1}a_{k},\ \sum _{k=1}^{j_{4}-1}a_{k},\ \ldots \right)\\[0.5em]&=\left(\sum _{k=1}^{j_{n+1}-1}a_{k}\right)_{n\in \mathbb {N} }
\end{align*}

Dies ist eine Teilfolge der ursprünglichen Partialsummenfolge. Weil diese ursprüngliche Partialsummenfolge konvergiert, konvergiert auch neue Partialsummenfolge und damit die dazugehörige Reihe. Auch der Grenzwert ist derselbe. Dies folgt daraus, dass jede Teilfolge einer konvergenten Folge gegen denselben Grenzwert konvergiert.

\end{proof*}

\section{Warum es kein allgemeines Assoziativ- und Kommutativgesetz für Reihen gibt}

Bei endlichen Summen ist es dank des \href{https://de.wikibooks.org/wiki/Mathe\_für\_Nicht-Freaks:\_Körperaxiome}
{Assoziativgesetzes der Addition} erlaubt, beliebig Klammern zu setzen. Beispielsweise ist

\begin{align*}
1-1+1-1+1-1&=(((((1-1)+1)-1)+1)-1)\\[0.3em]&=((((0+1)-1)+1)-1)\\[0.3em]&=\ldots =0
\end{align*}

und ebenso

\begin{align*}
(1-1)+(1-1)+(1-1)=0+0+0=0
\end{align*}

Bei unendlichen Reihen müssen wir hingegen aufpassen. Betrachten wir in Analogie die Reihe

\begin{align*}
\sum _{k=1}^{\infty }(-1)^{k+1}=1-1+1-1\pm \ldots 
\end{align*}

Diese ist nach dem \href{https://de.wikibooks.org/wiki/Mathe\_für\_Nicht-Freaks:\_Trivialkriterium,\_Nullfolgenkriterium,\_Divergenzkriterium}
{Trivialkriterium} divergent, da $(-1)^{k+1}$ keine Nullfolge ist. Dieses Kriterum werden wir später kennenlernen. Durch Setzen von Klammern erhalten wir jedoch eine gegen null konvergente Reihe:

\begin{align*}
\sum _{k=1}^{\infty }\left((-1)^{2k}+(-1)^{2k+1}\right)&=(1-1)+(1-1)+(1-1)+(1-1)+\ldots \\[0.3em]&=0+0+0+0+\ldots =0
\end{align*}

In einer \emph{divergenten} Reihe dürfen daher Klammern \emph{nicht} beliebig gesetzt oder weggelassen werden.

Frage: Gibt es auch Klammerungen der Reihe $\sum _{k=1}^{\infty }(-1)^{k+1}$, die gegen $1$ beziehungsweise $-1$ konvergieren? Antwort: Gegen $1$ konvergiert die Klammerung

\begin{align*}
1+\sum _{k=1}^{\infty }\left((-1)^{2k+1}+(-1)^{2k+2}\right)&=1+(-1+1)+(-1+1)+(-1+1)+\ldots \\[0.3em]&=1+0+0+0+\ldots =1
\end{align*}

Eine Klammerung, die gegen $-1$ konvergiert, ist hingegen nicht möglich.

\chapter{Teleskopsumme und Teleskopreihe}

Teleskopreihen sind spezielle Reihen, bei denen sich die Summanden zum Teil gegenseitig aufheben. Dadurch ist es bei Teleskopreihen einfacher als bei anderen Reihen, ihr Konvergenzverhalten und ihren Grenzwert zu bestimmen.

\section{Teleskopsumme}

\subsection{Beispiel}

Wir betrachten die Summe $\sum _{k=1}^{n}{\frac {1}{k(k+1)}}$ für $n\in \mathbb {N} $. Wie können wir den Wert dieser Summe ausrechnen, ohne jeden Summanden einzeln zu berechnen? Für alle $k$ können wir schreiben

\begin{align*}
{\frac {1}{k(k+1)}}={\frac {(k+1)-k}{k(k+1)}}={\frac {1}{k}}-{\frac {1}{k+1}}
\end{align*}

Also gilt

\begin{align*}
&\sum _{k=1}^{n}{\frac {1}{k(k+1)}}=\sum _{k=1}^{n}\left({\frac {1}{k}}-{\frac {1}{k+1}}\right)\\[0.3em]=\ &\left({\frac {1}{1}}{\color {Red}-{\frac {1}{1+1}}}\right)+\left({\color {Red}{\frac {1}{2}}}{\color {Orange}-{\frac {1}{2+1}}}\right)+\left({\color {Orange}{\frac {1}{3}}}{\color {Olive}-{\frac {1}{3+1}}}\right)\\[0.3em]&+\ldots +\left({\color {Blue}{\frac {1}{n-1}}}{\color {Purple}-{\frac {1}{(n-1)+1}}}\right)+\left({\color {Purple}{\frac {1}{n}}}-{\frac {1}{n+1}}\right)\\[0.3em]=\ &1+{\color {Red}\left(-{\frac {1}{2}}+{\frac {1}{2}}\right)}+{\color {Orange}\left(-{\frac {1}{3}}+{\frac {1}{3}}\right)}+{\color {Olive}\left(-{\frac {1}{4}}+{\frac {1}{4}}\right)}\\[0.3em]&+\ldots +{\color {Purple}\left(-{\frac {1}{n}}+{\frac {1}{n}}\right)}-{\frac {1}{n+1}}\\[0.3em]=\ &1-{\frac {1}{n+1}}
\end{align*}

Diese Formel ist wesentlich einfacher als die ursprüngliche Summe. Hier haben wir ausgenutzt, dass sich in der Summe fast alle Summanden aufheben, was wir nun verallgemeinern wollen.

\subsection{Einführung}

\begin{figure}[h]
\vspace{\baselineskip}
\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Teleskop.svg}{\textbf{Teleskop.svg}} by File Upload Bot (Magnus Manske) \textit{(Public domain)}}\centering
\adjincludegraphics[max width=.5\textwidth, max height=0.2\textheight]{file58teleskop9559237baa72d79267b2b7c2570fda2d17ab0b168a}
\caption*{Zusammenschieben von Teleskopen: Namensgeber der Teleskopsumme (\arabic{imagelabel})}
\end{figure}
Eine Teleskopsumme ist eine Summe der Form $\sum _{k=1}^{n}(a_{k}-a_{k+1})$. Hier heben sich benachbarte Summanden zum Teil auf. Man erhält

\begin{align*}
\sum _{k=1}^{n}(a_{k}-a_{k+1})&=(a_{1}-a_{2})+(a_{2}-a_{3})+\ldots +(a_{n}-a_{n+1})\\&=a_{1}+(-a_{2}+a_{2})+(-a_{3}+a_{3})+\ldots +(-a_{n}+a_{n})-a_{n+1}\\&=a_{1}+0+0+0+\ldots +0-a_{n+1}\\&=a_{1}-a_{n+1}
\end{align*}

Durch eine analoge Rechnung bekommt man

\begin{align*}
\sum _{k=1}^{n}(a_{k+1}-a_{k})=a_{n+1}-a_{1}
\end{align*}

Der Name leitet sich vom Zusammenschieben von Teleskopen ab, die aus einzelnen Rohren aufgebaut sind. Fassen wir zusammen:

\begin{definition*}[Teleskopsumme]
Eine Teleskopsumme ist eine Summe der Form $\sum _{k=1}^{n}(a_{k}-a_{k+1})$ beziehungsweise $\sum _{k=1}^{n}(a_{k+1}-a_{k})$.

\end{definition*}

\begin{theorem*}[Wert der Teleskopsumme]
Es ist:

\begin{align*}
\sum _{k=1}^{n}(a_{k}-a_{k+1})&=a_{1}-a_{n+1}\\[0.5em]\sum _{k=1}^{n}(a_{k+1}-a_{k})&=a_{n+1}-a_{1}
\end{align*}

\end{theorem*}

\section{Teleskopreihe}

Jetzt übertragen wir die Idee einer Teleskopsumme auf Reihen. Wir setzen das Beispiel zu Teleskopsummen fort.

\subsection{Beispiel}

Wir betrachten die Reihe $\sum _{k=1}^{\infty }{\frac {1}{k(k+1)}}$. Wir haben bereits gesehen, dass für alle $n\in \mathbb {N} $ gilt $\sum _{k=1}^{n}{\frac {1}{k(k+1)}}=\sum _{k=1}^{n}{\frac {1}{k}}-{\frac {1}{k+1}}=1-{\frac {1}{n+1}}$. Also ist der Wert der Reihe $\sum _{k=1}^{\infty }{\frac {1}{k(k+1)}}=\lim _{n\to \infty }1-{\frac {1}{n+1}}=1$.

\subsection{Einführung}

Teleskopreihen sind Reihen, deren Partialsummen Teleskopsummen sind. Sie sind also Reihen der Form $\sum _{k=1}^{\infty }(a_{k}-a_{k+1})$. Als Partialsummenfolge erhält man

\begin{align*}
\sum _{k=1}^{\infty }(a_{k}-a_{k+1})&=\left(\sum _{k=1}^{n}(a_{k}-a_{k+1})\right)_{n\in \mathbb {N} }\\[1em]&{\color {OliveGreen}\left\downarrow \ \sum _{k=1}^{n}(a_{k}-a_{k+1}){\text{ ist eine Teleskopsumme}}\right.}\\[1em]&=\left(a_{1}-a_{n+1}\right)_{n\in \mathbb {N} }
\end{align*}

Um die Konvergenz einer Teleskopreihe zu bestimmen, müssen wir also das Konvergenzverhalten der Folge $\left(a_{1}-a_{n+1}\right)_{n\in \mathbb {N} }$ untersuchen. Diese Folge konvergiert genau dann, wenn die Folge $(a_{n})_{n\in \mathbb {N} }$ konvergiert.

Nehmen wir einmal an, dass $(a_{n})_{n\in \mathbb {N} }$ konvergiert und dass $a$ der Grenzwert dieser Folge ist. Als Grenzwert der Teleskopreihe erhalten wir dann

\begin{align*}
\sum _{k=1}^{\infty }(a_{k}-a_{k+1})&=\lim _{n\to \infty }\sum _{k=1}^{n}(a_{k}-a_{k+1})\\[1em]&=\lim _{n\to \infty }(a_{1}-a_{n+1})\\[1em]&{\color {OliveGreen}\left\downarrow \ \lim _{n\to \infty }(a_{n}+b_{n})=\lim _{n\to \infty }a_{n}+\lim _{n\to \infty }b_{n}\right.}\\[1em]&=\lim _{n\to \infty }a_{1}-\lim _{n\to \infty }a_{n+1}\\&=a_{1}-a
\end{align*}

Wenn $(a_{n})_{n\in \mathbb {N} }$ divergiert, dann divergiert auch die Folge $(a_{1}-a_{n+1})_{n\in \mathbb {N} }$. Somit divergiert auch die Teleskopreihe.

\subsection{Definition und weiteres Beispiel}

\begin{definition*}[Teleskopreihe]
Eine Teleskopreihe ist eine Reihe der Form $\sum _{k=1}^{\infty }(a_{k}-a_{k+1})$.

\end{definition*}

Wir haben bereits bewiesen:

\begin{theorem*}[Konvergenz von Teleskopreihen]
Eine Teleskopreihe $\sum _{k=1}^{\infty }(a_{k}-a_{k+1})$ konvergiert genau dann, wenn die Folge $(a_{n})_{n\in \mathbb {N} }$ konvergiert. Der Grenzwert dieser Reihe ist dann

\begin{align*}
\sum _{k=1}^{\infty }(a_{k}-a_{k+1})=a_{1}-\lim _{n\to \infty }a_{n}
\end{align*}

\end{theorem*}

\begin{example*}[Teleskopreihe]
Die Reihe $\sum _{k=4}^{\infty }\left((-1)^{k}-(-1)^{k+1}\right)$ divergiert, weil die Folge $a_{k}=(-1)^{k}$ divergiert. Die Reihe $\sum _{k=2}^{\infty }\left({\sqrt[{k}]{9}}-{\sqrt[{k+1}]{9}}\right)$ hingegen konvergiert, da die Folge $a_{k}={\sqrt[{k}]{9}}$ gegen $1$ konvergiert. Der Grenzwert dieser Reihe ist

\begin{align*}
\sum _{k=2}^{\infty }\left({\sqrt[{k}]{9}}-{\sqrt[{k+1}]{9}}\right)&=\lim _{n\to \infty }\sum _{k=2}^{n}\left({\sqrt[{k}]{9}}-{\sqrt[{k+1}]{9}}\right)\\[1em]&{\color {OliveGreen}\left\downarrow \ {\text{Telekopsumme}}\right.}\\[1em]&=\lim _{n\to \infty }\left(\underbrace {\sqrt[{2}]{9}} _{=\ 3}-\underbrace {\sqrt[{n+1}]{9}} _{\to \ 1}\right)\\[1em]&=3-1\\&=2
\end{align*}

\end{example*}

\chapter{Geometrische Reihe}

Die geometrische Reihe hat die Form $\sum _{k=0}^{\infty }q^{k}$. Sie ist eine wichtige Reihe, die dir häufig in Beweisen und Herleitungen begegnen wird. Außerdem kann man mit der geometrischen Reihe Konvergenzkritierien wie das Quotienten- oder das Wurzel-Kriterium beweisen.

\section{Geometrische Summenformel}

Wir wiederholen die geometrische Summenformel. Mit dieser Formel können wir die Partialsummen der geometrischen Reihe explizit ausrechnen.  Beweisen wir nun die geometrische Summenformel:

\begin{theorem*}[Geometrische Summenformel]
Für alle reellen $q\neq 1$ ist

\begin{align*}
\sum _{k=0}^{n}q^{k}={\frac {1-q^{n+1}}{1-q}}
\end{align*}

\end{theorem*}

\begin{proof*}[Geometrische Summenformel]
Es ist

\begin{align*}
\sum _{k=0}^{n}q^{k}&=\ 1+q+q^{2}+\ldots +q^{n}\\[0.5em]&{\color {OliveGreen}\left\downarrow \ {\text{beide Seiten mit }}q{\text{ multiplizieren}}\right.}\\[0.5em]\implies \ q\cdot \sum _{k=0}^{n}q^{k}&=\ q+q^{2}+q^{3}+\ldots +q^{n+1}\\[0.5em]&{\color {OliveGreen}\left\downarrow \ {\text{zweite von erster Gleichung subtrahieren}}\right.}\\[0.5em]\implies \ \sum _{k=0}^{n}q^{k}-q\cdot \sum _{k=0}^{n}q^{k}&=\ (1+q+\ldots +q^{n})-(q+q^{2}+\ldots +q^{n+1})\\[0.5em]&=1-q^{n+1}\\[0.5em]&{\color {OliveGreen}\left\downarrow \ {\text{links }}\sum _{k=0}^{n}q^{k}{\text{ ausklammern}}\right.}\\[0.5em]\implies \ (1-q)\cdot \sum _{k=0}^{n}q^{k}&=\ 1-q^{n+1}\\[0.5em]&{\color {OliveGreen}\left\downarrow \ {}\cdot {\frac {1}{1-q}}{\text{, da }}q\neq 1\right.}\\[0.5em]\implies \ \sum _{k=0}^{n}q^{k}&=\ {\frac {1-q^{n+1}}{1-q}}\\[0.5em]
\end{align*}

\end{proof*}

\section{Geometrische Reihe}

\begin{figure}[h]
\vspace{\baselineskip}
\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Geometric_sequences.svg}{\textbf{Geometric\allowbreak\_sequences.svg}} by Resident Mario \textit{(CC BY-SA 3.0)}}\centering
\adjincludegraphics[max width=.5\textwidth, max height=0.2\textheight]{file58geometric95sequences95d07a9c00b7827123e49b3c348f79ea8ceff2a773}
\caption*{Die geometrische Reihe $\sum _{k=0}^{\infty }r^{k}$ für $r={\tfrac {1}{2}}$, $r={\tfrac {1}{3}}$ oder $r={\tfrac {1}{4}}$ konvergiert. (\arabic{imagelabel})}
\end{figure}
Wir betrachten zwei Fälle: $|q|<1{\text{ und }}|q|\geq 1$.

\subsection{Fall $|q|<1$}

Kommen wir zur geometrischen Reihe $\sum _{k=0}^{\infty }q^{k}$. Wir betrachten zunächst den Fall $|q|<1$ und damit $q\neq 1$, da wir nur in diesem Fall die geometrische Summenformel anwenden können. Mit dieser Formel können wir die Partialsumme explizit berechnen. Wir erhalten:

\begin{align*}
\sum _{k=0}^{\infty }q^{k}&=\left(\sum _{k=0}^{n}q^{k}\right)_{n\in \mathbb {N} }\\[1em]&{\color {OliveGreen}\left\downarrow \ {\text{Geometrische Summenformel }}(q\neq 1)\right.}\\[1em]&=\left({\frac {1-q^{n+1}}{1-q}}\right)_{n\in \mathbb {N} }
\end{align*}

Die geometrische Reihe konvergiert also genau dann, wenn die Folge $\left({\tfrac {1-q^{n+1}}{1-q}}\right)_{n\in \mathbb {N} }$ konvergiert. Dies ist genau dann der Fall, wenn $\left(q^{n}\right)_{n\in \mathbb {N} }$ eine konvergente Folge ist. Nun wissen wir, dass $\left(q^{n}\right)_{n\in \mathbb {N} }$ gegen $0$ konvergiert, wenn $|q|<1$ ist, und gegen $1$ konvergiert, wenn $q=1$ ist. Den Fall $q=1$ haben wir in diesem Abschnitt aber ausgeschlossen. Damit erhalten wir zunächst:

\begin{importantparagraph*}
Wenn $|q|<1$ ist, dann konvergiert die geometrische Reihe $\sum _{k=0}^{\infty }q^{k}$.

\end{importantparagraph*}

Berechnen wir nun den Grenzwert der geometrischen Reihe für $|q|<1$:

\begin{align*}
\sum _{k=0}^{\infty }q^{k}&=\lim _{n\to \infty }\sum _{k=0}^{n}q^{k}\\[0.3em]&=\lim _{n\to \infty }{\frac {1-q^{n+1}}{1-q}}\\[0.3em]&{\color {OliveGreen}\left\downarrow \ {\text{Grenzwertsätze}}\right.}\\[0.3em]&={\frac {1-\lim _{n\to \infty }q^{n+1}}{1-q}}\\[0.3em]&{\color {OliveGreen}\left\downarrow \ |q|<1\right.}\\[0.3em]&={\frac {1-0}{1-q}}\\[0.3em]&={\frac {1}{1-q}}
\end{align*}

\subsection{Fall $|q|\geq 1$}

Bei $|q|\geq 1$ gilt für alle $k\in \mathbb {N} _{0}$, dass $\left|q^{k}\right|\geq 1$. Also ist die Folge $\left(q^{k}\right)_{k\in \mathbb {N} _{0}}$ keine Nullfolge. Damit divergiert die Reihe $\sum _{k=0}^{\infty }q^{k}$ nach dem sogenannten \href{https://de.wikibooks.org/wiki/Mathe\_für\_Nicht-Freaks:\_Trivialkriterium,\_Nullfolgenkriterium,\_Divergenzkriterium}
{Trivialkriterium}, das wir später noch genauer betrachten.

Um die Divergenz zu veranschaulichen, betrachten wir den Fall für ein positives $q$, also $q\geq 1$. So folgt für alle $k\in \mathbb {N} {\text{, dass }}q^{k}\geq 1$. Damit können wir die Partialsummen abschätzen: $\sum _{k=1}^{n}q^{k}\geq \sum _{k=1}^{n}1=n$ Also ist die Folge der Partialsummen durch die Folge $(n)_{n\in \mathbb {N} }$ nach unten beschränkt. Da $(n)_{n\in \mathbb {N} }$ divergiert, divergiert auch die Reihe $\sum _{k=0}^{\infty }q^{k}$ als Folge der Partialsummen.

\subsection{Zusammenfassung}

Fassen wir das bereits Bewiesene zusammen: Für $|q|>1$, $q=-1$ und $q=1$ divergiert die geometrische Reihe. Diese drei Fälle können wir in der Bedingung $|q|\geq 1$ zusammenfassen. Für den Fall $|q|<1$ konvergiert die geometrische Reihe und hat als Grenzwert ${\tfrac {1}{1-q}}$:

\begin{theorem*}[Geometrische Reihe]
Die geometrische Reihe $\sum _{k=0}^{\infty }q^{k}$ konvergiert genau dann, wenn $|q|<1$ ist. Sie hat dann den Wert ${\tfrac {1}{1-q}}$:

\begin{align*}
\sum _{k=0}^{\infty }q^{k}={\begin{cases}{\frac {1}{1-q}}&;|q|<1\\{\text{divergent}}&;|q|\geq 1\end{cases}}
\end{align*}

\end{theorem*}

\begin{example*}[Geometrische Reihe]
Für $q={\tfrac {1}{2}}$, $q=-{\tfrac {1}{2}}$ und $q=2$ gilt

\begin{align*}
\sum _{k=0}^{\infty }\left({\frac {1}{2}}\right)^{k}&=1+{\frac {1}{2}}+{\frac {1}{4}}+{\frac {1}{8}}+{\frac {1}{16}}+\ldots ={\frac {1}{1-{\frac {1}{2}}}}={\frac {1}{\frac {1}{2}}}=2\\[1em]\sum _{k=0}^{\infty }\left(-{\frac {1}{2}}\right)^{k}&=1-{\frac {1}{2}}+{\frac {1}{4}}-{\frac {1}{8}}+{\frac {1}{16}}-\ldots ={\frac {1}{1+{\frac {1}{2}}}}={\frac {1}{\frac {3}{2}}}={\frac {2}{3}}\\[1em]\sum _{k=0}^{\infty }2^{k}&=1+2+4+8+16+\ldots \ {\text{ divergiert}}
\end{align*}

\end{example*}

\chapter{Harmonische Reihe}

Wir betrachten nun die Reihe

\begin{align*}
\sum _{k=1}^{\infty }{\frac {1}{k}}=1+{\frac {1}{2}}+{\frac {1}{3}}+{\frac {1}{4}}+\ldots 
\end{align*}

In der untenstehenden Grafik sind die ersten Partialsummen $S_{N}=\sum _{k=1}^{N}{\frac {1}{k}}$ dieser Reihe aufgetragen.

\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:HarmonicPartialSums.svg}{\textbf{HarmonicPartialSums.svg}} by Autorenkollektiv „Auswahlaxiom“ (Charlotte Dietze, Matthias Paulsen, Anne Reif) \textit{(CC BY-SA 4.0)}}\begin{center}
\adjincludegraphics[max width=0.5\textwidth,max height=0.2\textheight]{file58harmonicpartialsums9542b685866785cc6fe9fe6d620657a99d1094f6aa}\end{center}

\begin{align*}
S_{N+1}=\sum _{k=1}^{N+1}{\frac {1}{k}}={\frac {1}{N+1}}+\sum _{k=1}^{N}{\frac {1}{k}}={\frac {1}{N+1}}+S_{N}>S_{N}
\end{align*}

Wir wissen, dass monotone Folgen genau dann konvergieren, wenn sie beschränkt sind. Also ist auch hier die entscheidende Frage, ob die Folge der Partialsummen beschränkt ist.



\section{Harmonische Reihe}

\subsection{Divergenz der harmonischen Reihe}

Unsere Vermutung ist, dass die harmonische Reihe divergiert. Dies wollen wir nun beweisen:

\begin{theorem*}[Divergenz der harmonischen Reihe]
Die harmonische Reihe $\sum _{k=1}^{\infty }{\frac {1}{k}}$ divergiert.

\end{theorem*}

\begin{solutionprocess*}[Divergenz der harmonischen Reihe]
Die Folge $\left({\tfrac {1}{k}}\right)_{k\in \mathbb {N} }$ ist monoton fallend. Wenn $n\geq m$ ist, ist ${\tfrac {1}{n}}\leq {\tfrac {1}{m}}$. Dementsprechend können wir die Summanden geschickt nach unten abschätzen:

\begin{align*}
\sum _{k=1}^{\infty }{\frac {1}{k}}&=1+{\color {OliveGreen}{\frac {1}{2}}}+{\color {Indigo}\left({\frac {1}{3}}+{\frac {1}{4}}\right)}+{\color {Blue}\left({\frac {1}{5}}+{\frac {1}{6}}+{\frac {1}{7}}+{\frac {1}{8}}\right)}+\ldots \\[0.5em]&\left\downarrow \ {\color {Indigo}{\frac {1}{3}}\geq {\frac {1}{4}}}{\text{ und }}{\color {Blue}{\frac {1}{5}}\geq {\frac {1}{6}}\geq {\frac {1}{7}}\geq {\frac {1}{8}}}\right.\\[0.5em]&\geq 1+{\color {OliveGreen}{\frac {1}{2}}}+{\color {Indigo}\left({\frac {1}{4}}+{\frac {1}{4}}\right)}+{\color {Blue}\left({\frac {1}{8}}+{\frac {1}{8}}+{\frac {1}{8}}+{\frac {1}{8}}\right)}+\ldots \\[0.5em]&=1+{\color {OliveGreen}{\frac {1}{2}}}+{\color {Indigo}2\cdot {\frac {1}{4}}}+{\color {Blue}4\cdot {\frac {1}{8}}}+\ldots \\[0.5em]&=1+{\color {OliveGreen}{\frac {1}{2}}}+{\color {Indigo}{\frac {1}{2}}}+{\color {Blue}{\frac {1}{2}}}+\ldots 
\end{align*}

An der letzten Reihe können wir erkennen, dass die Abschätzung gegen unendlich strebt und damit divergiert. Da wir nach unten abgeschätzt haben, muss auch $\sum _{k=1}^{\infty }{\frac {1}{k}}$ divergieren. 

\end{solutionprocess*}



\chapter{Absolute Konvergenz einer Reihe}

In diesem Kapitel werden wir mit der absoluten Konvergenz eine neue und stärkere Art der Konvergenz kennenlernen, welche auch bei einer beliebigen Umsortierung der Summanden ihr Konvergenzverhalten nicht ändert, was wir in einem späteren Kapitel genauer betrachten werden.

\section{Motivation}

Bei endlichen Summen ist es egal, in welcher Reihenfolge man die Summanden aufschreibt. Beispielsweise ist das Ergebnis von

\begin{align*}
1+2+3+4+5+6+7+8+9+10
\end{align*}

dasselbe wie von

\begin{align*}
2+9+5+3+10+6+7+4+1+8
\end{align*}

Dies gilt aufgrund der Kommutativität der Addition. Sie besagt, dass $a+b=b+a$ für alle reellen Zahlen $a,b\in \mathbb {R} $ ist. Wenn man endlich oft benachbarte Summanden vertauscht, ändert sich das Ergebnis nicht. Diese Invarianz bzw. „Unveränderlichkeit“ des Werts endlicher Summen gegenüber Summandenvertauschungen geht bei unendlichen Summen (also bei Reihen) verloren. Nehmen wir eine Reihe

\begin{align*}
a_{1}+a_{2}+a_{3}+a_{4}+\ldots 
\end{align*}

Der Wert dieser Reihe kann sich durch eine Umordnung der Summanden ändern. So kann der Wert der folgenden Reihe ein anderer sein, als bei der ursprünglichen Reihe:

\begin{align*}
a_{42}+a_{9}+a_{1000}+a_{543}+\ldots 
\end{align*}

Bei einer Umordnung der Summanden kann eine konvergente Reihe sogar divergent werden und umgekehrt. Es stellt sich die Frage: Wann können wir die Summanden einer Reihe beliebig umordnen, ohne dass ihr Grenzwert oder gar ihr Konvergenzverhalten geändert wird? Für konvergente Reihen über reelle Zahlen kann man diese Frage leicht beantworten:

\begin{importantparagraph*}
Das Grenzwertverhalten einer reellwertigen und konvergenten Reihe ist genau dann immun gegen eine Umsortierung ihrer Summanden, wenn sie absolut konvergiert.

\end{importantparagraph*}

\section{Definition}

Was ist absolute Konvergenz?

\begin{definition*}[absolute Konvergenz]
Eine Reihe $\sum _{k=1}^{\infty }a_{k}$ konvergiert genau dann absolut, wenn $\sum _{k=1}^{\infty }|a_{k}|$ konvergiert.

\end{definition*}

Eine Reihe ist also genau dann absolut konvergent, wenn die Reihe ihrer Absolutbeträge konvergiert. Bei absolut konvergenten Reihen werden die Beträge ihrer Summanden so schnell klein, dass die Summe der Beträge beschränkt bleibt (und damit die Reihe konvergiert).

\begin{hint*}
Ist $a_{k}\geq 0$ für alle $k\in \mathbb {N} $, dann ist $|a_{k}|=a_{k}$. Die absolute Konvergenz einer Reihe $\sum _{k=1}^{\infty }a_{k}$ mit $a_{k}\geq 0$ ist damit gleichbedeutend mit der Konvergenz dieser Reihe.

Also: Eine Reihe mit ausschließlich nicht negativen Summanden konvergiert genau dann absolut, wenn sie konvergiert.

\end{hint*}

\section{Jede absolut konvergente Reihe konvergiert}

Absolute Konvergenz ist eine stärkere Form der Konvergenz einer Reihe. Absolut konvergente Reihen sind genau die konvergenten Reihen, deren Konvergenzverhalten immun gegen eine Umsortierung der Summanden ist. Jede absolut konvergente Reihe konvergiert. Dies zeigen wir im folgenden Satz:

\begin{theorem*}[Absolute Konvergenz impliziert normale Konvergenz.]
Jede absolut konvergente Reihe konvergiert.

\end{theorem*}

\begin{proof*}[Absolute Konvergenz impliziert normale Konvergenz.]
Sei $\sum _{k=1}^{\infty }a_{k}$ eine absolut konvergente Reihe. Das bedeutet $\sum _{k=1}^{\infty }|a_{k}|$ konvergiert. Wir betrachten nun die Reihen als Partialsummenfolgen. In diesem Beweis wollen wir Cauchy-Folgen anwenden. Wir zeigen, dass die Partialsummenfolge der Reihe $\sum _{k=1}^{\infty }a_{k}$ eine Cauchy-Folge ist. Sei dafür $\epsilon >0$. Da $\sum _{k=1}^{\infty }|a_{k}|$ konvergiert, ist die Partialsummenfolge $\left(\sum _{k=1}^{n}|a_{k}|\right)_{n\in \mathbb {N} }$ eine Cauchy-Folge. Also gibt es ein $N\in \mathbb {N} $, sodass für alle $m{\text{ und }}n>N{\text{ gilt }}\left|\sum _{k=1}^{n}|a_{k}|-\sum _{k=1}^{m}|a_{k}|\right|<\epsilon $. Fassen wir die Summen zusammen und nehmen o.B.d.A. an, dass $n>m$, dann gilt

\begin{align*}
\sum _{k=m+1}^{n}|a_{k}|=\left|\sum _{k=m+1}^{n}|a_{k}|\right|=\left|\sum _{k=1}^{n}|a_{k}|-\sum _{k=1}^{m}|a_{k}|\right|<\epsilon 
\end{align*}

Nun schätzen wir die Partialsummen der Reihe $\sum _{k=1}^{\infty }a_{k}$ ab. Seien $n>m>N$, so folgt

\begin{align*}
\left|\sum _{k=1}^{n}a_{k}-\sum _{k=1}^{m}a_{k}\right|&=\left|\sum _{k=m+1}^{n}a_{k}\right|\\[0.3em]&{\color {OliveGreen}\left\downarrow {\text{Dreiecksungleichung}}\right.}\\[0.3em]&\leq \sum _{k=m+1}^{n}|a_{k}|<\epsilon 
\end{align*}

Also ist $\left(\sum _{k=1}^{n}a_{k}\right)$ eine Cauchy-Folge und konvergiert deshalb. Somit konvergiert auch $\sum _{k=1}^{\infty }a_{k}$.

\end{proof*}

\begin{hint*}
Aus dem Beweis folgt insbesondere $\left|\sum _{k=1}^{\infty }a_{k}\right|\leq \sum _{k=1}^{\infty }|a_{k}|$, falls die Reihe absolut konvergiert. Es ist nämlich im Fall der absoluten Konvergenz:

\begin{align*}
\left|\sum _{k=1}^{\infty }a_{k}\right|&=\left|\lim _{n\to \infty }\sum _{k=1}^{n}a_{k}\right|\\[0.3em]&{\color {OliveGreen}\left\downarrow \lim _{n\to \infty }|c_{n}|=\left|\lim _{n\to \infty }c_{n}\right|\right.}\\[0.3em]&=\lim _{n\to \infty }\left|\sum _{k=1}^{n}a_{k}\right|\\[0.3em]&{\color {OliveGreen}\left\downarrow (\forall n:c_{n}\leq d_{n})\Rightarrow \lim _{n\to \infty }c_{n}\leq \lim _{n\to \infty }d_{n}\right.}\\[0.3em]&\leq \lim _{n\to \infty }\sum _{k=1}^{n}|a_{k}|\\[0.3em]&=\sum _{k=1}^{\infty }|a_{k}|
\end{align*}

\end{hint*}

\section{Nicht jede konvergente Reihe ist absolut konvergent}

Wir haben gerade bewiesen, dass jede absolut konvergente Reihe eine konvergente Reihe ist. Die Umkehrung gilt aber nicht: Es gibt konvergente Reihen, die nicht absolut konvergieren. Ein Beispiel ist die alternierende Reihe $\sum _{k=1}^{\infty }(-1)^{k}{\frac {1}{k}}$. Diese Reihe konvergiert, was man mit dem \href{https://de.wikibooks.org/wiki/Mathe\_für\_Nicht-Freaks:\_Leibniz-Kriterium}
{Leibniz-Kriterium} beweisen kann. Jedoch ist diese Reihe nicht absolut konvergent, da $\sum _{k=1}\left|(-1)^{k}{\frac {1}{k}}\right|=\sum _{k=1}^{\infty }{\frac {1}{k}}$ die divergente harmonische Reihe ist. Merken wir uns also:

\begin{importantparagraph*}
Jede absolut konvergente Reihe konvergiert, aber nicht jede konvergente Reihe konvergiert absolut.

\end{importantparagraph*}

\section{Charakteristisches Kriterium für absolute Konvergenz}

Nun möchten wir ein notwendiges und hinreichendes Kriterium für absolute Konvergenz untersuchen. Jede Reihe $\sum _{k=1}^{\infty }a_{k}$ lässt sich in ihre postiven und negativen Reihenglieder zerlegen. Formal definieren wir dazu

\begin{align*}
a_{k}^{+}={\begin{cases}a_{k}&a_{k}>0\\0&a_{k}\leq 0\end{cases}}
\end{align*}

und

\begin{align*}
a_{k}^{-}={\begin{cases}0&a_{k}\geq 0\\a_{k}&a_{k}<0\end{cases}}
\end{align*}

Ist beispielsweise $a_{k}=(-1)^{k+1}{\tfrac {1}{k^{2}}}$, so ist

\begin{align*}
a_{k}^{+}={\begin{cases}{\frac {1}{k^{2}}}&k{\text{ ungerade}}\\0&k{\text{ gerade}}\end{cases}}
\end{align*}

und

\begin{align*}
a_{k}^{-}={\begin{cases}0&k{\text{ ungerade}}\\-{\tfrac {1}{k^{2}}}&k{\text{ gerade}}\end{cases}}
\end{align*}

Es gilt $a_{k}=a_{k}^{+}+a_{k}^{-}$ und damit $\left(\sum _{k=1}^{n}a_{k}\right)_{n\in \mathbb {N} }=\left(\sum _{k=1}^{n}(a_{k}^{+}+a_{k}^{-})\right)_{n\in \mathbb {N} }$. Die Frage ist nun, wann die beiden Reihen $\left(\sum _{k=1}^{n}a_{k}^{+}\right)_{n\in \mathbb {N} }$ und $\left(\sum _{k=1}^{n}a_{k}^{-}\right)_{n\in \mathbb {N} }$ konvergieren. Dann folgt auch $\sum _{k=1}^{\infty }a_{k}=\sum _{k=1}^{\infty }a_{k}^{+}+\sum _{k=1}^{\infty }a_{k}^{-}$. Im folgenden Satz zeigen wir, dass die beiden Reihen genau dann konvergieren, wenn die ursprüngliche Reihe absolut konvergiert.

\begin{theorem*}[Kriterium für absolute Konvergenz]
Die Reihe $\sum _{k=1}^{\infty }a_{k}$ konvergiert genau dann absolut, wenn $\sum _{k=1}^{\infty }a_{k}^{+}$ und $\sum _{k=1}^{\infty }a_{k}^{-}$ konvergieren. In diesem Fall gilt $\sum _{k=1}^{\infty }a_{k}=\sum _{k=1}^{\infty }a_{k}^{+}+\sum _{k=1}^{\infty }a_{k}^{-}$.

\end{theorem*}
\clearpage
\begin{proof*}[Kriterium für absolute Konvergenz]
\proofstep{Hinrichtung:}
 Wenn $\sum _{k=1}^{\infty }a_{k}$ absolut konvergiert, dann konvergieren auch $\sum _{k=1}^{\infty }a_{k}^{+}$ und $\sum _{k=1}^{\infty }a_{k}^{-}$.\begin{indentblock}
Sei $\sum _{k=1}^{\infty }a_{k}$ absolut konvergent. Es konvergiert damit $\sum _{k=1}^{\infty }|a_{k}|$. Es gilt sowohl $|a_{k}^{+}|\leq |a_{k}|$ als auch $|a_{k}^{-}|\leq |a_{k}|$. Nach dem \href{https://de.wikibooks.org/wiki/Mathe\_für\_Nicht-Freaks:\_Majorantenkriterium\_und\_Minorantenkriterium}
{Majorantenkriterium für Reihen} konvergieren $\sum _{k=1}^{\infty }a_{k}^{+}$ und $\sum _{k=1}^{\infty }a_{k}^{-}$ absolut. Damit konvergieren sie auch im gewöhnlichen Sinn.

\end{indentblock}

\proofstep{Rückrichtung:}
 Wenn $\sum _{k=1}^{\infty }a_{k}^{+}$ und $\sum _{k=1}^{\infty }a_{k}^{-}$ konvergieren, dann konvergiert $\sum _{k=1}^{\infty }a_{k}$ absolut.\begin{indentblock}
Seien $\sum _{k=1}^{\infty }a_{k}^{+}$ und $\sum _{k=1}^{\infty }a_{k}^{-}$ konvergent. Es gilt $|a_{k}|=a_{k}^{+}-a_{k}^{-}$, und damit folgt aus den \href{https://de.wikibooks.org/wiki/Mathe\_für\_Nicht-Freaks:\_Rechenregeln\_für\_Reihen}
{Rechenregeln für Reihen} $\sum _{k=1}^{\infty }a_{k}^{+}-\sum _{k=1}^{\infty }a_{k}^{-}=\sum _{k=1}^{\infty }(a_{k}^{+}-a_{k}^{-})=\sum _{k=1}^{\infty }|a_{k}|$.

\end{indentblock}

\end{proof*}

\chapter{Umordnungssatz für Reihen}

In diesem Kapitel wollen wir untersuchen, unter welchen Voraussetzungen es erlaubt ist, Reihen umzuordnen, ohne dass sich deren Konvergenzverhalten beziehungsweise deren Grenzwert ändert.  Dabei werden wir uns schrittweise, beginnend bei endlichen Summen vorarbeiten. Wir untersuchen zunächst immer konkrete Beispiele, um unsere Überlegungen möglichst verständlich zu machen. Wie im Kapitel zuvor bereits erwähnt, spielt letztendlich die \href{https://de.wikibooks.org/wiki/Mathe\_für\_Nicht-Freaks:\_Absolute\_Konvergenz\_einer\_Reihe}
{\emph{absolute Konvergenz}} die entscheidende Rolle.

\section{Umordnung endlicher Summen}

Bei endlichen Summen ist es kein Problem, die Summanden umzuordnen. Der Grund hierfür ist, dass sich das {''}Kommutativgesetz der Addition{''} beliebig oft hintereinander anwenden lässt. Betrachten wir als konkretes Beispiel die Summe

\begin{align*}
\sum _{k=1}^{8}(-1)^{k+1}{\frac {1}{k}}&={\color {OliveGreen}1}{\color {blue}-{\frac {1}{2}}}{\color {OliveGreen}+{\frac {1}{3}}}{\color {blue}-{\frac {1}{4}}}{\color {OliveGreen}+{\frac {1}{5}}}{\color {blue}-{\frac {1}{6}}}{\color {OliveGreen}+{\frac {1}{7}}}{\color {blue}-{\frac {1}{8}}}={\frac {533}{840}}
\end{align*}

Ordnen wir die Summe nun so um, dass auf ein positives immer zwei negative Summenglieder folgen, ergibt sich

\begin{align*}
{\color {OliveGreen}1}{\color {blue}-{\frac {1}{2}}-{\frac {1}{4}}}{\color {OliveGreen}+{\frac {1}{3}}}{\color {blue}-{\frac {1}{6}}-{\frac {1}{8}}}{\color {OliveGreen}+{\frac {1}{5}}+{\frac {1}{7}}}={\frac {533}{840}}
\end{align*}

Etwas formaler können wir dies folgendermaßen formulieren: Ist $\sigma :\{1,2,3,4,5,6,7,8\}\to \{1,2,3,4,5,6,7,8\}$ die Bijektion mit

\begin{align*}
\sigma (1)=1,\ \sigma (2)=2,\ \sigma (3)=4,\ \sigma (4)=3,\ \sigma (5)=6,\ \sigma (6)=8,\ \sigma (7)=5,\ \sigma (8)=7
\end{align*}

so gilt

\begin{align*}
\underbrace {\sum _{k=1}^{8}(-1)^{k+1}{\frac {1}{k}}} _{{\color {OliveGreen}1}{\color {blue}-{\frac {1}{2}}}{\color {OliveGreen}+{\frac {1}{3}}}{\color {blue}-{\frac {1}{4}}}{\color {OliveGreen}+{\frac {1}{5}}}{\color {blue}-{\frac {1}{6}}}{\color {OliveGreen}+{\frac {1}{7}}}{\color {blue}-{\frac {1}{8}}}}=\underbrace {\sum _{k=1}^{8}(-1)^{\sigma (k)+1}{\frac {1}{\sigma (k)}}} _{{\color {OliveGreen}1}{\color {blue}-{\frac {1}{2}}-{\frac {1}{4}}}{\color {OliveGreen}+{\frac {1}{3}}}{\color {blue}-{\frac {1}{6}}-{\frac {1}{8}}}{\color {OliveGreen}+{\frac {1}{5}}+{\frac {1}{7}}}}
\end{align*}

Damit können wir für $n\in \mathbb {N} $ auf beliebige Summen $\sum _{k=1}^{n}a_{k}$ und beliebige Bijektionen $\sigma :\{1,\ldots ,n\}\to \{1,\ldots ,n\}$ ein \emph{verallgemeinertes Kommutativgesetz} formulieren:

\begin{align*}
\sum _{k=1}^{n}a_{k}=\sum _{k=1}^{n}a_{\sigma (k)}
\end{align*}

\section{Das Problem bei Reihen}

Bevor wir uns der Problematik bei Reihen zuwenden, wollen wir den Begriff der \emph{Umordnung einer Reihe} zunächst sauber definieren:

\begin{definition*}[Umordnung einer Reihe]
Sei $\sigma :\mathbb {N} \to \mathbb {N} $ eine bijektive Abbildung. Dann heißt die Reihe $\sum _{k=1}^{\infty }a_{\sigma (k)}$ eine \emph{Umordnung} der Reihe $\sum _{k=1}^{\infty }a_{k}$.

\end{definition*}

\begin{example*}[Umordnung einer Reihe]
Betrachten wir die harmonische Reihe $\sum _{k=1}^{\infty }{\tfrac {1}{k}}$. Dann entspricht die Reihe

\begin{align*}
{\tfrac {1}{2}}+1+{\tfrac {1}{4}}+{\tfrac {1}{3}}+{\tfrac {1}{6}}+{\tfrac {1}{5}}+{\tfrac {1}{8}}+{\tfrac {1}{7}}+\ldots 
\end{align*}

der Umordnung $\sum _{k=1}^{\infty }a_{\sigma (k)}$ mit der durch

\begin{align*}
\sigma (2k-1)=2k{\text{ und }}\sigma (2k)=2k-1{\text{ für }}k\in \mathbb {N} 
\end{align*}

definierten Permutation.

\end{example*}

Es wäre natürlich gut, wenn wir das verallgemeinerte Kommutativgesetz von oben auf unendliche Reihen verallgemeinern könnten. Betrachten wir als Beispiel die alternierende harmonische Reihe

\begin{align*}
\sum _{k=1}^{\infty }(-1)^{k+1}{\frac {1}{k}}={\color {OliveGreen}1}{\color {blue}-{\frac {1}{2}}}{\color {OliveGreen}+{\frac {1}{3}}}{\color {blue}-{\frac {1}{4}}}{\color {OliveGreen}+{\frac {1}{5}}}{\color {blue}-{\frac {1}{6}}}{\color {OliveGreen}+{\frac {1}{7}}}{\color {blue}-{\frac {1}{8}}}{\color {OliveGreen}+{\frac {1}{9}}}{\color {blue}-{\frac {1}{10}}}{\color {OliveGreen}+}\ldots {\color {OliveGreen}+{\frac {1}{2n-1}}}{\color {blue}-{\frac {1}{2n}}}\pm \ldots 
\end{align*}

Im Kapitel zum \href{https://de.wikibooks.org/wiki/Mathe\_für\_Nicht-Freaks:\_Leibniz-Kriterium}
{„Leibniz-Kriterium“} werden wir zeigen, dass diese Reihe konvergiert. Mit weiteren Hilfsmitteln kann man sogar zeigen, dass sie gegen $S=\ln(2)$ konvergiert.

Konvergiert jede Umordnung dieser Reihe gegen denselben Grenzwert? Wir wählen dieselbe Umordnung wie oben: Auf jeden positiven Summanden der Reihe lassen wir zwei negative folgen:

\begin{align*}
{\color {OliveGreen}1}{\color {blue}-{\frac {1}{2}}-{\frac {1}{4}}}{\color {OliveGreen}+{\frac {1}{3}}}{\color {blue}-{\frac {1}{6}}-{\frac {1}{8}}}{\color {OliveGreen}+}\ldots {\color {OliveGreen}+{\frac {1}{2n-1}}}{\color {blue}-{\frac {1}{4n-2}}-{\frac {1}{4n}}}\pm \ldots 
\end{align*}

Im Kapitel zu den \href{https://de.wikibooks.org/wiki/Mathe\_für\_Nicht-Freaks:\_Rechenregeln\_für\_Reihen}
{„Rechenregeln für Reihen“} hatten wir gezeigt, dass sich das Konvergenzverhalten und der Grenzwert einer konvergenten Reihe nicht ändern, wenn wir Klammern setzen. Daher konvergiert die Reihe, in der wir immer drei Summanden durch Klammern zusammenfassen, gegen denselben Grenzwert:

\begin{align*}
\left({\color {OliveGreen}1}{\color {blue}-{\frac {1}{2}}-{\frac {1}{4}}}\right){\color {OliveGreen}+}\left({\color {OliveGreen}{\frac {1}{3}}}{\color {blue}-{\frac {1}{6}}-{\frac {1}{8}}}\right){\color {OliveGreen}+}\ldots {\color {OliveGreen}+}\left({\color {OliveGreen}{\frac {1}{2n-1}}}{\color {blue}-{\frac {1}{4n-2}}-{\frac {1}{4n}}}\right)\pm \ldots 
\end{align*}

Diese lässt sich wie folgt umformen:

\begin{align*}
&\left({\color {OliveGreen}1}{\color {blue}-{\frac {1}{2}}-{\frac {1}{4}}}\right){\color {OliveGreen}+}\left({\color {OliveGreen}{\frac {1}{3}}}{\color {blue}-{\frac {1}{6}}-{\frac {1}{8}}}\right){\color {OliveGreen}+}\ldots {\color {OliveGreen}+}\left({\color {OliveGreen}{\frac {1}{2n-1}}}{\color {blue}-{\frac {1}{4n-2}}-{\frac {1}{4n}}}\right)\pm \ldots \\=&\left({\frac {1}{2}}{\color {blue}-{\frac {1}{4}}}\right){\color {OliveGreen}+}\left({\frac {1}{6}}{\color {blue}-{\frac {1}{8}}}\right){\color {OliveGreen}+}\ldots {\color {OliveGreen}+}\left({\frac {1}{4n-2}}{\color {blue}-{\frac {1}{4n}}}\right)\pm \ldots \\=&{\frac {1}{2}}\left(1{\color {blue}-{\frac {1}{2}}}\right){\color {OliveGreen}+}{\frac {1}{2}}\left({\frac {1}{3}}{\color {blue}-{\frac {1}{4}}}\right){\color {OliveGreen}+}\ldots {\color {OliveGreen}+}{\frac {1}{2}}\left({\frac {1}{2n-1}}{\color {blue}-{\frac {1}{2n}}}\right)\pm \ldots \\=&{\frac {1}{2}}\left(1-{\frac {1}{2}}+{\frac {1}{3}}-{\frac {1}{4}}+\ldots +{\frac {1}{2n-1}}-{\frac {1}{2n}}\pm \ldots \right)\\=&{\frac {1}{2}}\left(\sum _{k=1}^{\infty }(-1)^{k+1}{\frac {1}{k}}\right)
\end{align*}

Wir sehen also, dass die umgeordnete Reihe nicht gegen $S$, sondern gegen ${\tfrac {1}{2}}S$ konvergiert.

\begin{warning*}
Ordnet man eine konvergente Reihe um, so kann diese Umordnung gegen einen anderen Grenzwert konvergieren.

Leider kommt es noch „schlimmer“! Die umgeordnete Reihe kann sogar divergieren.

\end{warning*}

\section{Umordnung von Reihen mit nichtnegativen Gliedern}

Bei den Beispielen von oben bestand bei den Umordnungen das Problem, dass die Reihe alternierend war. Daher konnten bei den Umordnungen so viele positive Summanden hintereinander „gepackt“ werden, dass die umgeordnete Reihe gegen einen anderen Grenzwert konvergiert bzw. sogar divergiert. Dieses Problem sollte bei Reihen mit ausschließlich positiven oder ausschließlich negativen Gliedern nicht auftreten.

\begin{theorem*}[Umordungssatz für nichtnegative Reihen]
Sei $\sum _{k=1}^{\infty }a_{k}$ eine konvergente Reihe mit $a_{k}\geq 0$ für alle $k\in \mathbb {N} $. Dann konvergiert auch jede Umordnung dieser Reihe gegen denselben Grenzwert.

\end{theorem*}

\begin{proof*}[Umordungssatz für nichtnegative Reihen]
\proofstep{1. Schritt:}
 Umgeordnete Reihe konvergiert:\begin{indentblock}
Da $\sum _{k=1}^{\infty }a_{k}$ konvergiert, ist die Partialsummenfolge $(S_{n})_{n\in \mathbb {N} }=\left(\sum _{k=1}^{n}a_{k}\right)_{n\in \mathbb {N} }$ beschränkt. Sei weiter $\sum _{k=1}^{\infty }a_{\sigma (k)}$ eine beliebige Umordung der Reihe und bezeichne $T_{m}$ deren Partialsummen. Setzen wir $n=\max\{\sigma (1),\sigma (2),\ldots ,\sigma (m)\}$, folgt $T_{m}\leq S_{n}$ für alle $m\in \mathbb {N} $. Damit ist aber auch die Partialsummenfolge $(T_{m})_{m\in \mathbb {N} }$ beschränkt, und die Umordnung konvergiert.

\end{indentblock}

\proofstep{2. Schritt:}
 Umgeordnete Reihe konvergiert gegen denselben Grenzwert:\begin{indentblock}
Ist $S$ der Grenzwert der ursprünglichen Reihe und $S'$ der Grenzwert der umgeordneten Reihe, so folgt nach dem 1. Schritt $S'\leq S$. Die ursprüngliche Reihe ist ebenfalls eine Umordnung der umgeordneten Reihe, denn mit der Bijektion $\mu =\sigma ^{-1}$ gilt $\sum _{k=1}^{n}a_{k}=\sum _{k=1}^{n}a_{\sigma ^{-1}(\sigma (k))}=\sum _{k=1}^{n}a_{\mu (\sigma (k))}$. Mit derselben Argumentation wie im 1. Schritt gibt es ein $m'\in \mathbb {N} $ mit $S_{n}\leq T_{m'}$ für alle $n\in \mathbb {N} $. Damit gilt $S\leq S'$. Also sind die beiden Grenzwerte identisch.

\end{indentblock}

\end{proof*}

\begin{hint*}
Analog konvergiert auch jede Umordnung einer konvergenten Reihe mit nichtpositiven Gliedern gegen denselben Grenzwert wie die ursprüngliche Reihe.

\end{hint*}

\section{Umordnung absolut konvergenter Reihen}

Die Frage ist nun, ob wir die Voraussetzungen für unseren Umordnungssatz noch verallgemeinern können. D.h. gibt es auch konvergente Reihen mit negativen Gliedern (beispielsweise alternierende), die beliebig umgeordnet werden können und dabei immer gegen denselben Grenzwert konvergieren? Die Antwort ist ja! Betrachten wir hierzu das Beispiel der alternierenden Reihe $\sum _{k=1}^{\infty }{\tfrac {(-1)^{k+1}}{k^{2}}}$. Die entscheidende Eigenschaft dieser Reihe ist, dass sie absolut konvergiert, da $\sum _{k=1}^{\infty }{\tfrac {1}{k^{2}}}$ konvergiert. Nach dem Umordnungssatz für Reihen mit nichtnegativen Gliedern aus dem vorherigen Abschnitt konvergiert damit auch jede Umordnung $\sum _{k=1}^{\infty }{\tfrac {1}{\sigma (k)^{2}}}$ gegen denselben Grenzwert. Da \href{https://de.wikibooks.org/wiki/Mathe\_für\_Nicht-Freaks:\_Absolute\_Konvergenz\_einer\_Reihe\#Anker:absolute\_Konvergenz\_und\_Konvergenz}
{jede absolut konvergente Reihe konvergiert}, konvergiert auch jede Umordnung der ursprünglichen Reihe $\sum _{k=1}^{\infty }{\tfrac {(-1)^{\sigma (k+1)}}{\sigma (k)^{2}}}$.

Wir müssen nun nur noch zeigen, dass jede dieser Umordnungen gegen denselben Grenzwert wie die ursprüngliche Reihe konvergiert. Dazu benutzen wir das \href{https://de.wikibooks.org/wiki/Mathe\_für\_Nicht-Freaks:\_Absolute\_Konvergenz\_einer\_Reihe\#Anker:Kriterium\_absolute\_Konvergenz}
{charakteristische Kriterium für absolute Konvergenz}. Dieses besagt, dass eine Reihe $\sum _{k=1}^{\infty }a_{k}$ genau dann absolut konvergiert, wenn die Reihen ihrer nichtnegativen Glieder $\sum _{k=1}^{\infty }a_{k}^{+}$ und ihrer nichtpositiven Glieder $\sum _{k=1}^{\infty }a_{k}^{-}$ konvergieren. Da jede Umordnung $\sum _{k=1}^{\infty }{\tfrac {(-1)^{\sigma (k+1)}}{\sigma (k)^{2}}}=\sum _{k=1}^{\infty }a_{\sigma (k)}$ absolut konvergiert, konvergieren auch die Reihen $\sum _{k=1}^{\infty }a_{\sigma (k)}^{+}$ und $\sum _{k=1}^{\infty }a_{\sigma (k)}^{-}$. Weiter gilt

\begin{align*}
\sum _{k=1}^{\infty }a_{\sigma (k)}^{+}=\sum _{k=1}^{\infty }a_{k}^{+}
\end{align*}

und

\begin{align*}
\sum _{k=1}^{\infty }\underbrace {a_{\sigma (k)}^{-}} _{\leq 0}=-\left(-\sum _{k=1}^{\infty }a_{\sigma (k)}^{-}\right)=-\sum _{k=1}^{\infty }\underbrace {-a_{\sigma (k)}^{-}} _{\geq 0}=-\sum _{k=1}^{\infty }-a_{k}^{-}=\sum _{k=1}^{\infty }a_{k}^{-}
\end{align*}

Damit folgt aber nun

\begin{align*}
\sum _{k=1}^{\infty }a_{\sigma (k)}&=\sum _{k=1}^{\infty }(a_{\sigma (k)}^{+}+a_{\sigma (k)}^{-})=\sum _{k=1}^{\infty }a_{\sigma (k)}^{+}+\sum _{k=1}^{\infty }a_{\sigma (k)}^{-}\\[0.3em]&=\sum _{k=1}^{\infty }a_{k}^{+}+\sum _{k=1}^{\infty }a_{k}^{-}=\sum _{k=1}^{\infty }(a_{k}^{+}+a_{k}^{-})=\sum _{k=1}^{\infty }a_{k}
\end{align*}

Also konvergiert die umgeordnete Reihe tatsächlich gegen denselben Grenzwert.

\part{Kriterien für Konvergenz von Reihen}

\addxcontentsline{lof}{part}[\arabic{part}]{Kriterien für Konvergenz von Reihen}\begin{authors}
Who2010, Stephan Kulla, Evalain, Autorenkollektiv „Auswahlaxiom“ (Charlotte Dietze, Matthias Paulsen, Anne Reif), Matthias Greger, Phoible, 0-Brane, Flauschi, Jenny Kilian, Juetho, KaiJay, Werner Fröhlich, Fabiangabel\end{authors}

\chapter{Trivialkriterium}

In diesem Kapitel lernst du ein einfaches und nützliches Kriterium zur Divergenz einer Reihe kennen: das \emph{Trivialkriterium}, welches auch \emph{Nullfolgenkriterium} oder \emph{Divergenzkriterium} genannt wird. Es besagt, dass jede Reihe $\sum _{k=1}^{\infty }a_{k}$, bei der $(a_{k})_{k\in \mathbb {N} }$ keine Nullfolge ist, divergieren muss. Dies kannst du auch so formulieren: Bei jeder konvergenten Reihe $\sum _{k=1}^{\infty }a_{k}$ muss zwangsweise $\lim _{k\to \infty }a_{k}=0$ sein.

\section{Trivialkriterium}

Das Trivialkriterium lautet:

\begin{theorem*}[Trivialkriterium]
Wenn eine Reihe $\sum _{k=1}^{\infty }a_{k}$ konvergiert, dann ist $(a_{k})_{k\in \mathbb {N} }$ eine Nullfolge. Dies bedeutet, dass jede Reihe $\sum _{k=1}^{\infty }a_{k}$ divergieren muss, falls $(a_{k})_{k\in \mathbb {N} }$ divergiert oder $\lim _{k\to \infty }a_{k}\neq 0$ ist.

\end{theorem*}

\begin{example*}[Trivialkriterium]
Die Reihe $\sum _{k=1}^{\infty }(-1)^{k}$ divergiert, weil die Folge $\left((-1)^{k}\right)_{k\in \mathbb {N} }$ divergent ist (sie besitzt mit $1$ und $-1$ mehr als einen Häufungspunkt und kann deshalb nicht konvergieren).

Auch die Reihe $\sum _{k=1}^{\infty }{\sqrt[{k}]{4}}$ divergiert, weil $\lim _{k\to \infty }{\sqrt[{k}]{4}}=1\neq 0$ ist.

\end{example*}

\begin{warning*}
Dass $(a_{k})_{k\in \mathbb {N} }$ eine Nullfolge ist, ist nur ein \emph{notwendiges}, aber kein hinreichendes Kriterium für Konvergenz der Reihe $\sum _{k=1}^{\infty }a_{k}$.

Das bedeutet: Aus der Tatsache, dass $\lim _{k\to \infty }a_{k}=0$ kann \emph{nicht} gefolgert werden, dass $\sum _{k=1}^{\infty }a_{k}$ konvergiert. Beispielsweise ist die harmonische Reihe $\sum _{k=1}^{\infty }{\frac {1}{k}}$ divergent, auch wenn $\lim _{k\to \infty }{\tfrac {1}{k}}=0$ ist.

\end{warning*}

\section{Beweis über Teleskopsumme}

\begin{proof*}[Trivialkriterium über Teleskopsumme]
Wir nehmen an, dass $\sum _{k=1}^{\infty }a_{k}$ eine konvergente Reihe ist. Nun wollen wir zeigen, dass $(a_{n})_{n\in \mathbb {N} }$ eine Nullfolge ist.

Die Koeffizientenfolge $(a_{n})_{n\in \mathbb {N} }$ lässt sich als Differenz der beiden Partialsummen $S_{n}=\sum _{k=1}^{n}a_{k}$ und $S_{n-1}=\sum _{k=1}^{n-1}a_{k}$ schreiben:

\begin{align*}
a_{n}&=a_{n}+0\\&=a_{n}+a_{n-1}+a_{n-2}+\ldots +a_{2}+a_{1}-a_{n-1}-a_{n-2}-\ldots -a_{2}-a_{1}\\&={\color {Teal}(a_{n}+a_{n-1}+a_{n-2}+\ldots +a_{2}+a_{1})}-{\color {Indigo}(a_{n-1}+a_{n-2}+\ldots +a_{2}+a_{1})}\\&={\color {Teal}\sum _{k=1}^{n}a_{k}}-{\color {Indigo}\sum _{k=1}^{n-1}a_{k}}=S_{n}-S_{n-1}
\end{align*}

Nun gilt, dass die Reihe $\sum _{k=1}^{\infty }a_{k}$ gegen einen Grenzwert $s\in \mathbb {R} $ konvergiert. Also ist

\begin{align*}
\lim _{n\to \infty }S_{n}=\lim _{n\to \infty }\sum _{k=1}^{n}a_{k}=s
\end{align*}

Ebenso gilt $\lim _{n\to \infty }S_{n-1}=s$, da sich der Grenzwert bei Verschiebung der Folgenglieder nicht ändert. Zusammen erhalten wir nun:

\begin{align*}
&\lim _{n\to \infty }a_{n}\\[0.3em]&\quad {\color {OliveGreen}\left\downarrow \ a_{n}=S_{n}-S_{n-1}{\text{ (siehe oben)}}\right.}\\[0.3em]=&\lim _{n\to \infty }(S_{n}-S_{n-1})\\[0.3em]&\quad {\color {OliveGreen}\left\downarrow \ \lim _{n\to \infty }(a_{n}-b_{n})=\lim _{n\to \infty }a_{n}-\lim _{n\to \infty }b_{n}\right.}\\[0.3em]=&\lim _{n\to \infty }S_{n}-\lim _{n\to \infty }S_{n-1}=s-s=0
\end{align*}

Also ist $(a_{n})_{n\in \mathbb {N} }$ eine Nullfolge.

\end{proof*}

\section{Beispielaufgabe}

\begin{exercise*}
Beweise, dass $\sum _{n=1}^{\infty }{\frac {n}{n+1}}$ divergiert.

\end{exercise*}

\begin{solution*}
Es ist

\begin{align*}
\lim _{n\to \infty }{\frac {n}{n+1}}&=\lim _{n\to \infty }{\frac {1}{1+{\frac {1}{n}}}}\\[0.5em]&\quad {\color {OliveGreen}\left\downarrow \ {\text{Grenzwertsätze}}\right.}\\[0.5em]&={\frac {\lim _{n\to \infty }1}{\lim _{n\to \infty }1+\lim _{n\to \infty }{\frac {1}{n}}}}={\frac {1}{1+0}}=1
\end{align*}

Dies zeigt, dass die Folge $a_{n}={\tfrac {n}{n+1}}$ keine Nullfolge ist. Damit divergiert die Reihe $\sum _{n=1}^{\infty }{\frac {n}{n+1}}$ nach dem Trivialkriterium.

\end{solution*}

\chapter{Majoranten- und Minorantenkriterium}

In diesem Kapitel wirst du mit dem Majoranten- und Minorantenkriterium ein wichtiges Konvergenzkriterium kennenlernen. Mit diesem kannst du das Konvergenzverhalten einer Reihe auf das Konvergenzverhalten einer anderen Reihe zurückführen. So ist es möglich, eine Reihe „zu vereinfachen“. Mit diesen Kriterien kann nämlich eine Reihe so geschickt nach oben oder nach unten abgeschätzt werden, dass ein Beweis zum Konvergenzverhalten möglich wird.

Außerdem kann mit dem Majoranten- und Minorantenkriterium das Quotienten- sowie das Wurzelkriterium für Reihen bewiesen werden, welche beide in Aufgaben zur Reihenkonvergenz sehr nützlich sind.

\section{Majorantenkriterium}

Kommen wir zum Majorantenkriterium. Dieses lautet folgendermaßen:

\begin{theorem*}[Majorantenkriterium]
Sei $\sum _{k=1}^{\infty }a_{k}$ eine Reihe. Wenn es eine konvergente Reihe $\sum _{k=1}^{\infty }c_{k}$ mit $|a_{k}|\leq c_{k}$ für alle $k\in \mathbb {N} $ gibt, dann konvergiert $\sum _{k=1}^{\infty }a_{k}$ absolut.

\end{theorem*}

\begin{explanation*}[Majorantenkriterium]
 Beachte, dass aus der Ungleichung $|a_{k}|\leq c_{k}$ automatisch folgt, dass $c_{k}\geq 0$ ist, denn $c_{k}$ ist größer gleich der nicht negativen Zahl $|a_{k}|$.

\end{explanation*}

\begin{proof*}[Majorantenkriterium]
Wenn $\sum _{k=1}^{\infty }c_{k}$ konvergiert, dann ist deren Partialsummenfolge nach oben beschränkt . Wegen $|a_{k}|\leq c_{k}$ für alle $k\in \mathbb {N} $ ist dann auch die Partialsummenfolge zu $\sum _{k=1}^{\infty }|a_{k}|$ nach oben beschränkt. Es gilt nämlich

\begin{align*}
\sum _{k=1}^{n}|a_{k}|\leq \sum _{k=1}^{n}c_{k}\leq \sum _{k=1}^{\infty }c_{k}<\infty 
\end{align*}

wegen $0\leq |a_{k}|\leq c_{k}$ für alle $k\in \mathbb {N} $

Nun wächst die Partialsummenfolge von $\sum _{k=1}^{\infty }|a_{k}|$ monoton wegen $|a_{k}|\geq 0$. Also muss $\sum _{k=1}^{\infty }|a_{k}|$ konvergieren. 

\end{proof*}

\begin{hint*}
Es reicht beim Majorantenkriterium aus, wenn es ein $N\in \mathbb {N} $ gibt, so dass $|a_{k}|\leq c_{k}$ für alle $k\geq N$ gilt.

\end{hint*}

\begin{hint*}
Wie in der Einleitung schon angesprochen, möchten wir bei der Anwendung des Majorantenkriteriums eine möglichst einfach strukturierte Reihe als Majorante wählen, von der wir wissen, dass sie konvergiert. Häufig kann die konvergente Reihe $\sum _{k=1}^{\infty }{\tfrac {1}{k^{2}}}$ als Majorante gewählt werden. Ebenso kann jede andere verallgemeinerte harmonische Reihe $\sum _{k=1}^{\infty }{\tfrac {1}{k^{\alpha }}}$ für $\alpha >1$ in Betracht gezogen werden. Eine andere Möglichkeit ist es, die konvergente geometrische Reihe $\sum _{k=0}^{\infty }q^{k}$ für $|q|<1$, also etwa $\sum _{k=0}^{\infty }\left({\tfrac {1}{2}}\right)^{k}$, auszuprobieren.

\end{hint*}

\section{Minorantenkriterium}

Ähnlich zum Majorantenkriterium ist das Minorantenkriterium. Jedoch kann mit diesem Kriterium die Divergenz und nicht die Konvergenz einer Reihe bewiesen werden.

\begin{theorem*}[Minorantenkriterium]
Sei $\sum _{k=1}^{\infty }a_{k}$ eine Reihe mit $a_{k}\geq 0$ für alle $k\in \mathbb {N} $. Wenn es eine divergente Reihe $\sum _{k=1}^{\infty }c_{k}$ mit $a_{k}\geq c_{k}\geq 0$ für alle $k\in \mathbb {N} $ gibt, dann divergiert auch die Reihe $\sum _{k=1}^{\infty }a_{k}$.

\end{theorem*}

\begin{proof*}[Minorantenkriterium]
Wegen $c_{k}\geq 0$ wächst die Partialsummenfolge von $\sum _{k=1}^{\infty }c_{k}$ monoton. Da nach Prämisse diese Reihe divergiert, muss diese unbeschränkt sein. Wegen $c_{k}\leq a_{k}$ für alle $k\in \mathbb {N} $ ist auch stets $\sum _{k=1}^{n}c_{k}\leq \sum _{k=1}^{n}a_{k}$ für alle $n\in \mathbb {N} $ und damit muss auch die Partialsummenfolge von $\sum _{k=1}^{\infty }a_{k}$ unbeschränkt sein. Daraus folgt, dass $\sum _{k=1}^{\infty }a_{k}$ divergiert (jede unbeschränkte Folge muss divergieren).

\end{proof*}

\begin{hint*}
Analog zum Majorantenkriterium reicht es auch beim Minorantenkriterium aus, wenn die Voraussetzung $a_{k}\geq c_{k}\geq 0$ für alle $k\geq N$, für ein (festes) $N\in \mathbb {N} $, erfüllt ist.

\end{hint*}

\begin{hint*}
Beim Minorantenkriterium bietet sich häufig die divergente harmonische Reihe $\sum _{k=1}^{\infty }{\tfrac {1}{k}}$ als Minorante an. Ansonsten kommt aber auch jede der Reihen $\sum _{k=1}^{\infty }{\tfrac {1}{k^{\alpha }}}$ für $\alpha <1$ , also etwa $\sum _{k=1}^{\infty }{\tfrac {1}{\sqrt {k}}}$, als Minorante in Frage. Außerdem ist jede geometrische Reihe $\sum _{k=1}^{\infty }q^{k}$ mit $q\geq 1$ als Minorante geeignet.

\end{hint*}

\begin{warning*}
Beim Minorantenkriterium ist die zusätzliche Bedingung $a_{k}\geq 0$ notwendig! Gilt „nur“ die zum Majorantenkriterium analoge Voraussetzung $|a_{k}|\geq c_{k}$ und $a_{k}\in \mathbb {R} $ beliebig, so folgt aus der Divergenz von $\sum _{k=1}^{\infty }c_{k}$ \emph{nicht} die Divergenz von $\sum _{k=1}^{\infty }a_{k}$. Es folgt lediglich, dass $\sum _{k=1}^{\infty }a_{k}$ nicht \emph{absolut} konvergiert. Als Beispiel betrachten wir die Reihen $\sum _{k=1}^{\infty }{\tfrac {(-1)^{k}}{k}}$ und $\sum _{k=1}^{\infty }{\tfrac {1}{k}}$ mit $a_{k}={\tfrac {(-1)^{k}}{k}}$ und $c_{k}={\tfrac {1}{k}}$. Es gilt ${\tfrac {1}{k}}\leq \left|{\tfrac {(-1)^{k}}{k}}\right|={\tfrac {1}{k}}$, und die harmonische Reihe $\sum _{k=1}^{\infty }{\tfrac {1}{k}}$ divergiert. Jedoch kann man zeigen, dass $\sum _{k=1}^{\infty }{\tfrac {(-1)^{k}}{k}}$ nach dem \href{https://de.wikibooks.org/wiki/Mathe\_für\_Nicht-Freaks:\_Leibniz-Kriterium}
{Leibniz-Kriterium} konvergiert.

\end{warning*}

\section{Beispiele und Aufgaben}

\subsection{Beispiel und Aufgabe zum Majorantenkriterium}

\begin{exercise*}[Majorantenkriterium]
Untersuche, ob die Reihe $\sum _{k=1}^{\infty }\left({\tfrac {k+1}{k^{2}+3k}}\right)^{2}$ konvergiert.

\end{exercise*}

\begin{solutionprocess*}[Majorantenkriterium]
Wenn man noch wenig Erfahrung in solchen Konvergenzbeweisen hat, lässt sich nur schwer erraten, ob die Reihe konvergiert oder divergiert. Wenn du dir aber den Bruch ${\tfrac {k+1}{k^{2}+3k}}$ anschaust, dann erkennst du, dass der Zähler ein Polynom ersten und der Nenner ein Polynom zweiten Grades ist. Der gesamte Bruch fällt also mit einer Konvergenzgeschwindigkeit, die in der Größenordnung von ${\tfrac {1}{k}}$ ist. Nun wird dieser Bruch im Summanden der Reihe quadriert. Damit fällt $\left({\tfrac {k+1}{k^{2}+3k}}\right)^{2}$ mit der Konvergenzgeschwindigkeit wie ${\tfrac {1}{k^{2}}}$. Weil $\sum _{k=1}^{\infty }{\frac {1}{k^{2}}}$ konvergiert, sollte auch die Reihe $\sum _{k=1}^{\infty }\left({\frac {k+1}{k^{2}+3k}}\right)^{2}$ konvergieren.

Dies lässt sich so zwar kaum beweisen, gibt uns aber einen Anhaltspunkt. Mit dem Majorantenkriterium lässt sich nun die Konvergenz von $\sum _{k=1}^{\infty }\left({\frac {k+1}{k^{2}+3k}}\right)^{2}$ auf die Konvergenz von $\sum _{k=1}^{\infty }{\frac {1}{k^{2}}}$ zurückführen. Hierzu muss man geschickt abschätzen:

\begin{align*}
{\frac {k+1}{k^{2}+3k}}&\leq {\frac {k+1}{k^{2}}}\leq {\frac {k+k}{k^{2}}}\\[0.5em]&={\frac {2k}{k^{2}}}=2\cdot {\frac {1}{k}}
\end{align*}

Das Abschätzen erfolgte hier nach einem gewissen Schema: Summanden, die den Term verkleinern, wurden gestrichen. Dann wurden Summanden, die nicht gestrichen werden konnten, so abgeschätzt, dass sie mit anderen Summanden zusammengefasst werden können. Insgesamt erhalten wir so:

\begin{align*}
\left({\frac {k+1}{k^{2}+3k}}\right)^{2}\leq \left(2\cdot {\frac {1}{k}}\right)^{2}=4\cdot {\frac {1}{k^{2}}}
\end{align*}

Nun kann das Majorantenkriterium angewandt werden. Die Reihe $\sum _{k=1}^{\infty }4\cdot {\frac {1}{k^{2}}}$ konvergiert und damit auch die Reihe $\sum _{k=1}^{\infty }\left({\frac {k+1}{k^{2}+3k}}\right)^{2}$.

\end{solutionprocess*}

\begin{solution*}[Majorantenkriterium]
Es ist

\begin{align*}
\left({\frac {k+1}{k^{2}+3k}}\right)^{2}&\leq \left({\frac {k+1}{k^{2}}}\right)^{2}\leq \left({\frac {k+k}{k^{2}}}\right)^{2}=\left({\frac {2k}{k^{2}}}\right)^{2}\\[0.5em]&=\left(2\cdot {\frac {1}{k}}\right)^{2}=4\cdot {\frac {1}{k^{2}}}
\end{align*}

und

\begin{align*}
\sum _{k=1}^{\infty }4\cdot {\frac {1}{k^{2}}}=4\cdot \sum _{k=1}^{\infty }{\frac {1}{k^{2}}}<\infty 
\end{align*}

Damit konvergiert die Reihe nach dem Majorantenkriterium.

\end{solution*}
\clearpage
\subsection{Beispiel und Aufgabe zum Minorantenkriterium}

\begin{exercise*}[Minorantenkriterium]
Man untersuche, ob die Reihe $\sum _{k=1}^{\infty }{\tfrac {1}{\sqrt {k(k+2)}}}$ konvergiert oder divergiert.

\end{exercise*}

\begin{solutionprocess*}[Minorantenkriterium]
Hier lohnt sich ein Blick auf die Konvergenzgeschwindigkeit, mit der die Summanden gegen Null konvergieren. Das Produkt $k(k+2)$ konvergiert wie $k^{2}$ gegen Unendlich. Damit konvergiert ${\tfrac {1}{k(k+2)}}$ wie ${\tfrac {1}{k^{2}}}$ gegen 0. Da im Summanden noch die Wurzel gezogen wird, ist insgesamt die Konvergenzgeschwindigkeit von ${\tfrac {1}{\sqrt {k(k+2)}}}$ gegen 0 wie bei der Folge ${\tfrac {1}{k}}$. Da die harmonische Reihe $\sum _{k=1}^{\infty }{\tfrac {1}{k}}$ bekanntermaßen divergiert, sollte auch $\sum _{k=1}^{\infty }{\tfrac {1}{\sqrt {k(k+2)}}}$ divergieren.

Nachdem wir eine Vermutung über das Konvergenzverhalten aufgestellt haben, müssen wir diese Vermutung noch durch einen Beweis untermauern. Hier können wir das Minorantenkriterium benutzen, wobei wir zunächst geschickt nach unten abschätzen müssen:

\begin{align*}
{\frac {1}{\sqrt {k(k+2)}}}&={\frac {1}{\sqrt {k^{2}+2k}}}\geq {\frac {1}{\sqrt {k^{2}+2k^{2}}}}\\[0.3em]&={\frac {1}{\sqrt {3k^{2}}}}={\frac {1}{{\sqrt {3}}\cdot k}}
\end{align*}

Hier folgten wir einem gewissen Schema: Summanden, die den Term vergrößern, werden gestrichen. Danach haben wir die Summanden, die nicht gestrichen werden konnten, so abgeschätzt, dass sie mit anderen Summanden zusammengefasst werden konnten. Nun divergiert $\sum _{k=1}^{\infty }{\tfrac {1}{{\sqrt {3}}\cdot k}}$. Also muss nach dem Minorantenkriterium auch $\sum _{k=1}^{\infty }{\tfrac {1}{\sqrt {k(k+2)}}}$ divergieren.

\end{solutionprocess*}

\begin{proof*}[Minorantenkriterium]
Es ist

\begin{align*}
{\frac {1}{\sqrt {k(k+2)}}}&={\frac {1}{\sqrt {k^{2}+2k}}}\geq {\frac {1}{\sqrt {k^{2}+2k^{2}}}}\\[0.5em]&={\frac {1}{\sqrt {3k^{2}}}}={\frac {1}{{\sqrt {3}}\cdot k}}={\frac {1}{\sqrt {3}}}\cdot {\frac {1}{k}}
\end{align*}

Außerdem divergiert $\sum _{k=1}^{\infty }{\tfrac {1}{\sqrt {3}}}\cdot {\tfrac {1}{k}}$. Damit muss $\sum _{k=1}^{\infty }{\tfrac {1}{\sqrt {k(k+2)}}}$ nach dem Minorantenkriterium divergieren.

\end{proof*}

\chapter{Wurzelkriterium}

Kommen wir nun zum Wurzelkriterium, welches ein mächtiges Kriterium ist, um Konvergenz oder Divergenz einer konkret gegebenen Reihe nachzuweisen. Es basiert auf dem Majorantenkriterium, wobei hier die Konvergenz einer Reihe auf die Konvergenz der geometrischen Reihe $\sum _{k=1}^{\infty }q^{k}$ mit $0\leq q<1$ zurückgeführt wird.



\section{Herleitung}

\subsection{Wiederholung}

Wir haben bereits das Majorantenkriterium kennengelernt. Es besagt, dass eine Reihe $\sum _{k=1}^{\infty }a_{k}$ mit $a_{k}\geq 0$ konvergiert, wenn es eine konvergente Reihe $\sum _{k=1}^{\infty }b_{k}$ mit $a_{k}\leq b_{k}$ gibt.

Außerdem wissen wir, dass jede geometrische Reihe $\sum _{k=1}^{\infty }q^{k}$ mit $q\in [0,1)$ konvergiert.

\subsection{Erste Herleitung}

Sei $\sum _{k=1}^{\infty }a_{k}$ eine Reihe, deren Konvergenz wir mit Hilfe des Majorantenkriteriums nachweisen wollen, indem wir die Konvergenz der Reihe auf die Konvergenz der geometrischen Reihe zurückführen. Gehen wir zunächst davon aus, dass $a_{k}\geq 0$ für alle $k\in \mathbb {N} $ ist, weil dies eine Voraussetzung für das Majorantenkriterium ist. Um das Majorantenkriterium anwenden zu können, muss es ein $q\in [0,1)$ mit $a_{k}\leq q^{k}$ geben. Dann ist

\begin{align*}
\sum _{k=1}^{\infty }a_{k}&\leq \sum _{k=1}^{\infty }q^{k}=\sum _{k=0}^{\infty }q^{k}-q^{0}\\[0.3em]&={\frac {1}{1-q}}-1={\frac {q}{1-q}}<\infty 
\end{align*}

Die Reihe $\sum _{k=1}^{\infty }a_{k}$ konvergiert nach dem Majorantenkriterium. Die Ungleichung $a_{k}\leq q^{k}$ können wir umformen:

\begin{align*}
&&a_{k}&\leq q^{k}\\&\iff &{\sqrt[{k}]{a_{k}}}&\leq q
\end{align*}

Wenn es also ein $q$ mit $0\leq q<1$ gibt, so dass ${\sqrt[{k}]{a_{k}}}\leq q$ ist, dann ist $a_{k}\leq q^{k}$ und die Reihe $\sum _{k=1}^{\infty }a_{k}$ konvergiert.

\subsection{Umformulierung mit Limes Superior}

Für das Konvergenzverhalten ist der Wert von endlich vielen Summanden egal. Dementsprechend muss auch nicht ${\sqrt[{k}]{a_{k}}}\leq q$ für alle $k\in \mathbb {N} $ gelten, sondern nur für alle $k\in \mathbb {N} $ bis auf endlich viele Ausnahmen. Es muss also nur für fast alle $k\in \mathbb {N} $ die Ungleichung ${\sqrt[{k}]{a_{k}}}\leq q$ erfüllt sein.

Die Forderung, dass es ein $q\in [0,1)$ mit ${\sqrt[{k}]{a_{k}}}\leq q$ für fast alle $k\in \mathbb {N} $ gibt, können wir auch mit dem Limes Superior formulieren:

\begin{align*}
\exists {\tilde {q}}\in [0,1):\limsup _{k\to \infty }{\sqrt[{k}]{a_{k}}}\leq {\tilde {q}}
\end{align*}

Anders ausgedrückt:

\begin{align*}
\limsup _{k\to \infty }{\sqrt[{k}]{a_{k}}}<1
\end{align*}

Ist ${\sqrt[{k}]{a_{k}}}\leq q$ für fast alle $k$, dann ist die Folge $\left({\sqrt[{k}]{a_{k}}}\right)_{k\in \mathbb {N} }$ nach oben beschränkt und muss einen größten Häufungspunkt kleiner gleich $q$ besitzen. Dieser Häufungspunkt ist gleich $\limsup _{k\to \infty }{\sqrt[{k}]{a_{k}}}$ und es gilt $\limsup _{k\to \infty }{\sqrt[{k}]{a_{k}}}\leq q$.

Sei umgekehrt $\limsup _{k\to \infty }{\sqrt[{k}]{a_{k}}}\leq q$ für ein $q\in [0,1)$. Dann ist für alle $\epsilon >0$ die Ungleichung ${\sqrt[{k}]{a_{k}}}\leq q+\epsilon $ für fast alle $k\in \mathbb {N} $ erfüllt. Wegen $q<1$ gibt es ein $\epsilon >0$, das klein genug ist, damit auch $q+\epsilon <1$ ist. Setzen wir ${\tilde {q}}=q+\epsilon $. Es ist ${\tilde {q}}<1$ und die Ungleichung ${\sqrt[{k}]{a_{k}}}\leq {\tilde {q}}$ ist für fast alle $k$ erfüllt.

Zusammenfassung: Anstelle von ${\sqrt[{k}]{a_{k}}}\leq q$ für fast alle $k\in \mathbb {N} $ reicht auch $\limsup _{k\to \infty }{\sqrt[{k}]{a_{k}}}<1$, um die Konvergenz der Reihe zu zeigen.

\subsection{Die Sache mit der absoluten Konvergenz}

Was passiert, wenn nicht alle $a_{k}\geq 0$ sind? Dann können wir obige Argumentation nicht anwenden. Sie ergibt ohnehin keinen Sinn, weil ${\sqrt[{k}]{a_{k}}}$ für ein gerades $k$ und $a_{k}<0$ nicht definiert ist. Jedoch können wir die obige Argumentation auf die Reihe $\sum _{k=1}^{\infty }|a_{k}|$ anwenden, um die absolute Konvergenz der Reihe zu zeigen (aus der ja die normale Konvergenz folgt). Für Reihen mit $a_{k}\geq 0$ für alle $k$ ist $|a_{k}|=a_{k}$. Somit ändert sich für diese Reihen nichts, wenn man anstelle von $a_{k}$ den Betrag $|a_{k}|$ betrachtet. Wir können also zusammenfassen:

\begin{importantparagraph*}
Ist $\limsup _{k\to \infty }{\sqrt[{k}]{|a_{k}|}}<1$, dann konvergiert die Reihe $\sum _{k=1}^{\infty }a_{k}$ absolut.

\end{importantparagraph*}

\subsection{Wurzelkriterium für Divergenz}

Wir haben bisher nur das Wurzelkriterium für die Konvergenz einer Reihe kennengelernt. Gibt es auch ein Wurzelkriterium für die Divergenz einer Reihe? Stellen wir uns vor, dass $\limsup _{k\to \infty }{\sqrt[{k}]{|a_{k}|}}>1$ ist. Dann ist für fast alle $k\in \mathbb {N} $ die Ungleichung ${\sqrt[{k}]{|a_{k}|}}\geq 1$ erfüllt. Daraus folgt $|a_{k}|\geq 1^{k}=1$, womit $\left(|a_{k}|\right)_{k\in \mathbb {N} }$ keine Nullfolge ist. Damit kann aber auch $\left(a_{k}\right)_{k\in \mathbb {N} }$ keine Nullfolge sein. Aus dem Trivialkriterium folgt dann, dass $\sum _{k=1}^{\infty }a_{k}$ divergiert. Wir können diesen Fall verallgemeinern, indem wir anstelle von $\limsup _{k\to \infty }{\sqrt[{k}]{|a_{k}|}}>1$ die Ungleichung ${\sqrt[{k}]{|a_{k}|}}\geq 1$ für fast alle $k\in \mathbb {N} $ fordern.

\section{Definition}

Das Wurzelkriterium lautet:

\begin{theorem*}[Wurzelkriterium]
Sei $\sum _{k=1}^{\infty }a_{k}$ eine Reihe. Wenn $\limsup _{k\to \infty }{\sqrt[{k}]{|a_{k}|}}<1$ ist, dann konvergiert die Reihe absolut. Ist $\limsup _{k\to \infty }{\sqrt[{k}]{|a_{k}|}}>1$, dann divergiert die Reihe. Auch wenn ${\sqrt[{k}]{|a_{k}|}}\geq 1$ für unendlich viele $k\in \mathbb {N} $ ist, divergiert die Reihe.

\end{theorem*}

\begin{explanation*}[Wurzelkriterium]
Den Satz haben wir in der obigen Herleitung bereits bewiesen. Wir fassen den Beweis noch einmal kurz zusammen:

\end{explanation*}

\begin{proof*}[Wurzelkriterium]
\proofstep{Beweisschritt:}
 Aus $\limsup _{k\to \infty }{\sqrt[{k}]{|a_{k}|}}<1$ folgt die absolute Konvergenz von $\sum _{k=1}^{\infty }a_{k}$.\begin{indentblock}
Sei $\sum _{k=1}^{\infty }a_{k}$ eine Reihe. Ist $\limsup _{k\to \infty }{\sqrt[{k}]{|a_{k}|}}<1$, dann ist $\limsup _{k\to \infty }{\sqrt[{k}]{|a_{k}|}}\leq q$ für ein $0\leq q<1$ (man kann zum Beispiel $q=\limsup _{k\to \infty }{\sqrt[{k}]{|a_{k}|}}$ wählen).

Wähle nun $\epsilon >0$ so klein, dass $q+\epsilon <1$ ist. Aus der Definition des Limes Superior folgt, dass für fast alle $k\in \mathbb {N} $ die Ungleichung ${\sqrt[{k}]{|a_{k}|}}\leq q+\epsilon $ erfüllt ist. Daraus folgt $|a_{k}|\leq (q+\epsilon )^{k}$ für fast alle $k$. Weil die Reihe $\sum _{k=1}^{\infty }(q+\epsilon )^{k}$ konvergiert (dies ist eine geometrische Reihe mit $q+\epsilon <1$), konvergiert die Reihe $\sum _{k=1}^{\infty }|a_{k}|$ nach dem Majorantenkriterium. Also konvergiert die Reihe $\sum _{k=1}^{\infty }a_{k}$ absolut.

\end{indentblock}

\proofstep{Beweisschritt:}
 Aus $\limsup _{k\to \infty }{\sqrt[{k}]{|a_{k}|}}>1$ oder ${\sqrt[{k}]{|a_{k}|}}\geq 1$ für unendlich viele $k$ folgt die Divergenz von $\sum _{k=1}^{\infty }a_{k}$.\begin{indentblock}
Sei $\limsup _{k\to \infty }{\sqrt[{k}]{|a_{k}|}}>1$ bzw. ${\sqrt[{k}]{|a_{k}|}}\geq 1$ für unendlich viele $k$. Dann ist $|a_{k}|\geq 1^{k}=1$ für unendlich viele $k$. Deshalb kann $\left(|a_{k}|\right)_{k\in \mathbb {N} }$ keine Nullfolge sein. Damit kann aber auch $\left(a_{k}\right)_{k\in \mathbb {N} }$ keine Nullfolge sein. Also divergiert $\sum _{k=1}^{\infty }a_{k}$ nach dem Trivialkriterium.

\end{indentblock}

\end{proof*}

\begin{hint*}
Konvergiert $\left({\sqrt[{k}]{|a_{k}|}}\right)_{k\in \mathbb {N} }$, dann ist $\lim _{k\to \infty }{\sqrt[{k}]{|a_{k}|}}=\limsup _{k\to \infty }{\sqrt[{k}]{|a_{k}|}}$. Man kann also auch $\lim _{k\to \infty }{\sqrt[{k}]{|a_{k}|}}$ betrachten, wenn dieser Limes existiert. Dies wird in den meisten Konvergenzbeweisen mit dem Wurzelkriterium auch getan.

\end{hint*}

\section{Grenzen des Wurzelkriteriums}

Im Fall $\limsup _{k\to \infty }{\sqrt[{k}]{|a_{k}|}}=1$ können wir nichts über Konvergenz bzw. Divergenz der Reihe sagen. Es gibt nämlich sowohl konvergente als auch divergente Reihen, die diese Gleichung erfüllen. Ein Beispiel ist die divergente harmonische Reihe $\sum _{k=1}^{\infty }{\frac {1}{k}}$. Es ist

\begin{align*}
\limsup _{k\to \infty }{\sqrt[{k}]{|a_{k}|}}&=\limsup _{k\to \infty }{\sqrt[{k}]{\left|{\frac {1}{k}}\right|}}\\[0.5em]&=\limsup _{k\to \infty }{\sqrt[{k}]{\frac {1}{k}}}\\[0.5em]&=\limsup _{k\to \infty }{\frac {1}{\sqrt[{k}]{k}}}\\[0.5em]&\,{\color {OliveGreen}\left\downarrow \ \lim _{k\to \infty }{\sqrt[{k}]{k}}=1\right.}\\[0.5em]&={\frac {1}{1}}=1
\end{align*}

Aber auch die konvergente Reihe $\sum _{k=1}^{\infty }{\frac {1}{k^{2}}}$ erfüllt diese Gleichung:

\begin{align*}
\limsup _{k\to \infty }{\sqrt[{k}]{|a_{k}|}}&=\limsup _{k\to \infty }{\sqrt[{k}]{\left|{\frac {1}{k^{2}}}\right|}}\\[0.5em]&=\limsup _{k\to \infty }{\sqrt[{k}]{\frac {1}{k^{2}}}}\\[0.5em]&=\limsup _{k\to \infty }{\frac {1}{\sqrt[{k}]{k^{2}}}}\\[0.5em]&=\limsup _{k\to \infty }\left({\frac {1}{\sqrt[{k}]{k}}}\right)^{2}\\[0.5em]&\,{\color {OliveGreen}\left\downarrow \ \lim _{k\to \infty }{\sqrt[{k}]{k}}=1\right.}\\[0.5em]&=\left({\frac {1}{1}}\right)^{2}=1
\end{align*}

Dies zeigt, dass wir aus $\limsup _{k\to \infty }{\sqrt[{k}]{|a_{k}|}}=1$ weder zeigen können, dass die Reihe konvergiert, noch dass sie divergiert. Wir müssen also in einem solchen Fall ein anderes Konvergenzkriterium verwenden!

\section{Vorgehen bei der Anwendung des Wurzelkriteriums}

Um das Wurzelkriterium auf eine Reihe $\sum _{k=1}^{\infty }a_{k}$ anzuwenden, können wir folgendermaßen vorgehen: Wir bilden ${\sqrt[{k}]{|a_{k}|}}$ und betrachten dessen Limes (bei Existenz des Limes) bzw. dessen Limes Superior.

\begin{enumerate}
\item Ist $\limsup _{k\to \infty }{\sqrt[{k}]{|a_{k}|}}<1$, dann konvergiert die Reihe absolut.
\item Ist $\limsup _{k\to \infty }{\sqrt[{k}]{|a_{k}|}}>1$, dann divergiert die Reihe.
\item Ist ${\sqrt[{k}]{|a_{k}|}}\geq 1$ für unendlich viele $k$, dann divergiert die Reihe.
\item Trifft keiner der drei Fälle zu, können wir nichts zum Konvergenzverhalten der Reihe aussagen.
\end{enumerate}

\section{Beispielaufgaben}

\begin{exercise*}
Konvergiert oder divergiert $\sum _{k=1}^{\infty }{\frac {k^{3}}{3^{k}}}$?

\end{exercise*}

\begin{solution*}
Berechnen wir den Grenzwert von ${\sqrt[{k}]{\left|{\tfrac {k^{3}}{3^{k}}}\right|}}$:

\begin{align*}
\limsup _{k\to \infty }{\sqrt[{k}]{\left|{\frac {k^{3}}{3^{k}}}\right|}}&=\limsup _{k\to \infty }{\sqrt[{k}]{\frac {k^{3}}{3^{k}}}}\\[0.5em]&=\limsup _{k\to \infty }{\frac {\sqrt[{k}]{k^{3}}}{\sqrt[{k}]{3^{k}}}}\\[0.5em]&=\limsup _{k\to \infty }{\frac {{\sqrt[{k}]{k}}^{3}}{3}}\\[0.5em]&\,{\color {OliveGreen}\left\downarrow \ \lim _{k\to \infty }{\sqrt[{k}]{k}}=1\right.}\\[0.5em]&={\frac {1^{3}}{3}}={\frac {1}{3}}<1
\end{align*}

Damit ist $\limsup _{k\to \infty }{\sqrt[{k}]{\left|{\tfrac {k^{3}}{3^{k}}}\right|}}<1$, womit die Reihe nach dem Wurzelkriterium konvergiert.

\end{solution*}

\begin{exercise*}
Konvergiert oder divergiert $\sum _{k=1}^{\infty }\left({\frac {4k+5}{2k+3}}\right)^{k}$?

\end{exercise*}

\begin{solution*}
Es ist

\begin{align*}
\limsup _{k\to \infty }{\sqrt[{k}]{\left({\frac {4k+5}{2k+3}}\right)^{k}}}&=\limsup _{k\to \infty }{\frac {4k+5}{2k+3}}\\[0.5em]&=\limsup _{k\to \infty }{\frac {4+{\frac {5}{k}}}{2+{\frac {3}{k}}}}\\[0.5em]&={\frac {4}{2}}=2>1
\end{align*}

Wegen $\limsup _{k\to \infty }{\sqrt[{k}]{\left({\frac {4k+5}{2k+3}}\right)^{k}}}>1$ divergiert die Reihe nach dem Wurzelkriterium.

\end{solution*}


\chapter{Quotientenkriterium}

Das \emph{Quotientenkriterium} erlaubt Konvergenz- und Divergenzbeweise bei vielen konkret gegebenen Reihen und wird deswegen häufig eingesetzt. Es ist zwar bei weniger Reihen einsetzbar als das Wurzelkriterium, jedoch sind Beweise mit dem Quotientenkriterium in der Regel einfacher zu führen als solche mit dem Wurzelkriterium.



\section{Herleitung}

\subsection{Erste Schritte}

Genau wie beim Wurzelkriterium wird beim Quotientenkriterium die Konvergenz einer Reihe über das Majorantenkriterium auf die Konvergenz einer geometrischen Reihe zurückgeführt. Sei also $\sum _{k=1}^{\infty }a_{k}$ eine gegebene Reihe mit $a_{k}\geq 0$ für alle $k\in \mathbb {N} $. Die Forderung, dass die Reihe nur nichtnegative Summanden besitzt, brauchen wir für das Majorantenkriterium. Wir wissen, dass die Reihe konvergiert, wenn es ein $q\in [0,1)$ mit $a_{k}\leq q^{k}$ für alle $k\in \mathbb {N} $ gibt. Dies folgt aus dem Majorantenkriterium und der Tatsache, dass die geometrische Reihe $\sum _{k=1}^{\infty }q^{k}$ für $0\leq q<1$ konvergiert.

Beim Wurzelkriterium wird die Ungleichung $a_{k}\leq q^{k}$ direkt zu ${\sqrt[{k}]{a_{k}}}\leq q$ umgeformt. Beim Quotientenkriterium wählt man ein rekursives Kriterium, das $a_{k}\leq q^{k}$ impliziert. Zunächst wissen wir, dass $a_{1}\leq q$ sein muss. Im Rekursionsschritt brauchen wir eine Bedingung, mit der man aus der Ungleichung $a_{k}\leq q^{k}$ die Ungleichung $a_{k+1}\leq q^{k+1}$ schließen kann. Gehen wir also davon aus, dass wir $a_{k}\leq q^{k}$ bereits bewiesen haben. Es gilt dann (wenn wir davon ausgehen, dass $a_{k}\neq 0$ ist):

\begin{align*}
a_{k+1}={\frac {a_{k+1}}{a_{k}}}\cdot a_{k}\leq {\frac {a_{k+1}}{a_{k}}}\cdot q^{k}
\end{align*}

Um $a_{k+1}\leq q^{k+1}$ zu beweisen, können wir alternativ mit der obigen Umformung auch ${\tfrac {a_{k+1}}{a_{k}}}\cdot q^{k}\leq q^{k+1}$ zeigen. Wir erhalten die Ungleichung ${\tfrac {a_{k+1}}{a_{k}}}\leq q$. Hierzu benötigen wir die Aussage $a_{k}>0$, die wir im Folgenden annehmen. Aus der Bedingung $a_{k}>0$ können wir wiederum folgern, dass ${\tfrac {a_{k+1}}{a_{k}}}>0$ und damit auch $q>0$ ist.

\subsection{Zusammenfassung der ersten Überlegungen}

Aus $a_{1}\leq q$ und ${\tfrac {a_{k+1}}{a_{k}}}\leq q$ können wir zeigen, dass $a_{k}\leq q^{k}$ ist und die Reihe somit nach dem Majorantenkriterium konvergiert. Ein Beweis ist hier über vollständige Induktion möglich. Zunächst haben wir den Induktionsanfang $a_{1}\leq q$ direkt gegeben. Im Induktionsschritt gehen wir davon aus, dass $a_{k}\leq q^{k}$ wahr ist und können damit folgern

\begin{align*}
a_{k+1}&={\frac {a_{k+1}}{a_{k}}}\cdot a_{k}\\[0.5em]&\ {\color {OliveGreen}\left\downarrow \ a_{k}\leq q^{k}\right.}\\[0.5em]&\leq {\frac {a_{k+1}}{a_{k}}}\cdot q^{k}\\[0.5em]&\ {\color {OliveGreen}\left\downarrow \ {\frac {a_{k+1}}{a_{k}}}\leq q\right.}\\[0.5em]&\leq q^{k+1}\\[0.5em]
\end{align*}

\subsection{Erste Verbesserung}

Die Konvergenz einer Reihe hängt nicht vom Wert von endlich vielen Summanden ab. Das heißt, die Änderung endlich vieler Summanden beeinflusst die Konvergenz der Reihe nicht. Dementsprechend kann man vermuten, dass die Bedingung $a_{1}\leq q$ nicht benötigt wird. Wenn wir nur die Bedingung ${\tfrac {a_{k+1}}{a_{k}}}\leq q$ annehmen, dann erhalten wir

\begin{align*}
a_{2}&=a_{1}\cdot {\frac {a_{2}}{a_{1}}}\leq a_{1}\cdot q\\a_{3}&=a_{2}\cdot {\frac {a_{3}}{a_{2}}}\leq a_{2}\cdot q\leq a_{1}\cdot q^{2}\\a_{4}&=a_{3}\cdot {\frac {a_{4}}{a_{3}}}\leq a_{3}\cdot q\leq a_{1}\cdot q^{3}\\a_{5}&=a_{4}\cdot {\frac {a_{5}}{a_{4}}}\leq a_{4}\cdot q\leq a_{1}\cdot q^{4}\\&\vdots 
\end{align*}

Insgesamt erhalten wir so $a_{k}\leq a_{1}\cdot q^{k-1}$. Dies reicht aus, um mit Hilfe des Majorantenkriteriums die Konvergenz zu zeigen, weil $\sum _{k=1}^{\infty }a_{1}\cdot q^{k-1}$ eine konvergente Reihe ist. Damit kann man allein aus ${\frac {a_{k+1}}{a_{k}}}\leq q$ für alle $k\in \mathbb {N} $ die Konvergenz der Reihe zeigen.

\subsection{Zweite Verbesserung}

Wir können weiter verallgemeinern, indem wir ${\tfrac {a_{k+1}}{a_{k}}}\leq q$ nur für fast alle anstatt für alle natürlichen Zahlen $k$ fordern. Sei $K$ die erste natürliche Zahl, ab der ${\tfrac {a_{k+1}}{a_{k}}}\leq q$ für alle $k\geq K$ gilt. Dann ist

\begin{align*}
a_{K+1}&=a_{K}\cdot {\frac {a_{K+1}}{a_{K}}}\leq a_{K}\cdot q\\[0.5em]a_{K+2}&=a_{K+1}\cdot {\frac {a_{K+2}}{a_{K+1}}}\leq a_{K+1}\cdot q\leq a_{K}\cdot q^{2}\\[0.5em]a_{K+3}&=a_{K+2}\cdot {\frac {a_{K+3}}{a_{K+2}}}\leq a_{K+2}\cdot q\leq a_{K}\cdot q^{3}\\[0.5em]a_{K+4}&=a_{K+3}\cdot {\frac {a_{K+4}}{a_{K+3}}}\leq a_{K+3}\cdot q\leq a_{K}\cdot q^{4}\\[0.5em]&\vdots 
\end{align*}

Insgesamt erhalten wir so $a_{K+l}\leq a_{K}\cdot q^{l}$. Indem man $k=K+l$ setzt, folgt die Ungleichung $a_{k}\leq a_{K}\cdot q^{k-K}$ und somit:

\begin{align*}
\sum _{k=1}^{\infty }a_{k}&=\sum _{k=1}^{K-1}a_{k}+\sum _{k=K}^{\infty }a_{k}\\[0.5em]&{\color {OliveGreen}\left\downarrow \ \forall k\geq K:a_{k}\leq a_{K}\cdot q^{k-K}\right.}\\[0.5em]&\leq \sum _{k=1}^{K-1}a_{k}+\sum _{k=K}^{\infty }a_{K}\cdot q^{k-K}\\[0.5em]&=\underbrace {\underbrace {\sum _{k=1}^{K-1}a_{k}} _{\text{beschränkt}}+\underbrace {a_{K}\sum _{k=0}^{\infty }q^{k}} _{\text{konvergent}}} _{\text{beschränkt}}\\[0.5em]
\end{align*}

Damit konvergiert die Reihe nach dem Majorantenkriterium. Es reicht also, ${\tfrac {a_{k+1}}{a_{k}}}\leq q$ nur für fast alle $k\in \mathbb {N} $ zu fordern.

\subsection{Umformulierung mit Limes superior}

Die Bedingung, dass ${\tfrac {a_{k+1}}{a_{k}}}\leq q$ für ein festes $q$ mit $0<q<1$ und für fast alle $k\in \mathbb {N} $ ist, kann auch mit dem Limes superior ausgedrückt werden. Diese Bedingung gilt nämlich genau dann, wenn $\limsup _{k\to \infty }{\tfrac {a_{k+1}}{a_{k}}}<1$ ist.

Einerseits folgt aus ${\tfrac {a_{k+1}}{a_{k}}}\leq q$ für fast alle $k$, dass der größte Häufungspunkt, also der Limes superior, von $\left({\tfrac {a_{k+1}}{a_{k}}}\right)_{k\in \mathbb {N} }$ kleiner als $q$ und damit kleiner als $1$ ist.

Sei andererseits $\limsup _{k\to \infty }{\tfrac {a_{k+1}}{a_{k}}}={\tilde {q}}<1$. Dann ist für alle $\epsilon >0$ die Ungleichung ${\tfrac {a_{k+1}}{a_{k}}}\leq {\tilde {q}}+\epsilon $ für fast alle $k\in \mathbb {N} $ erfüllt. Wegen ${\tilde {q}}<1$ kann ein $\epsilon >0$ so klein gewählt werden, dass ${\tilde {q}}+\epsilon <1$ ist. Setzen wir $q={\tilde {q}}+\epsilon $. Dann ist zum einen $q<1$ und zum anderen ist ${\tfrac {a_{k+1}}{a_{k}}}\leq q$ für fast alle $k\in \mathbb {N} $.

Zusammenfassung: Aus $\limsup _{k\to \infty }{\tfrac {a_{k+1}}{a_{k}}}<1$ folgt zunächst für ein $q<1$, dass ${\tfrac {a_{k+1}}{a_{k}}}\leq q$ für fast alle $k\in \mathbb {N} $ ist. Daraus folgt die Konvergenz der Reihe $\sum _{k=1}^{\infty }a_{k}$.

\subsection{Die Sache mit der absoluten Konvergenz}

In der obigen Argumentation haben wir nur Reihen betrachtet, deren Summanden nichtnegativ sind. Was passiert mit Reihen $\sum _{k=1}^{\infty }a_{k}$, bei denen einige Summanden $a_{k}$ negativ sind?

Wir können obige Argumentation zumindest auf die Reihe $\sum _{k=1}^{\infty }|a_{k}|$ anwenden. So können wir die absolute Konvergenz beweisen, die ja auch die normale Konvergenz der Reihe impliziert. Bei Reihen mit nichtnegativen Summanden ändert sich beim Übergang von $\sum _{k=1}^{\infty }a_{k}$ auf $\sum _{k=1}^{\infty }|a_{k}|$ nichts, da für solche Reihen die Gleichung $a_{k}=|a_{k}|$ für alle $k$ erfüllt ist. Wir können also zusammenfassen:

\begin{importantparagraph*}
Ist $\limsup _{k\to \infty }{\tfrac {|a_{k+1}|}{|a_{k}|}}=\limsup _{k\to \infty }\left|{\tfrac {a_{k+1}}{a_{k}}}\right|<1$, dann konvergiert die Reihe $\sum _{k=1}^{\infty }a_{k}$ absolut.

\end{importantparagraph*}

\subsection{Quotientenkriterium für Divergenz}

Lässt sich mit einer ähnlichen Argumentation auch die Divergenz einer Reihe beweisen? Schauen wir uns $\left|{\tfrac {a_{k+1}}{a_{k}}}\right|$ an. Wenn der Quotient im Betrag größer gleich eins ist, dann ist

\begin{align*}
\left|{\frac {a_{k+1}}{a_{k}}}\right|\geq 1\implies |a_{k+1}|\geq |a_{k}|
\end{align*}

Wenn also ab einem beliebigen Index $K$ für alle nachfolgenden Indizes $k$ die Ungleichung $\left|{\tfrac {a_{k+1}}{a_{k}}}\right|\geq 1$ erfüllt ist, dann wächst die Folge $\left(|a_{k}|\right)_{k\in \mathbb {N} }$ ab dem Index $K$ monoton. Diese Folge kann keine Nullfolge sein, da sie nach dem Folgenglied $|a_{K}|$ monoton wächst und $|a_{K}|>0$. Wenn aber $\left(|a_{k}|\right)_{k\in \mathbb {N} }$ keine Nullfolge ist, dann ist auch $\left(a_{k}\right)_{k\in \mathbb {N} }$ keine Nullfolge. Daraus folgt nach dem Trivialkriterium, dass die Reihe $\sum _{k=1}^{\infty }a_{k}$ keine Nullfolge ist. Das Trivialkriterium besagt ja, dass $\lim _{k\to \infty }a_{k}=0$ wäre, wenn die Reihe $\sum _{k=1}^{\infty }a_{k}$ konvergieren würde. Fassen wir zusammen:

\begin{importantparagraph*}
Ist $\left|{\tfrac {a_{k+1}}{a_{k}}}\right|\geq 1$ für fast alle $k\in \mathbb {N} $ erfüllt, dann ist $\left(a_{k}\right)_{k\in \mathbb {N} }$ keine Nullfolge. Die Reihe $\sum _{k=1}^{\infty }a_{k}$ divergiert nach dem Trivialkriterium.

\end{importantparagraph*}
\clearpage
\section{Das Quotientenkriterium}

\subsection{Satz}

\begin{theorem*}[Quotienten-Kriterium für Konvergenz]
Sei $\sum _{k=1}^{\infty }a_{k}$ eine Reihe mit $a_{k}\neq 0$ für alle $k\in \mathbb {N} $. Wenn $\limsup _{k\to \infty }\left|{\tfrac {a_{k+1}}{a_{k}}}\right|<1$ ist, dann ist die Reihe $\sum _{k=1}^{\infty }a_{k}$ absolut konvergent.

Wenn $\left|{\tfrac {a_{k+1}}{a_{k}}}\right|\geq 1$ für fast alle $k\in \mathbb {N} $ ist (also für alle $k\geq K$ für ein bestimmtes $K\in \mathbb {N} $), dann ist die Reihe $\sum _{k=1}^{\infty }a_{k}$ divergent.

\end{theorem*}

\begin{explanation*}[Quotienten-Kriterium für Konvergenz]
Fassen wir die obige Herleitung in einem Beweis zusammen:

\end{explanation*}

\begin{proof*}[Quotienten-Kriterium für Konvergenz]
Sei $\sum _{k=1}^{\infty }a_{k}$ eine Reihe mit Summanden ungleich Null.

\proofstep{Konvergenz mit dem Quotientenkriterium:}
 \begin{indentblock}
Sei ${\tilde {\theta }}=\limsup _{k\to \infty }\left|{\tfrac {a_{k+1}}{a_{k}}}\right|<1$. Wir wählen nun $\epsilon >0$ so klein, dass $\theta ={\tilde {\theta }}+\epsilon <1$ ist. Wegen ${\tilde {\theta }}<1$ existiert dieses $\epsilon $ (beispielsweise kann $\epsilon ={\tfrac {1-{\tilde {\theta }}}{2}}$ gewählt werden). Aus den Eigenschaften des Limes superior folgt, dass für fast alle $k\in \mathbb {N} $ die Ungleichung $\left|{\tfrac {a_{k+1}}{a_{k}}}\right|<\theta $ erfüllt ist. Es gibt also eine natürliche Zahl $K$, sodass $\left|{\tfrac {a_{k+1}}{a_{k}}}\right|<\theta $ für alle $k\geq K$ ist. Es folgt:

\begin{align*}
|a_{K+1}|&=|a_{K}|\cdot \left|{\frac {a_{K+1}}{a_{K}}}\right|\leq |a_{K}|\cdot q\\[0.5em]|a_{K+2}|&=|a_{K+1}|\cdot \left|{\frac {a_{K+2}}{a_{K+2}}}\right|\leq |a_{K+1}|\cdot q\leq |a_{K}|\cdot q^{2}\\[0.5em]|a_{K+3}|&=|a_{K+2}|\cdot \left|{\frac {a_{K+4}}{a_{K+3}}}\right|\leq |a_{K+2}|\cdot q\leq |a_{K}|\cdot q^{3}\\[0.5em]|a_{K+4}|&=|a_{K+3}|\cdot \left|{\frac {a_{K+5}}{a_{K+4}}}\right|\leq |a_{K+3}|\cdot q\leq |a_{K}|\cdot q^{4}\\[0.5em]&\vdots 
\end{align*}

Insgesamt erhält man so $|a_{K+l}|\leq |a_{K}|\cdot q^{l}$. Indem man $k=K+l$ setzt, folgt die Ungleichung $|a_{k}|\leq |a_{K}|\cdot q^{k-K}$ und somit:

\begin{align*}
\sum _{k=1}^{\infty }|a_{k}|&=\sum _{k=1}^{K-1}|a_{k}|+\sum _{k=K}^{\infty }|a_{k}|\\[0.5em]&{\color {OliveGreen}\left\downarrow \ \forall k\geq K:|a_{k}|\leq |a_{K}|\cdot q^{k-K}\right.}\\[0.5em]&\leq \sum _{k=1}^{K-1}|a_{k}|+\sum _{k=K}^{\infty }|a_{K}|\cdot q^{k-K}\\[0.5em]&{\color {OliveGreen}\left\downarrow \ {\text{Indexverschiebung in der 2. Reihe}}\right.}\\[0.5em]&=\underbrace {\underbrace {\sum _{k=1}^{K-1}|a_{k}|} _{\text{beschränkt}}+\underbrace {|a_{K}|\sum _{k=0}^{\infty }q^{k}} _{\text{konvergent = beschränkt}}} _{\text{beschränkt}}\\[0.5em]
\end{align*}

Damit konvergiert die Reihe $\sum _{k=1}^{\infty }|a_{k}|$ nach dem Majorantenkriterium. Dies bedeutet wiederum, dass $\sum _{k=1}^{\infty }a_{k}$ absolut konvergiert.

\end{indentblock}

\proofstep{Divergenz mit dem Quotientenkriterium:}
 \begin{indentblock}
Sei nun $K$ eine natürliche Zahl, sodass $\left|{\tfrac {a_{k+1}}{a_{k}}}\right|\geq 1$ für fast alle $k\geq K$. Es ist dann für alle $k\geq K$:

\begin{align*}
&&\left|{\frac {a_{k+1}}{a_{k}}}\right|&\geq 1\\[0.5em]&\Rightarrow &|a_{k+1}|&\geq |a_{k}|
\end{align*}

Damit wächst die Folge $\left(|a_{k}|\right)_{k\in \mathbb {N} }$ ab dem Index $K$ monoton. $\left(|a_{k}|\right)_{k\in \mathbb {N} }$ ist keine Nullfolge, weil $|a_{K}|>0$ ist (wegen $a_{K}\neq 0$). Damit ist aber auch $\left(a_{k}\right)_{k\in \mathbb {N} }$ keine Nullfolge. Aus dem Trivialkriterium für Reihen folgt, dass die Reihe $\sum _{k=1}^{\infty }a_{k}$ divergiert.

\end{indentblock}

\end{proof*}

\subsection{Verschärfung mit Limes inferior}

Die gerade behandelte Voraussetzung für die Divergenz lässt sich mit Hilfe des Limes inferior verschärfen. So ist das Kriterium leichter anzuwenden. Gilt $\liminf _{k\to \infty }\left|{\tfrac {a_{k+1}}{a_{k}}}\right|>1$, folgt daraus $\left|{\tfrac {a_{k+1}}{a_{k}}}\right|\geq 1$ für fast alle $k\geq K$. Also divergiert die Reihe. Die umgekehrte Richtung muss nicht gelten. Aus $\left|{\tfrac {a_{k+1}}{a_{k}}}\right|\geq 1$ für fast alle $k\geq K$ muss nicht $\liminf _{k\to \infty }\left|{\tfrac {a_{k+1}}{a_{k}}}\right|>1$ folgen, da die Folge $\left(\left|{\tfrac {a_{k+1}}{a_{k}}}\right|\right)_{k\in \mathbb {N} }$ nicht zwangsläufig einen kleinsten Häufungspunkt besitzt. Es handelt sich also um eine stärkere Voraussetzung für die Divergenz der Reihe.

\begin{hint*}
Ist $\lim _{k\to \infty }\left|{\tfrac {a_{k+1}}{a_{k}}}\right|<1$, dann gibt es ein $\theta <1$ mit $\left|{\tfrac {a_{k+1}}{a_{k}}}\right|<\theta $ für fast alle $k\in \mathbb {N} $, womit die Reihe absolut konvergiert. Analog divergiert die Reihe, wenn $\lim _{k\to \infty }\left|{\tfrac {a_{k+1}}{a_{k}}}\right|>1$ ist.

\end{hint*}

\section{Grenzen des Quotientenkriteriums}

Bei $\lim _{k\to \infty }\left|{\tfrac {a_{k+1}}{a_{k}}}\right|=1$ können wir nichts über Konvergenz bzw. Divergenz der Reihe aussagen. Es gibt nämlich sowohl konvergente als auch divergente Reihen, die diese Bedingung erfüllen. Ein Beispiel hierfür ist die divergente Reihe $\sum _{k=1}^{\infty }{\frac {1}{k}}$:

\begin{align*}
\lim _{k\to \infty }\left|{\frac {a_{k+1}}{a_{k}}}\right|&=\lim _{k\to \infty }{\frac {k}{k+1}}\\[0.5em]&=\lim _{k\to \infty }{\frac {1}{1+{\frac {1}{k}}}}=1
\end{align*}

Auch die konvergente Reihe $\sum _{k=1}^{\infty }{\frac {1}{k^{2}}}$ erfüllt diese Gleichung:

\begin{align*}
\lim _{k\to \infty }\left|{\frac {a_{k+1}}{a_{k}}}\right|&=\lim _{k\to \infty }{\frac {k^{2}}{(k+1)^{2}}}\\[0.5em]&=\lim _{k\to \infty }{\frac {k^{2}}{k^{2}+2k+1}}\\[0.5em]&=\lim _{k\to \infty }{\frac {1}{1+{\frac {2}{k}}+{\frac {1}{k}}}}=1
\end{align*}

Wir können also aus $\lim _{k\to \infty }\left|{\tfrac {a_{k+1}}{a_{k}}}\right|=1$ weder folgern, dass die Reihe konvergiert, noch, dass sie divergiert. Wir müssen in einem solchen Fall ein anderes Konvergenzkriterium verwenden.

\section{Vorgehen bei der Anwendung des Quotientenkriteriums}

Um das Quotientenkriterium auf eine Reihe $\sum _{k=1}^{\infty }a_{k}$ anzuwenden, bilden wir zunächst $\left|{\tfrac {a_{k+1}}{a_{k}}}\right|$ und betrachten den Grenzwert:

\begin{enumerate}
\item Ist $\limsup _{k\to \infty }\left|{\tfrac {a_{k+1}}{a_{k}}}\right|<1$, dann konvergiert die Reihe absolut.
\item Ist $\liminf _{k\to \infty }\left|{\tfrac {a_{k+1}}{a_{k}}}\right|>1$, dann divergiert die Reihe.
\item Ist $\left|{\tfrac {a_{k+1}}{a_{k}}}\right|\geq 1$ für fast alle $k\in \mathbb {N} $, dann divergiert die Reihe.
\item Können wir keinen der drei Fälle anwenden, können wir nichts über die Konvergenz der Reihe aussagen.
\end{enumerate}

\section{Beispielaufgaben}

\begin{exercise*}
Untersuche die Reihe $\sum _{k=1}^{\infty }{\frac {2^{k}}{k!}}$ auf Konvergenz oder Divergenz.

\end{exercise*}

\begin{solutionprocess*}
Zunächst bilden wir den Quotienten $\left|{\tfrac {a_{k+1}}{a_{k}}}\right|$ und betrachten dessen Grenzwert:

\begin{align*}
\lim _{k\to \infty }\left|{\frac {a_{k+1}}{a_{k}}}\right|&=\lim _{k\to \infty }{\frac {2^{k+1}}{(k+1)!}}\cdot {\frac {k!}{2^{k}}}\\[0.5em]&=\lim _{k\to \infty }{\frac {2}{k+1}}\\[0.5em]&=0
\end{align*}

Damit ist $\lim _{k\to \infty }\left|{\tfrac {a_{k+1}}{a_{k}}}\right|<1$, womit aus dem Quotientenkriterium folgt, dass die Reihe absolut konvergiert.

\end{solutionprocess*}

\begin{proof*}
Die Reihe $\sum _{k=1}^{\infty }{\frac {2^{k}}{k!}}$ konvergiert absolut nach dem Quotientenkriterium, denn es ist

\begin{align*}
\lim _{k\to \infty }\left|{\frac {a_{k+1}}{a_{k}}}\right|&=\lim _{k\to \infty }{\frac {2^{k+1}}{(k+1)!}}\cdot {\frac {k!}{2^{k}}}\\[0.5em]&=\lim _{k\to \infty }{\frac {2}{k+1}}\\[0.5em]&=0<1
\end{align*}

\end{proof*}

\begin{exercise*}
Untersuche die Reihe $\sum _{k=1}^{\infty }{\frac {k^{k}}{k!}}$ auf Konvergenz bzw. Divergenz.

\end{exercise*}

\begin{solutionprocess*}
Wir haben $a_{k}={\tfrac {k^{k}}{k!}}$. Schauen wir uns nun $\left|{\tfrac {a_{k+1}}{a_{k}}}\right|$ an

\begin{align*}
\left|{\frac {a_{k+1}}{a_{k}}}\right|&=\left|{\frac {(k+1)^{k+1}}{(k+1)!}}\cdot {\frac {k!}{k^{k}}}\right|\\[0.5em]&\ {\color {OliveGreen}\left\downarrow \ {\frac {k!}{(k+1)!}}={\frac {1}{k+1}}\right.}\\[0.5em]&={\frac {(k+1)^{k+1}}{(k+1)\cdot k^{k}}}\\[0.5em]&={\frac {(k+1)^{k}}{k^{k}}}\\[0.5em]&=\left(1+{\frac {1}{k}}\right)^{k}
\end{align*}

Nun ist $1+{\tfrac {1}{k}}\geq 1$ und damit auch $\left(1+{\tfrac {1}{k}}\right)^{k}\geq 1$. Daraus folgt, dass $\left|{\tfrac {a_{k+1}}{a_{k}}}\right|\geq 1$ für alle und damit insbesondere für fast alle $k\in \mathbb {N} $. Aus dem Quotientenkriterium folgt, dass die Reihe divergiert.

\end{solutionprocess*}

\begin{proof*}
Die Reihe $\sum _{k=1}^{\infty }{\frac {k^{k}}{k!}}$ divergiert, denn für $a_{k}={\tfrac {k^{k}}{k!}}$ ist

\begin{align*}
\left|{\frac {a_{k+1}}{a_{k}}}\right|&=\left|{\frac {(k+1)^{k+1}}{(k+1)!}}\cdot {\frac {k!}{k^{k}}}\right|\\[0.5em]&\ {\color {OliveGreen}\left\downarrow \ {\frac {k!}{(k+1)!}}={\frac {1}{k+1}}\right.}\\[0.5em]&={\frac {(k+1)^{k+1}}{(k+1)\cdot k^{k}}}\\[0.5em]&={\frac {(k+1)^{k}}{k^{k}}}\\[0.5em]&=\left(1+{\frac {1}{k}}\right)^{k}\geq 1
\end{align*}

\end{proof*}

\begin{hint*}
Du weißt vielleicht schon, dass $\lim _{k\to \infty }\left(1+{\tfrac {1}{k}}\right)^{k}=e$ ist. Dementsprechend kannst du alternativ auch den Beweis darüber führen, dass $\lim _{k\to \infty }\left|{\tfrac {a_{k+1}}{a_{k}}}\right|=\lim _{k\to \infty }\left(1+{\tfrac {1}{k}}\right)^{k}=e>1$. Diese Argumentation kannst du aber nur anwenden, wenn du $\lim _{k\to \infty }\left(1+{\tfrac {1}{k}}\right)^{k}=e$ bereits in deiner Vorlesung bewiesen hast.

\end{hint*}

\section{Vergleich zwischen Quotienten- und Wurzelkriterium}

Das Quotientenkriterium lässt sich bei einigen Reihen wesentlich leichter anwenden als das Wurzelkriterium. Die beiden vorherigen Beispiele bestätigen dies. Das Quotientenkriterium war in beiden Fällen sehr gut anwendbar.

Wendet man hingegen das Wurzelkriterium auf die Reihe $\sum _{k=1}^{\infty }{\frac {2^{k}}{k!}}$ an, so erhält man die Wurzelfolge

\begin{align*}
{\sqrt[{k}]{|a_{k}|}}={\frac {2}{\sqrt[{k}]{k!}}}
\end{align*}

Bei dieser ist zunächst völlig unklar, ob und wogegen sie konvergiert. Dass $k!$ mit größer werdendem $k$ schnell anwächst, könnte dafür sprechen, dass eine Nullfolge vorliegt, allerdings wird dies durch das Ziehen der $k$-ten Wurzel stark abgeschwächt. Tatsächlich lässt sich ${\sqrt[{k}]{k!}}\to \infty $ zeigen (und damit ${\sqrt[{k}]{|a_{k}|}}\to 0$). Das ist jedoch sehr aufwendig und wird daher hier nicht weiter ausgeführt.

Ähnlich verhält es sich bei der Reihe $\sum _{k=1}^{\infty }{\frac {k^{k}}{k!}}$. Hier ist

\begin{align*}
{\sqrt[{k}]{|a_{k}|}}={\frac {k}{\sqrt[{k}]{k!}}}.
\end{align*}

Man kann beweisen, dass diese Folge gegen $e$ konvergiert. Das ist jedoch sehr aufwendig und erfordert zusätzliche Konvergenzkriterien, die oftmals in einer Analysis-Grundvorlesung nicht zur Verfügung stehen. In beiden Fällen ist die Lösung mit dem Quotientenkriterium einfacher.

Allerdings gibt es auch Beispiele von Reihen, die mit dem Wurzelkriterium lösbar sind und bei denen das Quotientenkriterium gar nicht anwendbar ist. Ein Beispiel dafür ist die Reihe

\begin{align*}
\sum _{k=0}^{\infty }a_{k}{\text{ mit }}a_{k}={\begin{cases}\left({\frac {1}{2}}\right)^{k}&{\text{ für gerade }}k,\\\left({\frac {1}{3}}\right)^{k}&{\text{ für ungerade }}k.\end{cases}}
\end{align*}

Das Quotientenkriterium ist hier nicht anwendbar.

Hingegen liefert das Wurzelkriterium

\begin{align*}
{\sqrt[{k}]{|a_{k}|}}={\begin{cases}{\frac {1}{2}}&{\text{ für gerade }}k,\\{\frac {1}{3}}&{\text{ für ungerade }}k.\end{cases}}
\end{align*}

Damit ist $\limsup \limits _{k\to \infty }{\sqrt[{k}]{|a_{k}|}}={\frac {1}{2}}<1$ und die Reihe konvergiert absolut.

Das Wurzelkriterium hat einen größeren Anwendungsbereich als das Quotientenkriterium, weil allgemein folgende Ungleichung gilt:

\begin{align*}
\liminf _{k\to \infty }\left|{\frac {a_{k+1}}{a_{k}}}\right|\leq \liminf _{k\to \infty }{\sqrt[{k}]{|a_{k}|}}\leq \limsup _{k\to \infty }{\sqrt[{k}]{|a_{k}|}}\leq \limsup _{k\to \infty }\left|{\frac {a_{k+1}}{a_{k}}}\right|.
\end{align*}

Hier wird offensichtlich: Ist $\limsup _{k\to \infty }\left|{\frac {a_{k+1}}{a_{k}}}\right|<1$, so ist automatisch $\limsup _{k\to \infty }{\sqrt[{k}]{|a_{k}|}}<1$. Ist $\liminf _{k\to \infty }\left|{\frac {a_{k+1}}{a_{k}}}\right|>1$, ist automatisch $\limsup _{k\to \infty }{\sqrt[{k}]{|a_{k}|}}>1$. Ist also das Quotientenkriterium anwendbar, ist immer das Wurzelkriterium anwendbar, umgekehrt jedoch nicht. Wir verzichten hier auf den etwas theoretischen und langwierigen Beweis der Ungleichung.

\chapter{Leibniz-Kriterium}

Das \emph{Leibniz-Kriterium} ist ein spezielles Konvergenzkriterium für \emph{alternierende} Reihen. Das sind Reihen, bei denen das Vorzeichen bei jedem Summanden wechselt, also Reihen der Form $\sum _{k=1}^{\infty }(-1)^{k+1}b_{k}$ oder $\sum _{k=1}^{\infty }(-1)^{k}b_{k}$, wobei alle $b_{k}$ positiv sind. Da solche Reihen häufig konvergieren, aber nicht absolut konvergieren, scheitern die anderen Konvergenzkriterien oftmals.

Wie der Name schon vermuten lässt, wurde das Kriterium von dem Mathematiker \href{https://de.wikipedia.org/wiki/Gottfried\%20Wilhelm\%20Leibniz}
{Gottfried Wilhelm Leibniz} im Jahre 1682 veröffentlicht. Übrigens wurde auch der Butterkeks mit seinen 52 Zähnen (in Anlehnung an die 52 Zahnräder der ersten von Leibniz entwickelten Rechenmaschine) nach ihm benannt.

\section{Konvergenz der alternierenden harmonischen Reihe}

Da Beweisideen an konkreten Beispielen oftmals besser veranschaulicht werden können, betrachten wir zunächst das Beispiel der alternierenden harmonischen Reihe $\sum _{k=1}^{\infty }{\frac {(-1)^{k+1}}{k}}$. Für die Konvergenz müssen wir zeigen, dass die Folge der Partialsummen $(S_{n})_{n\in \mathbb {N} }=\left(\sum _{k=1}^{n}{\frac {(-1)^{k+1}}{k}}\right)_{n\in \mathbb {N} }$ konvergiert. Für $n=1,2,3,4,5,6,7,8$ haben die Partialsummen die Werte

\begin{align*}
S_{1}&=\sum _{k=1}^{1}{\frac {(-1)^{k+1}}{k}}=1,&S_{2}&=\sum _{k=1}^{2}{\frac {(-1)^{k+1}}{k}}={\frac {1}{2}}=0.5,\\S_{3}&=\sum _{k=1}^{3}{\frac {(-1)^{k+1}}{k}}={\frac {5}{6}}=0.8{\overline {3}},&S_{4}&=\sum _{k=1}^{4}{\frac {(-1)^{k+1}}{k}}={\frac {7}{12}}=0.58{\overline {3}},\\S_{5}&=\sum _{k=1}^{5}{\frac {(-1)^{k+1}}{k}}={\frac {47}{60}}=0.78{\overline {3}},&S_{6}&=\sum _{k=1}^{6}{\frac {(-1)^{k+1}}{k}}={\frac {37}{60}}=0.61{\overline {6}},\\S_{7}&=\sum _{k=1}^{7}{\frac {(-1)^{k+1}}{k}}={\frac {329}{420}}=0.759,&S_{8}&=\sum _{k=1}^{8}{\frac {(-1)^{k+1}}{k}}={\frac {533}{840}}=0.634.
\end{align*}

Daran erkennen wir, dass die Werte in immer kleiner werdenden Schritten hin und her springen. Außerdem fällt auf, dass die Partialsummen mit ungeraden Indizes $S_{2n-1}$ anscheinend monoton fallen und diejenigen mit geraden Indizes $S_{2n}$ monoton wachsen. Dies können wir allgemein leicht nachrechnen. Für alle $n\in \mathbb {N} $ gilt nämlich

\begin{align*}
S_{2n+1}-S_{2n-1}&=\sum _{k=1}^{2n+1}{\frac {(-1)^{k+1}}{k}}-\sum _{k=1}^{2n-1}{\frac {(-1)^{k+1}}{k}}\\[0.5em]&=({\color {Teal}1-{\frac {1}{2}}+{\frac {1}{3}}-\ldots +{\frac {(-1)^{2n}}{2n-1}}}+{\frac {(-1)^{2n+1}}{2n}}+{\frac {(-1)^{2n+2}}{2n+1}})\\[0.5em]&-({\color {Indigo}1-{\frac {1}{2}}+{\frac {1}{3}}-\ldots +{\frac {(-1)^{2n}}{2n-1}}})\\[0.5em]&={\frac {(-1)^{2n+1}}{2n}}+{\frac {(-1)^{2n+2}}{2n+1}}\\[0.5em]&={\frac {1}{2n+1}}-{\frac {1}{2n}}\\[0.5em]&{\color {OliveGreen}\left\downarrow \ 2n+1\geq 2n\Rightarrow {\frac {1}{2n+1}}\leq {\frac {1}{2n}}\right.}\\[0.5em]&\leq 0,
\end{align*}

d.h. $S_{2n+1}\leq S_{2n-1}$. Und ganz analog $S_{2n+2}-S_{2n}={\frac {(-1)^{2n+3}}{2n+2}}+{\frac {(-1)^{2n+2}}{2n+1}}=-{\frac {1}{2n+2}}+{\frac {1}{2n+1}}\geq 0$, d.h. $S_{2n+2}\geq S_{2n}$. Damit ist $(S_{2n-1})$ monoton fallend und $(S_{2n})$ monoton steigend.

Wenn wir zeigen könnten, dass $(S_{2n-1})$ nach unten und $(S_{2n})$ nach oben beschränkt sind, dann wären beide (Teil-)Folgen nach dem Monotoniekriterium konvergent. Nun sind aber alle ungeraden Partialsummen durch die geraden Partialsummen nach unten und umgekehrt alle geraden durch die ungeraden nach oben beschränkt, denn für alle $n\in \mathbb {N} $ gilt

\begin{align*}
S_{2n-1}-S_{2n}=-{\frac {(-1)^{2n+1}}{2n}}={\frac {(-1)^{2n+2}}{2n}}={\frac {1}{2n}}\geq 0,
\end{align*}

und damit $S_{2n-1}\geq S_{2n}$ bzw. $S_{2n}\leq S_{2n-1}$. Insbesondere gilt daher $S_{2n-1}\geq S_{2n}\geq S_{2}={\frac {1}{2}}$ und $S_{2n}\leq S_{2n-1}\leq S_{1}=1$. Also ist $(S_{2n-1})$ nach unten durch ${\frac {1}{2}}$ und $(S_{2n})$ nach oben durch $1$ beschränkt.

Nach dem Monotoniekriterium sind somit $(S_{2n-1})$ und $(S_{2n})$ konvergent.

Wir sind aber noch nicht fertig! Zum einen müssen wir zeigen, dass beide Teilfolgen gegen denselben Grenzwert konvergieren und zum anderen, dass daraus auch die Konvergenz von $(S_{n})$ folgt.

Sei also $\lim _{n\to \infty }S_{2n-1}=S$ und $\lim _{n\to \infty }S_{2n}=S'$. Wir müssen nun zeigen, dass beide Grenzwerte gleich sind, also dass $S=S'$ gilt. Dies lässt sich aber schnell erledigen. Einerseits ist nämlich mit der Summenregel für Grenzwerte

\begin{align*}
S-S'=\lim _{n\to \infty }S_{2n-1}-\lim _{n\to \infty }S_{2n}=\lim _{n\to \infty }(S_{2n-1}-S_{2n}).
\end{align*}

Andererseits haben wir oben $S_{2n-1}-S_{2n}={\frac {1}{2n}}$ gezeigt. Damit ist nun

\begin{align*}
\lim _{n\to \infty }(S_{2n-1}-S_{2n})=\lim _{n\to \infty }{\frac {1}{2n}}=0.
\end{align*}

Also ist $S-S'=0$ und daher $S=S'$.

Nun müssen wir noch zeigen, dass $(S_{n})$ ebenfalls gegen $S$ konvergiert. Dazu müssen wir die Definition der Konvergenz benutzen, d.h. wir müssen zeigen

\begin{align*}
\forall \epsilon >0\,\exists N\in \mathbb {N} \,\forall n\geq N:|S_{n}-S|<\epsilon .
\end{align*}

Wir wissen aber bereits

\begin{align*}
&\forall \epsilon >0\,\exists N_{1}\in \mathbb {N} \,\forall n\geq N_{1}:|S_{2n-1}-S|<\epsilon \\[0.3em]&\forall \epsilon >0\,\exists N_{2}\in \mathbb {N} \,\forall n\geq N_{2}:|S_{2n}-S|<\epsilon ,
\end{align*}

da ja $(S_{2n-1})$ und $(S_{2n})$ gegen denselben Grenzwert $S$ konvergieren. Setzen wir nun $N=\max\{2N_{1}-1,2N_{2}\}$, so folgt unmittelbar

\begin{align*}
\forall \epsilon >0\,\exists N\in \mathbb {N} \,\forall n\geq N:|S_{n}-S|<\epsilon .
\end{align*}

Frage: Warum reicht $N=\max\{N_{1},N_{2}\}$ nicht aus? Antwort: Die Aussage $\forall \epsilon >0\,\exists N_{1}\in \mathbb {N} \,\forall n\geq N_{1}:|S_{2n-1}-S|<\epsilon $ bedeutet, dass ab dem Folgenglied $S_{2N_{1}-1}$ für alle ungeraden Folgenglieder von $(S_{n})$ die Ungleichung $|S_{n}-S|<\epsilon $ erfüllt ist.

Analog bedeutet $\forall \epsilon >0\,\exists N_{2}\in \mathbb {N} \,\forall n\geq N_{2}:|S_{2n}-S|<\epsilon $, dass ab dem Folgenglied $S_{2N_{2}}$ für alle geraden Folgenglieder von $(S_{n})$ die Ungleichung $|S_{n}-S|<\epsilon $ erfüllt ist.

Da nun aber $2N_{1}-1\geq N_{1}$ und $2N_{2}>N_{2}$ gilt, müssen die Ungleichungen natürlich noch nicht ab den Folgengliedern $S_{N_{1}}$ bzw. $S_{N_{2}}$ gelten.

\section{Verallgemeinerung der Beweisidee für das Leibniz-Kriterium}

Die Frage ist nun, inwiefern wir den gerade geführten Beweis für die Konvergenz der alternierenden harmonischen Reihe verallgemeinern können, um ein allgemeines Konvergenzkriterium für alternierende Reihen zu erhalten. Dazu müssen wir uns klar machen, welche Eigenschaften der alternierenden harmonischen Reihe wir für den Konvergenzbeweis herangezogen haben.

\begin{itemize}
\item Zum einen wissen wir, dass die nichtnegative Koeffizientenfolge ohne das alternierende Vorzeichen $(b_{k})=\left({\frac {1}{k}}\right)$ monoton fällt. Daraus hat sich dann die Monotonie und die Beschränktheit der beiden (Teil-)Partialfolgen $(S_{2n-1})$ und $(S_{2n})$ und damit deren Konvergenz ergeben.
\item Zum anderen haben wir davon Gebrauch gemacht, dass $(b_{k})=\left({\frac {1}{k}}\right)$ eine Nullfolge ist. Daraus konnten wir schließlich folgern, dass $(S_{2n-1})$ und $(S_{2n})$ und damit auch $(S_{n})$ gegen denselben Grenzwert konvergieren.
\end{itemize}

Mehr Eigenschaften der alternierenden harmonischen Reihe hatten wir im Beweis oben nicht verwendet. Genau das sind auch die Voraussetzungen für das Leibniz-Kriterium:

\begin{theorem*}[Leibniz-Kriterium]
Sei $(b_{k})_{k\in \mathbb {N} }$ eine nichtnegative monoton fallende Folge reeller Zahlen mit $\lim _{k\to \infty }b_{k}=0$, dann konvergiert die alternierende Reihe $\sum _{k=1}^{\infty }(-1)^{k+1}b_{k}$.

\end{theorem*}

\begin{explanation*}[Leibniz-Kriterium]
Für den Beweis müssen wir nun nur noch einmal den Beweis, den wir für die Konvergenz der alternierenden harmonischen Reihe geführt haben, für eine allgemeine alternierende Reihe mit denselben Eigenschaften durchführen.

\end{explanation*}

\begin{proof*}[Leibniz-Kriterium]
Erneut müssen wir die Konvergenz der Partialsummenfolge $(S_{n})=\left(\sum _{k=1}^{n}(-1)^{k+1}b_{k}\right)$ zeigen.

\textbf{Beweisschritt 1:} $(S_{2n-1})$ ist monoton fallend und $(S_{2n})$ monoton steigend, denn für $n\in \mathbb {N} $ gilt

\begin{align*}
S_{2n+1}-S_{2n-1}&=\sum _{k=1}^{2n+1}(-1)^{k+1}b_{k}-\sum _{k=1}^{2n-1}(-1)^{k+1}b_{k}\\[0.5em]&=({\color {Teal}b_{1}-b_{2}+b_{3}-\ldots +(-1)^{2n}b_{2n-1}}\\[0.5em]&+(-1)^{2n+1}b_{2n}+(-1)^{2n+2}b_{2n+1})\\[0.5em]&-({\color {Indigo}b_{1}-b_{2}+b_{3}-\ldots +(-1)^{2n}b_{2n-1}})\\[0.5em]&=(-1)^{2n+2}b_{2n+1}+(-1)^{2n+1}b_{2n}\\[0.5em]&=b_{2n+1}-b_{2n}\\[0.5em]&{\color {OliveGreen}\left\downarrow \ b_{2n+1}\leq b_{2n}\right.}\\[0.5em]&\leq 0,
\end{align*}

und analog $S_{2n+2}-S_{2n}=(-1)^{2n+3}b_{2n+2}+(-1)^{2n+2}b_{2n+1}=-b_{2n+2}+b_{2n+1}\geq 0$.

\textbf{Beweisschritt 2:} $(S_{2n-1})$ ist nach unten und $(S_{2n})$ nach oben beschränkt, denn für $n\in \mathbb {N} $ gilt

\begin{align*}
S_{2n-1}-S_{2n}=(-1)^{2n+2}b_{2n}=b_{2n}\geq 0
\end{align*}

Damit ist $S_{2n-1}\geq S_{2n}\geq S_{2}=b_{1}-b_{2}$ sowie $S_{2n}\leq S_{2n-1}\leq S_{1}=b_{1}$

Nach dem Monotoniekriterium konvergieren somit die Partialsummenfolgen $(S_{2n-1})$ und $(S_{2n})$.

\textbf{Beweisschritt 3:} $(S_{2n-1})$ und $(S_{2n})$ konvergieren gegen denselben Grenzwert. Sei $\lim _{n\to \infty }S_{2n-1}=S$ und $\lim _{n\to \infty }S_{2n}=S'$. Da wir in Beweisschritt 2 die Konvergenz beider Folgen gezeigt haben, können wir die Summenregel für Grenzwerte anwenden. Es folgt

\begin{align*}
S-S'=\lim _{n\to \infty }S_{2n-1}-\lim _{n\to \infty }S_{2n}=\lim _{n\to \infty }(S_{2n-1}-S_{2n})
\end{align*}

Andererseits gilt

\begin{align*}
\lim _{n\to \infty }(S_{2n-1}-S_{2n})=\lim _{n\to \infty }b_{2n}=0,
\end{align*}

da $(b_{k})$ eine Nullfolge ist und damit auch die Teilfolge $(b_{2n})$. Also ist $S=S'$.

\textbf{Beweisschritt 4:} $(S_{n})$ konvergiert ebenfalls gegen $S$. Da $(S_{2n-1})$ und $(S_{2n})$ gegen $S$ konvergieren, gilt

\begin{align*}
&\forall \epsilon >0\,\exists N_{1}\in \mathbb {N} \,\forall n\geq N_{1}:|S_{2n-1}-S|<\epsilon \\[0.5em]&\forall \epsilon >0\,\exists N_{2}\in \mathbb {N} \,\forall n\geq N_{2}:|S_{2n}-S|<\epsilon 
\end{align*}

Setzen wir nun $N=\max\{2N_{1}-1,2N_{2}\}$, so folgt

\begin{align*}
\forall \epsilon >0\,\exists N\in \mathbb {N} \,\forall n\geq N:|S_{n}-S|<\epsilon 
\end{align*}

Also konvergiert die Reihe $\sum _{k=1}^{\infty }(-1)^{k+1}b_{k}$.

\end{proof*}

\begin{example*}[Verallgemeinerte alternierende harmonische Reihe]
Die verallgemeinerte alternierende harmonische Reihe $\sum _{k=1}^{\infty }{\tfrac {(-1)^{k+1}}{k^{\alpha }}}$ konvergiert für alle $\alpha >0$ nach dem Leibniz-Kriterium, denn es gilt

\begin{itemize}
\item $b_{k}={\tfrac {1}{k^{\alpha }}}\geq 0$ für alle $k\in \mathbb {N} $.
\item $b_{k+1}={\tfrac {1}{(k+1)^{\alpha }}}{\overset {(k+1)^{\alpha }\geq k^{\alpha }}{\leq }}{\tfrac {1}{k^{\alpha }}}$ für alle $k\in \mathbb {N} $, also ist $(b_{k})$ monoton fallend.
\item $\lim _{k\to \infty }b_{k}=\lim _{k\to \infty }{\tfrac {1}{k^{\alpha }}}=0$, also ist $(b_{k})$ eine Nullfolge.
\end{itemize}

\end{example*}

\section{Anmerkungen zum Leibniz-Kriterium}

\begin{itemize}
\item Natürlich gilt das Leibniz-Kriterium auch für Reihen der Form $\sum _{k=1}^{\infty }(-1)^{k}b_{k}$. Denn diese unterscheiden sich nur durch die {''}umgedrehten{''} Vorzeichen. Der Beweis funktioniert ganz analog mit vertauschten Rollen von $(S_{2n-1})$ und $(S_{2n})$.
\item Ebenso gilt es für Reihen der Form $\sum _{k=0}^{\infty }(-1)^{k}b_{k}$ oder $\sum _{k=0}^{\infty }(-1)^{k+1}b_{k}$. Lass dich durch Indexverschiebungen nicht aus der Ruhe bringen!
\item Beachte, dass aus dem Leibniz-Kriterium nur die Konvergenz und nicht die absolute Konvergenz der Reihe folgt. Wie oben schon erwähnt, gibt es viele konvergente alternierende Reihen, die nicht absolut konvergieren. Ein Standardbeispiel ist wieder die alternierende harmonische Reihe $\sum _{k=1}^{\infty }{\frac {(-1)^{k+1}}{k}}$.
\item Im Gegensatz zu manch anderem Konvergenzkriterium kann aus dem Leibniz-Kriterium nie die Divergenz einer Reihe gefolgert werden. Besitzt eine Reihe nicht alle Eigenschaften, die das Kriterium fordert, heißt das nicht, dass die Reihe divergieren muss. Das Leibniz-Kriterium ist in diesen Fällen nicht anwendbar. 
\end{itemize}

\chapter{Anwendung der Konvergenzkriterien}

Viele Dozenten stellen gerne Aufgaben, in denen die Konvergenz oder Divergenz einer Reihe bewiesen werden muss. Auf dieser Seite sammeln wir einige Tipps und Tricks, die dir bei solchen Aufgaben helfen. Sie zeigen auch exemplarisch, wie erfahrene Mathematiker Konvergenzaufgaben angehen. Anschließend werden wir diese Vorgehensweisen an mehreren Anwendungsbeispielen veranschaulichen.

\section{Tipps zur Bestimmung des Konvergenzverhaltens}

\subsection{Trivialkriterium überprüfen}

Überprüfe zunächst, ob du das Trivialkriterium anwenden kannst. Dieses besagt, dass jede Reihe $\sum _{k=1}^{\infty }a_{k}$, bei der die Folge $(a_{k})_{k\in \mathbb {N} }$ nicht gegen null konvergiert, divergieren muss. Bestimme also zunächst den Grenzwert von $(a_{k})_{k\in \mathbb {N} }$. Wenn dieser Grenzwert nicht existiert oder wenn dieser Grenzwert ungleich null ist, dann divergiert die Reihe $\sum _{k=1}^{\infty }a_{k}$. Es kann allerdings vorkommen, dass die Bauart der Folge $(a_{k})_{k\in \mathbb {N} }$ zu kompliziert ist, um das Konvergenzverhalten ohne großen Aufwand feststellen zu können. In diesem Fall müssen wir auf die anderen Konvergenzkriterien zurückgreifen. Ist $(a_{k})_{k\in \mathbb {N} }$ eine Nullfolge, so müssen wir ebenfalls ein anderes Kriterium anwenden.

\subsection{Quotientenkriterium}

Bei Reihen der Form $\sum _{k=1}^{\infty }{\frac {a_{k}}{b_{k}}}$ lohnt sich oft eine Überprüfung mit dem Quotientenkriterium. Wenn $\limsup _{k\to \infty }\left|{\tfrac {\tfrac {a_{k+1}}{b_{k+1}}}{\tfrac {a_{k}}{b_{k}}}}\right|=\limsup _{k\to \infty }\left|{\tfrac {a_{k+1}b_{k}}{b_{k+1}a_{k}}}\right|<1$, dann konvergiert die Reihe nach dem Quotientenkriterium absolut. Ist $\liminf _{k\to \infty }\left|{\tfrac {a_{k+1}b_{k}}{b_{k+1}a_{k}}}\right|>1$, dann divergiert die Reihe. Gilt jedoch $\liminf _{k\to \infty }\left|{\tfrac {a_{k+1}b_{k}}{b_{k+1}a_{k}}}\right|=1=\limsup _{k\to \infty }\left|{\tfrac {a_{k+1}b_{k}}{b_{k+1}a_{k}}}\right|$, so ist keine Konvergenzaussage möglich und wir müssen andere Konvergenzkriterien in Betracht ziehen.

\subsection{Wurzelkriterium bei Reihen über Potenzen}

Bei einer Reihe über einer Potenz der Form $\sum _{k=1}^{\infty }a_{k}^{k}$ oder $\sum _{k=1}^{\infty }a_{k}^{k^{2}}$ ist oft das Wurzelkriterium hilfreich. Dieses besagt, dass die Reihe absolut konvergiert, wenn $\limsup _{k\to \infty }{\sqrt[{k}]{|a_{k}^{k}|}}=\limsup _{k\to \infty }|a_{k}|<1$ bzw. $\limsup _{k\to \infty }{\sqrt[{k}]{|a_{k}^{k^{2}}|}}=\limsup _{k\to \infty }|a_{k}^{k}|<1$ ist. Ist $\limsup _{k\to \infty }|a_{k}|>1$ bzw. $\limsup _{k\to \infty }|a_{k}^{k}|>1$, so divergiert die Reihe. Konvergiert die Folge $(a_{k})_{k\in \mathbb {N} }$ bzw. $(a_{k}^{k})_{k\in \mathbb {N} }$, so gelten die entsprechenden Aussagen mit $\lim $, statt $\limsup $. Ist der Limes Superior gleich $1$, dann ist das Wurzelkriterium, genau wie das Quotientenkriterium, nicht anwendbar.

\subsection{Leibnizkriterium bei alternierenden Reihen}

Bei alternierenden Folgen kann oft das Leibniz-Kriterium angewandt werden. Dieses besagt, dass jede Reihe der Form $\sum _{k=1}^{\infty }(-1)^{k}a_{k}$ oder der Form $\sum _{k=1}^{\infty }(-1)^{k+1}a_{k}$ konvergiert, wenn die Folge $(a_{k})_{k\in \mathbb {N} }$ eine monoton fallende Nullfolge ist. Wichtig ist, dass beide Eigenschaften (monoton fallend und Nullfolge) erfüllt sind. Ist $(a_{k})_{k\in \mathbb {N} }$ \emph{keine} Nullfolge, so divergiert die Reihe nach dem Trivialkriterium. Ist $(a_{k})_{k\in \mathbb {N} }$ eine Nullfolge, jedoch \emph{nicht} monton fallend, so kann die Reihe konvergieren oder divergieren. Dies muss mit einem der anderen Kriterien überprüft werden. Außerdem müssen wir beachten, dass wir mit dem Leibniz-Kriterium nur die Konvergenz und nicht die absolute Konvergenz der Reihe folgern können. Diese muss zusätzlich noch gezeigt bzw. widerlegt werden. Oftmals ist das mit dem Majoranten- bzw. Minorantenkriterium möglich.

\subsection{Majoranten- und Minorantenkriterium}

Die beiden Kriterien lassen sich häufig bei Reihen der Form $\sum _{k=1}^{\infty }{\tfrac {P(k)}{Q(k)}}$ anwenden, wobei $P$ und $Q$ Polynomfunktionen sind. Das Quotienten- sowie das Wurzelkriterium versagen bei Reihen dieser Form. Als Majorante eignet sich häufig die konvergente Reihe $\sum _{k=1}^{\infty }{\tfrac {1}{k^{2}}}$ und als divergente Minorante die harmonische Reihe $\sum _{k=1}^{\infty }{\tfrac {1}{k}}$. Ansonsten ist auch jede Reihe $\sum _{k=1}^{\infty }{\tfrac {1}{k^{\alpha }}}$ mit $\alpha >1$ als Majorante und jede Reihe $\sum _{k=1}^{\infty }{\tfrac {1}{k^{\alpha }}}$ mit $\alpha \leq 1$ als Minorante geeignet. Um eine geeignete Majorante zu finden, müssen wir den Zähler $P(k)$ nach oben und den Nenner $Q(k)$ nach unten abschätzen. Um eine geeignete Minorante zu finden, funktioniert es genau umgekehrt. Als Anhaltspunkt, ob die Reihe konvergiert oder divergiert, gilt die folgende Merkregel: Ist ${\text{grad}}(P)<{\text{grad}}(Q)-1$, so konvergiert die Reihe und wir können das Majorantenkriterium anwenden. Gilt hingegen ${\text{grad}}(P)\geq {\text{grad}}(Q)-1$, so divergiert die Reihe. In diesem Fall können wir das Minorantenkriterium anwenden. Dabei bezeichnet ${\text{grad}}(P)$ bzw. ${\text{grad}}(Q)$ die gößte Potenz des Polynoms $P$ bzw. $Q$.

\section{Anwendungsbeispiele}

\subsection{Anwendungsbeispiel 2}

Als nächstes sehen wir uns die Reihe

\begin{align*}
\sum _{k=1}^{\infty }{\frac {k^{k}}{2^{k^{2}}}}
\end{align*}

an. Hier ist

\begin{align*}
(a_{k})_{k\in \mathbb {N} }=\left({\frac {k^{k}}{2^{k^{2}}}}\right)_{k\in \mathbb {N} }
\end{align*}

Diese Folge ist eine Nullfolge. Dies ist nicht so offensichtlich, wird aber klar, wenn wir $a_{k}={\tfrac {k^{k}}{2^{k^{2}}}}=\left({\tfrac {k}{2^{k}}}\right)^{k}$ schreiben. Bedenken wir nun, dass $\lim _{k\to \infty }{\tfrac {k}{2^{k}}}=0$ ist, so folgt auch $\lim _{k\to \infty }a_{k}=\lim _{k\to \infty }\left({\tfrac {k}{2^{k}}}\right)^{k}=0$. Alternierend ist die Folge wieder nicht, da alle Glieder positiv sind. Es handelt sich wieder um eine Quotientenfolge. Allerdings besteht der Nenner aus dem Term $2^{k^{2}}$. Durch Ziehen der $k$-ten Wurzel vereinfacht sich dieser zu $2^{k}$. Ebenso vereinfacht sich der Zähler nach dem Wurzelziehen. Daher wenden wir hier das \emph{Wurzelkriterium} an. Es gilt

\begin{align*}
{\sqrt[{k}]{|a_{k}|}}&={\sqrt[{k}]{\frac {k^{k}}{2^{k^{2}}}}}\\[0.5em]&={\sqrt[{k}]{\left({\frac {k}{2^{k}}}\right)^{k}}}\\[0.5em]&={\frac {k}{2^{k}}}
\end{align*}

Wegen

\begin{align*}
\lim _{k\to \infty }{\frac {k}{2^{k}}}=0<1
\end{align*}

konvergiert die Reihe $\sum _{k=1}^{\infty }{\tfrac {k^{k}}{2^{k^{2}}}}$ nun (absolut).

\subsection{Anwendungsbeispiel 3}

Als Drittes untersuchen wir die Reihe

\begin{align*}
\sum _{k=1}^{\infty }{\frac {(-1)^{k}}{2k-1}}
\end{align*}

auf Konvergenz. Wieder ist

\begin{align*}
(a_{k})_{k\in \mathbb {N} }=\left({\frac {(-1)^{k}}{2k-1}}\right)_{k\in \mathbb {N} }
\end{align*}

eine Nullfolge. Denn $\lim _{k\to \infty }|a_{k}|=\lim _{k\to \infty }{\tfrac {1}{2k-1}}=0$. Da die Folge alternierend ist, bietet sich das \emph{Leibniz-Kriterium} an. Die Nullfolgeneigenschaft von $(b_{k})_{k\in \mathbb {N} }=\left({\tfrac {1}{2k-1}}\right)_{k\in \mathbb {N} }$ haben wir uns schon überlegt. Als nächstes müssen wir überprüfen, ob $(b_{k})_{k\in \mathbb {N} }$ auch monoton fallend ist. Es gilt für alle $k\in \mathbb {N} $:

\begin{align*}
&2k+1\geq 2k-1\\[0.5em]\iff &\underbrace {\frac {1}{2k+1}} _{=a_{k+1}}\leq \underbrace {\frac {1}{2k-1}} _{=a_{k}}
\end{align*}

Also ist $\left({\tfrac {1}{2k-1}}\right)_{k\in \mathbb {N} }$ eine monoton fallende Nullfolge und die Reihe $\sum _{k=1}^{\infty }{\tfrac {(-1)^{k}}{2k-1}}$ konvergiert. Wir müssen sie nun noch auf \emph{absolute} Konvergenz untersuchen. Dazu überlegen wir, ob $\sum _{k=1}^{\infty }\left|{\tfrac {(-1)^{k}}{2k-1}}\right|=\sum _{k=1}^{\infty }{\tfrac {1}{2k-1}}$ konvergiert. Diese kann allerdings nicht konvergieren, denn es gilt

\begin{align*}
2k-1\leq 2k\iff {\frac {1}{2k-1}}\geq {\frac {1}{2k}}
\end{align*}

Da die harmonische Reihe $\sum _{k=1}^{\infty }{\tfrac {1}{k}}$ divergiert, divergiert auch die Reihe $\sum _{k=1}^{\infty }{\tfrac {1}{2k}}=\sum _{k=1}^{\infty }{\tfrac {1}{2}}\cdot {\tfrac {1}{k}}$. Nach dem \emph{Minorantenkriterium} divergiert somit die Reihe $\sum _{k=1}^{\infty }{\tfrac {1}{2k-1}}$. Unsere Reihe $\sum _{k=1}^{\infty }{\tfrac {(-1)^{k}}{2k-1}}$ konvergiert daher \emph{nicht} absolut.

\subsection{Anwendungsbeispiel 5}

Als Letztes betrachten wir noch die Reihe

\begin{align*}
\sum _{k=1}^{\infty }(-1)^{k}\left({\frac {k}{k+1}}\right)
\end{align*}

Dieses Beispiel soll veranschaulichen, dass bei alternierenden Reihen nicht immer zwangsläufig das Leibniz-Kriterium angewendet werden kann. Der Grund liegt in diesem Fall darin, dass die Koeffizientenfolge $(a_{k})_{k\in \mathbb {N} }$ mit $a_{k}=(-1)^{k}\left({\tfrac {k}{k+1}}\right)$ \emph{keine} Nullfolge ist. Es gilt nämlich

\begin{align*}
|a_{k}|={\frac {k}{k+1}}={\frac {1}{1+{\frac {1}{k}}}}{\overset {k\to \infty }{\to }}{\frac {1}{1+0}}=1
\end{align*}

Also ist $(|a_{k}|)_{k\in \mathbb {N} }$ keine Nullfolge und damit gilt dies auch für $(a_{k})_{k\in \mathbb {N} }$. Die Reihe $\sum _{k=1}^{\infty }(-1)^{k}\left({\tfrac {k}{k+1}}\right)$ divergiert daher nach dem \emph{Trivialkriterium}.

\part{Stetigkeit von Funktionen}

\addxcontentsline{lof}{part}[\arabic{part}]{Stetigkeit von Funktionen}\begin{authors}
Stephan Kulla, Michael D'Erchie, Who2010, Mrvnfrtz, Sven Prüfer, GraffL, Chris ShuYu Dong, Alexander Sedlmayr, Beezle73, Benjamin Wolba, Matheoldie, S jwiese, 0-Brane, Autorenkollektiv „Auswahlaxiom“ (Charlotte Dietze, Matthias Paulsen, Anne Reif), Farbstift Rot, Claudia Renner, Florianwicher, BenniSERLO, Jenny Kilian, Katharina Kircher, Meitnerium266, Oztafankolibril, Werner Fröhlich, Deconimus, OmaMath, SerloBot, DominikJW, Hydro\end{authors}

\chapter{Folgenkriterium}

\section{Motivation und Herleitung}

\subsection{Erste Beispiele}

Betrachte den Grenzwert $\lim _{n\to \infty }\exp \left({\tfrac {1}{n}}\right)$. In der Schule würde man diesen Grenzwert folgendermaßen ausrechnen:

\begin{align*}
\lim _{n\to \infty }\exp \left({\frac {1}{n}}\right)&=\exp \left(\lim _{n\to \infty }{\frac {1}{n}}\right)\\[0.5em]&{\color {OliveGreen}\left\downarrow \ \lim _{n\to \infty }{\frac {1}{n}}=0\right.}\\[0.5em]&=\exp(0)\\&=1
\end{align*}

Diese Rechnung ergibt intuitiv Sinn: Wenn ${\tfrac {1}{n}}\to 0$, dann sollte $\exp \left({\tfrac {1}{n}}\right)\to \exp(0)$ sein. Doch können wir so argumentieren? Ist es erlaubt, den Limes in die Funktion reinzuziehen? Betrachte hierzu die Vorzeichenfunktion $\operatorname {sgn}(x)$, welche das Vorzeichen von $x$ zurückgibt:

\begin{align*}
\operatorname {sgn}(x)={\begin{cases}1&x>0\\0&x=0\\-1&x<0\end{cases}}
\end{align*}

Wegen ${\begin{aligned}{\tfrac {1}{n}}>0\end{aligned}}$ gilt:

\begin{align*}
\lim _{n\to \infty }\operatorname {sgn} \left({\tfrac {1}{n}}\right)=\lim _{n\to \infty }1=1\neq 0=\operatorname {sgn}(0)=\operatorname {sgn} \left(\lim _{n\to \infty }{\frac {1}{n}}\right)
\end{align*}

Also ist $\lim _{n\to \infty }\operatorname {sgn} \left({\tfrac {1}{n}}\right)\neq \operatorname {sgn} \left(\lim _{n\to \infty }{\tfrac {1}{n}}\right)$. Dies zeigt, dass man den Limes nicht ohne Weiteres in eine Funktion hineinziehen kann. Im Funktionsplot sieht man, warum dies bei $\exp(x)$, jedoch nicht bei $\operatorname {sgn}(x)$ möglich ist. Bei $\exp(x)$ konvergiert nämlich die Folge $\left(\exp \left({\tfrac {1}{n}}\right)\right)_{n\in \mathbb {N} }$ gegen $\exp(0)$, wenn $n\to \infty $ geht:

\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Continuity_of_the_Exponential_at_0.svg}{\textbf{Continuity\allowbreak\_of\allowbreak\_the\allowbreak\_Exponential\allowbreak\_at\allowbreak\_0.svg}} by Stephan Kulla \textit{(CC BY-SA 4.0)}}\begin{center}
\adjincludegraphics[max width=0.5\textwidth,max height=0.2\textheight]{file58continuity95of95the95exponential95at9509548ec7b8a7588a53fa1929474eac8dd91122a682f}\end{center}

Bei der Vorzeichenfunktion gibt es einen Sprung im Graphen bei $x=0$, und deswegen konvergiert die Folge $\left(\operatorname {sgn} \left({\tfrac {1}{n}}\right)\right)_{n\in \mathbb {N} }$ nicht gegen $\operatorname {sgn}(0)$:

\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Discontinuity_of_the_sign_function_at_0.svg}{\textbf{Discontinuity\allowbreak\_of\allowbreak\_the\allowbreak\_sign\allowbreak\_function\allowbreak\_at\allowbreak\_0.svg}} by Stephan Kulla \textit{(CC BY-SA 4.0)}}\begin{center}
\adjincludegraphics[max width=0.5\textwidth,max height=0.2\textheight]{file58discontinuity95of95the95sign95function95at95095becf67e1468a7a0b5546efca55eb2d7fed61f9e0}\end{center}

Wir stellen fest: Es gibt Funktionen, bei denen der Limes hineingezogen werden kann, und Funktionen, bei denen es nicht (immer) geht.

\subsection{Herleitung des Folgenkriteriums}

Fassen wir das bisher Gefundene zusammen:

\begin{importantparagraph*}
Wenn eine Funktion $f:D\to \mathbb {R} $ an der Stelle $x\in D$ einen Sprung besitzt, gibt es mindestens eine Folge $(x_{n})_{n\in \mathbb {N} }$ von Argumenten mit $\lim _{n\to \infty }x_{n}=x$ und $\lim _{n\to \infty }f(x_{n})\neq f(x)$.

\end{importantparagraph*}

Sprich: Im Fall eines Sprungs an der Stelle $x\in D$ gilt $\lim _{n\to \infty }f(x_{n})\neq f(x)=f\left(\lim _{n\to \infty }x_{n}\right)$ für mindestens eine Folge $(x_{n})_{n\in \mathbb {N} }$ von Argumenten mit Grenzwert $x$. Nun ist nach unserer Intuition eine Funktion genau dann unstetig an einer Stelle, wenn ihr Graph dort einen Sprung macht. Wir können also definieren:

\begin{importantparagraph*}
Der Graph einer Funktion $f:D\to \mathbb {R} $ ist unstetig an der Stelle $x\in D$, wenn es mindestens eine Folge $(x_{n})_{n\in \mathbb {N} }$ von Argumenten mit $\lim _{n\to \infty }x_{n}=x$ und $\lim _{n\to \infty }f(x_{n})\neq f(x)$ gibt.

\end{importantparagraph*}

Um die Definition der Stetigkeit an einer Stelle zu finden, müssen wir die Negation der obigen Aussage nehmen. Nach dieser Überlegung können wir uns die Stetigkeit an einer Stelle als Abwesenheit eines Sprungs an der betrachteten Stelle vorstellen, und wir erhalten:

\begin{importantparagraph*}
Eine Funktion $f:D\to \mathbb {R} $ ist an der Stelle $x\in D$ stetig, wenn für alle Folgen $(x_{n})_{n\in \mathbb {N} }$ aus $D$ mit $\lim _{n\to \infty }x_{n}=x$ gilt:

\begin{align*}
\lim _{n\to \infty }f(x_{n})=f(x)=f\left(\lim _{n\to \infty }x_{n}\right)
\end{align*}

\end{importantparagraph*}

Bei Stetigkeit einer Funktion an einer Stelle kann der Limes hineingezogen werden, wenn die Argumentenfolge gegen diese Stelle konvergiert. Diese Definition der Stetigkeit ist das Folgenkriterium der Stetigkeit. Nun ist eine Funktion genau dann stetig, wenn sie an jeder Stelle ihres Definitonsbereichs stetig ist. Damit erhalten wir für die Stetigkeit einer Funktion:

\begin{importantparagraph*}
Eine Funktion $f:D\to \mathbb {R} $ ist stetig, wenn für alle konvergenten Folgen $(x_{n})_{n\in \mathbb {N} }$ gilt $\lim _{n\to \infty }f(x_{n})=f(\lim _{n\to \infty }x_{n})$. Dabei müssen alle Folgenglieder $x_{n}$ und der Grenzwert $\lim _{n\to \infty }x_{n}$ Elemente des Definitionsbereiches $D$ von $f$ sein, damit die Gleichung $\lim _{n\to \infty }f(x_{n})=f\left(\lim _{n\to \infty }x_{n}\right)$ Sinn ergibt.

\end{importantparagraph*}

Wir können zusammenfassen: Bei einer stetigen Funktion kann man den Limes in die Funktion hineinziehen – unabhängig vom Grenzwert der Argumentenfolge. Weil beispielsweise die Exponentialfunktion stetig ist, kann man immer den Limes in diese Funktion hineinziehen. Die Vorzeichenfunktion ist bei $x=0$ unstetig, und damit kann der Limes nicht in die Funktion gezogen werden, wenn die Argumentenfolge gegen null konvergiert.

\section{Definition}

Im obigen Abschnitt haben wir bereits die Definition der Stetigkeit kennengelernt. Hier haben wir festgelegt:



\begin{definition*}[Folgenkriterium der Stetigkeit an einer Stelle]
Eine Funktion $f:D\to \mathbb {R} $ mit $D\subseteq \mathbb {R} $ ist stetig an der Stelle $x_{0}\in D$, wenn für alle Folgen $(x_{n})_{n\in \mathbb {N} }$ mit $\forall n\in \mathbb {N} :x_{n}\in D$ und $\lim _{n\to \infty }x_{n}=x_{0}$ gilt:

\begin{align*}
\lim _{n\to \infty }f(x_{n})=f\left(\lim _{n\to \infty }x_{n}\right)=f(x_{0})
\end{align*}

\end{definition*}



Darauf aufbauend haben wir definiert, dass eine Funktion stetig ist, wenn sie an jeder Stelle stetig ist:



\begin{definition*}[Folgenkriterium der Stetigkeit]
Eine Funktion $f:D\to \mathbb {R} $ mit $D\subseteq \mathbb {R} $ ist stetig, wenn \emph{für alle} $x\in D$ und für alle Folgen $(x_{n})_{n\in \mathbb {N} }$ mit $\forall n\in \mathbb {N} :x_{n}\in D$ und $\lim _{n\to \infty }x_{n}=x$ gilt:

\begin{align*}
\lim _{n\to \infty }f(x_{n})=f\left(\lim _{n\to \infty }x_{n}\right)=f(x)
\end{align*}

\end{definition*}



Stetigkeit garantiert uns also, dass wir den Limes in die Funktion hineinziehen können. Dies kann die Grenzwertberechnung ungemein vereinfachen, und somit ist Stetigkeit ein Konzept, auf das man bei Grenzwertberechnungen stößt.

\chapter{Epsilon-Delta-Kriterium}

Das Epsilon-Delta-Kriterium ist neben dem Folgenkriterium eine weitere Variante, die \href{https://de.wikibooks.org/wiki/Mathe\_für\_Nicht-Freaks:\_Stetigkeit\_von\_Funktionen}
{Stetigkeit einer Funktion} zu definieren. Sie umschreibt die charakteristische Eigenschaft stetiger Funktionen, dass hinreichend kleine Änderungen des Arguments beliebig kleine Änderungen im Funktionswert verursachen.

\section{Motivation}

Zu Beginn des Kapitels haben wir gelernt, dass die Stetigkeit einer Funktion zumindest vereinfacht als Abwesenheit von Sprüngen interpretiert werden kann. An einer stetigen Stelle ändern sich die Funktionswerte also beliebig wenig, wenn nur das Argument hinreichend wenig geändert wird. Es gilt also $f(x)\approx f(x_{0})$, wenn $x$ hinreichend nah an $x_{0}$ liegt. Solche Funktionswerte $f(x)$ können zur Annäherung von $f(x_{0})$ herangezogen werden.

\subsection{Stetigkeit bei Approximation von Funktionswerten}

Hat eine Funktion keine Sprünge, kann man ihre Funktionswerte durch umliegende Werte approximieren. Für diese Annäherung und somit auch für den Beweis der Stetigkeit verwenden wir das Epsilon-Delta-Kriterium stetiger Funktionen. Doch was bedeutet das genau?

Nehmen wir an, wir führen ein Experiment durch, bei dem wir die Lufttemperatur messen wollen. Sei $f$ die Funktion für den Temperaturverlauf. $f(x)$ ist also die Temperatur zum Zeitpunkt $x$. Aufgrund eines technischen Fehlers fehlt uns ein bestimmter Wert $f(x_{0})$, den wir nun möglichst genau approximieren wollen:

\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Illustration_for_epsilon-delta_definition_of_continuity_2.svg}{\textbf{Illustration\allowbreak\_for\allowbreak\_epsilon\allowbreak-delta\allowbreak\_definition\allowbreak\_of\allowbreak\_continuity\allowbreak\_2.svg}} by Lukasstockner, Stephan Kulla \textit{(CC0)}}\begin{center}
\adjincludegraphics[max width=0.5\textwidth,max height=0.2\textheight]{file58illustration95for95epsilon45delta95definition95of95continuity952953c173e7c64e4ba482329c264372252fa317850f6}\end{center}

Durch den technischen Fehler war die direkte Messung von $f(x_{0})$ nicht möglich. Weil sich der Temperaturverlauf kontinuierlich ändert und es damit insbesondere zum Zeitpunkt $x_{0}$ keinen Sprung im Temperaturverlauf gibt, können wir ersatzweise die Temperatur zeitnah an $x_{0}$ bestimmen. Wir nähern also den Wert $f(x_{0})$ an, indem wir eine Temperatur $f(x)$ bestimmen, bei der der Zeitpunkt $x$ nah an $x_{0}$ liegt. $f(x)$ ist dann eine Annäherung von $f(x_{0})$. Wie nah muss hierzu $x$ an $x_{0}$ liegen?

Nehmen wir an, dass sich für die spätere Auswertung die gemessene Temperatur maximal um den Fehler $\epsilon =0{,}1\ \mathrm {^{\circ }C} $ von der tatsächlichen Temperatur unterscheiden darf. Unser Messwert muss sich also im grau hinterlegten Bereich der folgenden Grafik befinden. Das sind alle Punkte, deren Funktionswerte zwischen $f(x_{0})-\epsilon $ und $f(x_{0})+\epsilon $ liegen, die sich also im offenen Intervall $(f(x_{0})-\epsilon ,f(x_{0})+\epsilon )$ befinden:

\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Illustration_for_epsilon-delta_definition_of_continuity_3.svg}{\textbf{Illustration\allowbreak\_for\allowbreak\_epsilon\allowbreak-delta\allowbreak\_definition\allowbreak\_of\allowbreak\_continuity\allowbreak\_3.svg}} by Lukasstockner, Stephan Kulla \textit{(CC0)}}\begin{center}
\adjincludegraphics[max width=0.5\textwidth,max height=0.2\textheight]{file58illustration95for95epsilon45delta95definition95of95continuity953955444c5bcb22f9827677adf147c49630779428116}\end{center}

In der Graphik sehen wir, dass es um $x_{0}$ einen Bereich gibt, in dem sich die Funktionswerte maximal um $\epsilon $ von $f(x_{0})$ unterscheiden. Es gibt also einen Zeitabstand $\delta $, so dass alle Funktionswerte mit Argumenten im Intervall $(x_{0}-\delta ,x_{0}+\delta )$im grau hinterlegten Bereich liegen:

\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Illustration_for_epsilon-delta_definition_of_continuity_4.svg}{\textbf{Illustration\allowbreak\_for\allowbreak\_epsilon\allowbreak-delta\allowbreak\_definition\allowbreak\_of\allowbreak\_continuity\allowbreak\_4.svg}} by Lukasstockner, Stephan Kulla \textit{(CC0)}}\begin{center}
\adjincludegraphics[max width=0.5\textwidth,max height=0.2\textheight]{file58illustration95for95epsilon45delta95definition95of95continuity9549535c2a2ae77a16aebafd2d9bb10de01d83ac6356d}\end{center}

Es ist also möglich, unseren gesuchten Wert $f(x_{0})$ ausreichend gut (sprich mit einem Maximalfehler von $\epsilon $) zu approximieren. Wenn wir nämlich einen Zeitpunkt $x$ mit einem Abstand von $x_{0}$ kleiner als den Zeitabstand $\delta $ wählen, so ist der Abstand von $f(x)$ zu $f(x_{0})$ kleiner als der geforderte Maximalabstand $\epsilon $. Wir können so $f(x)$ als Annäherung von $f(x_{0})$ wählen.

\begin{importantparagraph*}
Zusammenfassung: Es gibt ein $\delta >0$, so dass der Abstand $|f(x)-f(x_{0})|$ kleiner als $\epsilon $ für $|x-x_{0}|$ kleiner als $\delta $ ist. Also: $|x-x_{0}|<\delta \implies |f(x)-f(x_{0})|<\epsilon $

\end{importantparagraph*}

\subsection{Erhöhte Anforderungen an die Approximation}

Was passiert, wenn wir bei unserer Messung aufgrund erhöhter Anforderungen an die Auswertung den Temperaturwert besser kennen müssen? Was ist, wenn beispielsweise der geforderte Maximalfehler der Temperaturmessung nun $\epsilon _{2}=0{,}05\ \mathrm {^{\circ }C} $ und nicht mehr $\epsilon =0{,}1\ \mathrm {^{\circ }C} $ ist?

\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Illustration_for_epsilon-delta_definition_of_continuity_5.svg}{\textbf{Illustration\allowbreak\_for\allowbreak\_epsilon\allowbreak-delta\allowbreak\_definition\allowbreak\_of\allowbreak\_continuity\allowbreak\_5.svg}} by Lukasstockner, Stephan Kulla \textit{(CC0)}}\begin{center}
\adjincludegraphics[max width=0.5\textwidth,max height=0.2\textheight]{file58illustration95for95epsilon45delta95definition95of95continuity9559590855c5869b07d09b7dbc4fbb5db44d22cfde292}\end{center}

Auch in diesem Fall gibt es einen Bereich um $x_{0}$, in dem sich die Funktionswerte weniger als $\epsilon _{2}$ von $f(x_{0})$ unterscheiden. Es gibt also ein $\delta _{2}>0$, so dass sich $f(x)$ um maximal $\epsilon _{2}$ von $f(x_{0})$ unterscheidet, wenn $|x-x_{0}|<\delta _{2}$ ist:

\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Illustration_for_epsilon-delta_definition_of_continuity_6.svg}{\textbf{Illustration\allowbreak\_for\allowbreak\_epsilon\allowbreak-delta\allowbreak\_definition\allowbreak\_of\allowbreak\_continuity\allowbreak\_6.svg}} by Lukasstockner, Stephan Kulla \textit{(CC0)}}\begin{center}
\adjincludegraphics[max width=0.5\textwidth,max height=0.2\textheight]{file58illustration95for95epsilon45delta95definition95of95continuity9569546c06d6437925e92398704035f06743ea556ec49}\end{center}

Egal wie klein $\epsilon $ gewählt wird, es kann wegen des kontinuierlichen Temperaturverlaufes immer ein $\delta >0$ gefunden werden, so dass sich $f(x)$ um maximal $\epsilon $ von $f(x_{0})$ unterscheidet, wenn der Abstand von $x$ zu $x_{0}$ kleiner als $\delta $ ist. Es gilt:

\begin{importantparagraph*}
Egal welchen Maximalfehler $\epsilon >0$ wir vorgeben, es gibt immer einen Bereich um $x_{0}$ in der Form $(x_{0}-\delta ,x_{0}+\delta )$ mit $\delta >0$, in der die Funktionswerte einen Abstand kleiner als $\epsilon $ von $f(x_{0})$ entfernt liegen.

\end{importantparagraph*}

Der obige Umstand ist deswegen erfüllt, weil die Funktion $f$ bei $x_{0}$ kontinuierlich verläuft und keinen Sprung macht oder – anders formuliert – weil die Funktion $f$ an der Stelle $x_{0}$ stetig ist. Es gilt sogar mehr: Dieser Umstand charakterisiert auf eine formale Art die Tatsache, dass es bei $x_{0}$ keinen Sprung im Funktionsgraphen von $f$ gibt. Wir können ihn also als formale Definition der Stetigkeit nutzen. Wegen der auftretenden Variablen $\epsilon $ und $\delta $ wird diese Definition das \emph{Epsilon-Delta-Kriterium der Stetigkeit} genannt.

\subsection{Epsilon-Delta-Kriterium der Stetigkeit}

Warum gilt das Epsilon-Delta-Kriterium genau dann, wenn der Funktionsgraph an der entsprechenden Stelle keinen Sprung macht (also an dieser Stelle stetig ist)? Am Beispiel des Temperaturverlaufs konnten wir intuitiv nachvollziehen, dass das Epsilon-Delta-Kriterium bei stetigen Funktionen erfüllt ist. Ist es auch so, dass bei Sprüngen an einer Stelle im Funktionsgraphen das Epsilon-Delta-Kriterium nicht erfüllt ist? Nehmen wir nun hypothetisch an, dass der Temperaturverlauf an der Stelle $x_{0}$ einen Sprung macht:

\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Illustration_for_epsilon-delta_definition_of_continuity_7.svg}{\textbf{Illustration\allowbreak\_for\allowbreak\_epsilon\allowbreak-delta\allowbreak\_definition\allowbreak\_of\allowbreak\_continuity\allowbreak\_7.svg}} by Lukasstockner, Stephan Kulla \textit{(CC0)}}\begin{center}
\adjincludegraphics[max width=0.5\textwidth,max height=0.2\textheight]{file58illustration95for95epsilon45delta95definition95of95continuity95795ea121b812fa7bfe8ccf7ec3a6926c8b69fb0169d}\end{center}

Sei nun $\epsilon $ ein Maximalfehler, der kleiner als die Sprungweite ist:

\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Illustration_for_epsilon-delta_definition_of_continuity_8.svg}{\textbf{Illustration\allowbreak\_for\allowbreak\_epsilon\allowbreak-delta\allowbreak\_definition\allowbreak\_of\allowbreak\_continuity\allowbreak\_8.svg}} by Lukasstockner, Stephan Kulla \textit{(CC0)}}\begin{center}
\adjincludegraphics[max width=0.5\textwidth,max height=0.2\textheight]{file58illustration95for95epsilon45delta95definition95of95continuity9589552f956f413b49470c8fe01643ebbc9d7c760b492}\end{center}

Dann können wir keinen $\delta $-Bereich $(x_{0}-\delta ,x_{0}+\delta )$ um $x_{0}$ finden, in dem alle Funktionswerte einen Abstand kleiner als $\epsilon $ von $f(x_{0})$ besitzen. Wenn wir beispielsweise das folgende $\delta $ wählen, dann gibt es ein $x$ zwischen $x_{0}-\delta $ und $x_{0}+\delta $, welches einen Abstand größer als $\epsilon $ von $f(x_{0})$ besitzt:

\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Illustration_for_epsilon-delta_definition_of_continuity_9.svg}{\textbf{Illustration\allowbreak\_for\allowbreak\_epsilon\allowbreak-delta\allowbreak\_definition\allowbreak\_of\allowbreak\_continuity\allowbreak\_9.svg}} by Lukasstockner, Stephan Kulla \textit{(CC0)}}\begin{center}
\adjincludegraphics[max width=0.5\textwidth,max height=0.2\textheight]{file58illustration95for95epsilon45delta95definition95of95continuity95995d1fbb8243bcf16d37c15ae9f4f106464f6c56d5c}\end{center}

Auch wenn wir ein kleineres $\delta _{2}$ wählen, findet sich ein $x\in (x_{0}-\delta _{2},x_{0}+\delta _{2})$ mit $|f(x)-f(x_{0})|\geq \epsilon $:

\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Illustration_for_epsilon-delta_definition_of_continuity_10.svg}{\textbf{Illustration\allowbreak\_for\allowbreak\_epsilon\allowbreak-delta\allowbreak\_definition\allowbreak\_of\allowbreak\_continuity\allowbreak\_10.svg}} by Lukasstockner, Stephan Kulla \textit{(CC0)}}\begin{center}
\adjincludegraphics[max width=0.5\textwidth,max height=0.2\textheight]{file58illustration95for95epsilon45delta95definition95of95continuity9510956f0259b29f4aaf86b46867c254315191c8fbc064}\end{center}

Egal wie klein $\delta $ ist, es gibt immer mindestens ein Argument $x$ mit einen Abstand kleiner als $\delta $ von $x_{0}$, dessen Funktionswert $f(x)$ sich mehr als $\epsilon $ um $f(x_{0})$ unterscheidet. So sehen wir intuitiv, dass das Epsilon-Delta-Kriterium bei Sprüngen im Graphen nicht erfüllt ist. Damit charakterisiert das Epsilon-Delta-Kriterium die Tatsache, dass der Funktionsgraph an der betrachteten Stelle keinen Sprung macht. Es ist eine Definition der Stetigkeit. Da in diesem Kriterium nur bereits definierte mathematische Begriffe verwendet werden, genügt es den Anforderungen einer formalen Definition.

\section{Definition}

\subsection{Epsilon-Delta-Kriterium der Stetigkeit}

Die $\epsilon $-$\delta $ Definition der Stetigkeit an einer Stelle $x_{0}$ im Definitionsbereich lautet:



\begin{definition*}[Epsilon-Delta-Definition der Stetigkeit]
Eine Funktion $f:D\to \mathbb {R} $ mit $D\subseteq \mathbb {R} $ ist genau dann stetig an der Stelle $x_{0}\in D$, wenn es für alle $\epsilon >0$ ein $\delta >0$ gibt, so dass $|f(x)-f(x_{0})|<\epsilon $ für alle $x\in D$ mit $|x-x_{0}|<\delta $ erfüllt ist. $f$ ist also genau dann in $x_{0}\in D$ stetig, wenn gilt

\begin{align*}
\forall \epsilon >0\,\exists \delta >0\,\forall x\in D:|x-x_{0}|<\delta \implies |f(x)-f(x_{0})|<\epsilon 
\end{align*}

\end{definition*}



Erläuterung der Quantorenschreibweise:

\begin{align*}
{\begin{array}{l}\underbrace {{\underset {}{}}\forall \epsilon >0} _{{\text{Für alle }}\epsilon >0}\underbrace {{\underset {}{}}\exists \delta >0} _{{\text{ gibt es ein }}\delta >0}\underbrace {{\underset {}{}}\forall x\in D} _{{\text{, so dass für alle }}x\in D}\\[1em]\quad \underbrace {{\underset {}{}}|x-x_{0}|<\delta } _{{\text{ mit Abstand von }}x_{0}{\text{ kleiner }}\delta }\underbrace {{\underset {}{}}\implies } _{\text{ gilt}}\underbrace {{\underset {}{}}|f(x)-f(x_{0})|<\epsilon } _{{\text{, dass der Abstand von }}f(x){\text{ zu }}f(x_{0}){\text{ kleiner als }}\epsilon {\text{ ist}}}\end{array}}
\end{align*}

Die obige Definition beschreibt die Stetigkeit an einem Punkt. Eine Funktion $f:D\to \mathbb {R} $ nennt man stetig, wenn sie an jedem Punkt in ihrem Definitionsbereich nach dem Epsilon-Delta-Kriterium stetig ist.

\subsection{Epsilon-Delta-Kriterium für Unstetigkeit}



\begin{definition*}[Epsilon-Delta-Definition der Unstetigkeit]
Eine Funktion $f:D\to \mathbb {R} $ mit $D\subseteq \mathbb {R} $ ist genau dann unstetig an der Stelle $x_{0}\in D$, wenn es ein $\epsilon >0$ gibt, so dass es für alle $\delta >0$ ein $x\in D$ mit $|x-x_{0}|<\delta $ und $|f(x)-f(x_{0})|\geq \epsilon $ gibt. $f$ ist also genau dann in $x_{0}\in D$ unstetig, wenn gilt

\begin{align*}
\exists \epsilon >0\,\forall \delta >0\,\exists x\in D:|x-x_{0}|<\delta \land |f(x)-f(x_{0})|\geq \epsilon 
\end{align*}

\end{definition*}



Erläuterung der Quantorenschreibweise:

\begin{align*}
{\begin{array}{l}\underbrace {{\underset {}{}}\exists \epsilon >0} _{{\text{Es gibt ein }}\epsilon >0,}\underbrace {{\underset {}{}}\forall \delta >0} _{{\text{ so dass für alle }}\delta >0}\underbrace {{\underset {}{}}\exists x\in D} _{{\text{ ein }}x\in D{\text{ existiert}}}\\[1em]\quad \underbrace {{\underset {}{}}|x-x_{0}|<\delta } _{{\text{ mit Abstand von }}x_{0}{\text{ kleiner }}\delta }\underbrace {{\underset {}{}}\land } _{\text{ und}}\underbrace {{\underset {}{}}|f(x)-f(x_{0})|\geq \epsilon } _{{\text{der Abstand von }}f(x){\text{ zu }}f(x_{0}){\text{ ist größer gleich }}\epsilon }\end{array}}
\end{align*}

\section{Erklärungen zum Epsilon-Delta-Kriterium}

Die Ungleichung $|x-x_{0}|<\delta $ bedeutet, dass der Abstand zwischen $x$ und $x_{0}$ kleiner als $\delta $ ist. Analog ist $|f(x)-f(x_{0})|<\epsilon $ gleichbedeutend damit, dass der Abstand zwischen $f(x)$ und $f(x_{0})$ kleiner als $\epsilon $ ist. Aus der Implikation $|x-x_{0}|<\delta \implies |f(x)-f(x_{0})|<\epsilon $ folgt damit, dass der Abstand zwischen $f(x)$ und $f(x_{0})$ garantiert kleiner als $\epsilon $ ist, wenn der Abstand zwischen $x$ und $x_{0}$ kleiner als $\delta $ ist. Die Epsilon-Delta-Definition der Stetigkeit kann somit auch folgendermaßen interpretiert werden:

\begin{importantparagraph*}
Egal wie klein man den Maximalabstand $\epsilon $ bezüglich des Funktionswertes $f(x)$ vorgibt, gibt es ein $\delta >0$, so dass der Abstand von $f(x)$ zu $f(x_{0})$ garantiert kleiner als $\epsilon $ ist, wenn $x$ einen Abstand kleiner als $\delta $ zu $x_{0}$ besitzt.

\end{importantparagraph*}

Bei stetigen Funktionen kann man also den Fehler – sprich den Abstand – in den Funktionswerten kontrollieren, indem man den Abstand in den Argumenten hinreichend klein hält. Die Suche nach dem $\delta $ entspricht der Beantwortung der Frage: Wie klein muss ich den Abstand im Argument wählen, damit der Abstand im Funktionswert maximal $\epsilon $ ist? Diese Frage ist durchaus relevant. Stell dir vor, du erhebst einen Messwert $x_{0}$ und berechnest damit einen Wert $f(x_{0})$ über eine stetige Funktion $f$. Dann kannst du einen Argumentenfehler $\delta $ bestimmen, der dir garantiert, dass der Endfehler der Berechnung $|f(x)-f(x_{0})|$ garantiert kleiner als $\epsilon $ ist, wenn der Fehler im Argument $|x-x_{0}|$ kleiner als $\delta $ ist.

Ein $\delta $ kann nur dann gefunden werden, wenn kleine Änderungen des Arguments $x_{0}$ kleine Änderungen des Funktionswertes $f(x_{0})$ verursachen. Bei stetigen Funktionen an der Stelle $x_{0}$ muss also gelten:

\begin{align*}
x\approx x_{0}\implies f(x)\approx f(x_{0})
\end{align*}

Diese Implikation muss man so lesen: Wenn $x$ hinreichend nah an $x_{0}$ liegt, dann ist $f(x)$ ungefähr $f(x_{0})$. Diese Tatsache kann auch mit dem Begriff der $\epsilon $-Umgebung beschrieben werden:

\begin{importantparagraph*}
Für jede noch so kleine $\epsilon $-Umgebung $(f(x_{0})-\epsilon ,f(x_{0})+\epsilon )$ um $f(x_{0})$ gibt es eine $\delta $-Umgebung $(x_{0}-\delta ,x_{0}+\delta )$ um $x_{0}$, deren Funktionswerte alle in der $\epsilon $-Umgebung liegen.

\end{importantparagraph*}

Diese Beschreibung wird in der Topologie weiter verallgemeinert und führt zur topologischen Definition der Stetigkeit.

\section{Visuelle Interpretation des Epsilon-Delta-Kriteriums}

\subsection{Beschreibung der Stetigkeit im Graphen}

Das Epsilon-Delta-Kriterium kann gut im Graphen visualisiert werden. Wir starten hier mit der Implikation $|x-x_{0}|<\delta \implies |f(x)-f(x_{0})|<\epsilon $. Nach ihr ist der Abstand von $f(x)$ zu $f(x_{0})$ kleiner als Epsilon, wenn der Abstand von $x$ zu $x_{0}$ kleiner als $\delta $ ist. Sprich: Für $x\in (x_{0}-\delta ,x_{0}+\delta )$ ist $f(x)\in (f(x_{0})-\epsilon ,f(x_{0})+\epsilon )$. Dies kann dadurch illustriert werden, dass der Punkt $(x,f(x))$ im Inneren des Rechtecks $(x_{0}-\delta ,x_{0}+\delta )\times (f(x_{0})-\epsilon ,f(x_{0})+\epsilon )$ liegt. Dabei ist $(x_{0}-\delta ,x_{0}+\delta )\times (f(x_{0})-\epsilon ,f(x_{0})+\epsilon )$ das Innere des Rechtecks mit Breite $2\delta $ und der Höhe $2\epsilon $ um den Mittelpunkt $(x_{0},f(x_{0}))$:

\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:2epsilon-2delta-rectangle.svg}{\textbf{2epsilon\allowbreak-2delta\allowbreak-rectangle.svg}} by Lukasstockner, Stephan Kulla \textit{(CC0)}}\begin{center}
\adjincludegraphics[max width=0.5\textwidth,max height=0.2\textheight]{file582epsilon452delta45rectangle951517195eb79d109fa3a50cc7a6c765d5772cdf20}\end{center}

Dieses Rechteck werden wir im Folgenden $2\epsilon $-$2\delta $-Rechteck nennen. Der Rand des Rechtecks gehört dabei nicht dazu. Nach dem Epsilon-Delta-Kriterium ist die Implikation $|x-x_{0}|<\delta \implies |f(x)-f(x_{0})|<\epsilon $ für alle Argumente $x$ erfüllt. Damit müssen alle Punkte des Graphen von $f$ eingeschränkt auf die Argumente im Intervall $(x_{0}-\delta ,x_{0}+\delta )$ im Inneren des $2\epsilon $-$2\delta $-Rechtecks liegen (grüner Bereich in der folgenden Zeichnung) und dürfen sich nicht oberhalb oder unterhalb des Rechtecks befinden (roter Bereich):

\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:2epsilon-2delta-rectangle_with_colored_areas.svg}{\textbf{2epsilon\allowbreak-2delta\allowbreak-rectangle\allowbreak\_with\allowbreak\_colored\allowbreak\_areas.svg}} by Lukasstockner, Stephan Kulla \textit{(CC0)}}\begin{center}
\adjincludegraphics[max width=0.5\textwidth,max height=0.2\textheight]{file582epsilon452delta45rectangle95with95colored95areas957bab6692503f32d1bad416b08c11b8f0d1d12e2b}\end{center}

Insgesamt kann das Epsilon-Delta-Kriterium folgendermaßen beschrieben werden:

\begin{importantparagraph*}
Für jedes $\epsilon >0$ gibt es ein (hinreichend kleines) $\delta >0$, so dass der Graph von $f$ eingeschränkt auf $(x_{0}-\delta ,x_{0}+\delta )$ komplett im Inneren des $2\epsilon $-$2\delta $-Rechtecks liegt.

\end{importantparagraph*}

\chapter{Komposition stetiger Funktionen}

Viele Funktionen sind als Verkettungen von anderen Funktionen definiert. Die direkte Überprüfung auf Stetigkeit mit Hilfe des Folgen- oder des Epsilon-Delta-Kriteriums ist bei diesen Funktionen oftmals aufwändig. Jedoch kann man beweisen, dass Verkettungen stetiger Funktionen wieder stetig sind. Diese Verkettungssätze erleichtern den Nachweis der Stetigkeit ungemein.

\section{Die Verkettungssätze}

Die Verkettungssätze für stetige Funktionen lauten:

\begin{theorem*}[Verkettungssätze]
Sei $D\subseteq \mathbb {R} $ eine Teilmenge der reellen Zahlen und $\lambda \in \mathbb {R} $ eine beliebige reelle Zahl. Seien $f,g:D\to \mathbb {R} $ reellwertige Funktionen, die in $a\in D$ stetig sind. Es gilt also $\lim _{x\to a}{f(x)}=f(a)$ und $\lim _{x\to a}{g(x)}=g(a)$. Unter diesen Voraussetzungen sind die folgenden Funktionen stetig in $a$:

\begin{itemize}
\item $f+g:D\to \mathbb {R} :x\mapsto f(x)+g(x)$
\item $\lambda f:D\to \mathbb {R} :x\mapsto \lambda \cdot f(x)$
\item $fg:D\to \mathbb {R} :x\mapsto f(x)\cdot g(x)$
\end{itemize}

Sei $D'=\{x\in D:g(x)\neq 0\}$ und seien $f$ sowie $g$ stetig in $a\in D'$. Dann ist die folgende Funktion stetig in $a$:

\begin{align*}
{\frac {f}{g}}:D'\to \mathbb {R} :x\mapsto {\frac {f(x)}{g(x)}}
\end{align*}

Sei $h:E\to \mathbb {R} $ eine reellwertige Funktion mit $f(D)\subseteq E$. Sei $h$ in $b=f(a)\in E$ stetig, dann ist die Verkettung $h\circ f$ stetig in $a$:

\begin{align*}
h\circ f:D\to \mathbb {R} :x\mapsto h(f(x))
\end{align*}

\end{theorem*}

\section{Beispielaufgabe}

Die folgende Aufgabe zeigt, wie einfach mit Hilfe der Verkettungssätze die Stetigkeit einer Funktion bewiesen werden kann:



\begin{exercise*}[Stetigkeit einer verketteten Wurzelfunktion]
Zeige, dass folgende Funktion stetig ist:

\begin{align*}
f:\mathbb {R} \to \mathbb {R} :x\mapsto {\sqrt {5+x^{2}}}
\end{align*}

\end{exercise*}

\begin{solutionprocess*}[Stetigkeit einer verketteten Wurzelfunktion]
Die gegebene Funktion ist eine Verkettung verschiedener Funktionen. Zunächst müssen wir die Grundfunktionen dieser Verkettung finden. Diese sind:

\begin{itemize}
\item $a:\mathbb {R} \to \mathbb {R} :x\mapsto x$
\item $b:\mathbb {R} \to \mathbb {R} :x\mapsto 5$
\item $c:\mathbb {R} _{0}^{+}\to \mathbb {R} :x\mapsto {\sqrt {x}}$
\end{itemize}

Die Funktion $f$ kann dargestellt als:

\begin{align*}
f(x)={\sqrt {5+x^{2}}}={\sqrt {5+x\cdot x}}=c(b(x)+a(x)\cdot a(x))
\end{align*}

Damit ist $f$ eine Verkettung stetiger Funktionen und somit wieder stetig.

\end{solutionprocess*}

\begin{proof*}[Stetigkeit einer verketteten Wurzelfunktion]
Seien folgende Funktionen gegeben:

\begin{itemize}
\item $a:\mathbb {R} \to \mathbb {R} :x\mapsto x$
\item $b:\mathbb {R} \to \mathbb {R} :x\mapsto 5$
\item $c:\mathbb {R} _{0}^{+}\to \mathbb {R} :x\mapsto {\sqrt {x}}$
\end{itemize}

Diese Funktionen sind stetig. Es ist außerdem $f(x)=c(b(x)+a(x)\cdot a(x))$. Damit ist $f$ eine Verkettung stetiger Funktionen und nach den Verkettungssätzen selbst wieder stetig.

\end{proof*}



\chapter{Stetigkeit beweisen}

\section{Übersicht}

Es gibt mehrere Möglichkeiten die Stetigkeit einer Funktion zu beweisen:

\begin{itemize}
\item \emph{Verkettungssätze:} Wenn die Funktion als Verkettung stetiger Funktionen dargestellt werden kann, ist sie nach den Verkettungssätzen stetig. Dies ist ein einfacher Beweis, der aber erst dann angewandt werden kann, nachdem die Verkettungssätze in der Vorlesung eingeführt wurden.
\item \emph{Ausnutzung der lokalen Natur der Stetigkeit:} Wenn eine Funktion in einer kleinen Umgebung um einen Punkt dieselbe Funktionsvorschrift wie die einer stetigen Funktion besitzt, dann muss sie an diesem Punkt auch stetig sein.
\item \emph{Betrachtung des links- und rechtsseitigen Grenzwerts:} Wenn man zeigen kann, dass der linksseitige und rechtsseitige Grenzwert einer Funktion an einer Stelle existiert und gleich dem dortigen Funktionswert ist, dann ist die Funktion an dieser Stelle stetig.
\item \emph{Nachweis Folgenkriterium:} Beim Folgenkriterium muss man zeigen, dass der Limes an der betrachteten Stelle in die Funktion gezogen werden kann. Für eine Folge $(x_{n})_{n\in \mathbb {N} }$ von Argumenten mit Grenzwert $x_{0}$ muss also gelten $\lim _{n\to \infty }f(x_{n})=f(x_{0})=f\left(\lim _{n\to \infty }x_{n}\right)$.
\item \emph{Nachweis Epsilon-Delta-Kriterium:} Für jedes $\epsilon >0$ muss man zeigen, dass es ein $\delta >0$ gibt, so dass für alle Argumente $x$ mit Abstand kleiner als $\delta $ von der betrachteten Stelle $x_{0}$ die Ungleichung $|f(x)-f(x_{0})|<\epsilon $ erfüllt ist.
\end{itemize}

\section{Ausnutzung der lokalen Natur der Stetigkeit}

Gegebenenfalls kann man ausnutzen, dass die Stetigkeit eine lokale Eigenschaft ist. Wenn eine Funktion nämlich in einer kleinen Umgebung um einen Punkt dieselbe Funktionsvorschrift wie die einer stetigen Funktion besitzt, dann muss sie an diesem Punkt auch stetig sein. Betrachte zum Beispiel die Funktion $f:\mathbb {R} \setminus \{0\}\to \mathbb {R} $ mit $f(x)=1$ für positive Zahlen $x$ und $f(x)=-1$ für negative Zahlen $x$. Nehmen wir nun eine beliebige positive Zahl $x_{0}$. In einer hinreichend kleinen Umgebung um $x_{0}$ ist $f$ konstant $1$:

\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Example_of_a_locally_constant_function_with_sgn(x).svg}{\textbf{Example\allowbreak\_of\allowbreak\_a\allowbreak\_locally\allowbreak\_constant\allowbreak\_function\allowbreak\_with\allowbreak\_sgn(x).svg}} by Stephan Kulla \textit{(CC0)}}\begin{center}
\adjincludegraphics[max width=0.5\textwidth,max height=0.2\textheight]{file58example95of95a95locally95constant95function95with95sgn40x4195b2625a6965b225116da70048f6e1fa86e5b57cef}\end{center}

Da konstante Funktionen stetig sind, ist auch $f$ an der Stelle $x_{0}$ stetig. Analog kann man zeigen, dass $f$ auch bei negativen Zahlen und damit insgesamt stetig ist. Im Beweis kann man schreiben:

\begin{importantparagraph*}
„Für jedes $x_{0}\in \mathbb {R} \setminus \{0\}$ gibt es eine Umgebung um $x_{0}$, wo $f$ konstant $1$ oder $-1$ ist. Da konstante Funktionen stetig sind, muss auch $f$ an der Stelle $x_{0}$ stetig sein. Damit ist $f$ stetig“.

\end{importantparagraph*}

Eine solche Argumentation kann oft bei Funktionen angewandt werden, die über eine Fallunterscheidung definiert sind. Unsere Funktion $f$ ist hierfür ein gutes Beispiel. Schließlich ist sie definiert als:

\begin{align*}
f:\mathbb {R} \setminus \{0\}\to \mathbb {R} :x\mapsto {\begin{cases}1&x>0\\0&x<0\end{cases}}
\end{align*}

Jedoch kann nicht bei allen Fallunterscheidungen allein mit der lokalen Natur der Stetigkeit argumentiert werden. Nehmen wir folgende Funktion $g$:

\begin{align*}
g:\mathbb {R} \to \mathbb {R} :x\mapsto {\begin{cases}x^{2}&x\geq 0\\|x|&x<0\end{cases}}
\end{align*}

Für alle Stellen ungleich Null können wir so wie in diesem Abschnitt beschrieben einen Beweis formulieren, dass dort die Funktion stetig ist. An der Stelle $x_{0}=0$ muss anders argumentiert werden. Hier könnte zum Beispiel der links- und rechtsseitige Grenzwert betrachtet werden.

\section{Folgenkriterium}

\subsection{Allgemeine Beweisstruktur}

Um die Stetigkeit einer Funktion an einer Stelle $x_{0}$ zu beweisen, müssen wir zeigen, dass für jede Folge $(x_{n})_{n\in \mathbb {N} }$ von Argumenten mit $\lim _{n\to \infty }x_{n}=x_{0}$ gilt, dass $\lim _{n\to \infty }f(x_{n})=f(x_{0})$ ist. Dementsprechend könnte ein Beweis lauten:

\begin{importantparagraph*}
Sei $f:\ldots $ eine Funktion mit $f(x)=\ldots $ und sei $x_{0}=\ldots $ gegeben. Sei $(x_{n})_{n\in \mathbb {N} }$ eine beliebige Folge von Argumenten mit $\lim _{n\to \infty }x_{n}=x_{0}$. Es gilt:

\begin{align*}
\lim _{n\to \infty }f(x_{n})=\ldots =f(x_{0})
\end{align*}

\end{importantparagraph*}

Um die Stetigkeit der Funktion $f$ zu beweisen, muss das Beweisschema etwas angepasst werden:

\begin{importantparagraph*}
Sei $f:\ldots $ eine Funktion mit $f(x)=\ldots $ und sei $x_{0}$ \emph{eine beliebige Zahl aus dem Definitionsbereich von $f$}. Sei $(x_{n})_{n\in \mathbb {N} }$ eine beliebige Folge von Argumenten mit $\lim _{n\to \infty }x_{n}=x_{0}$. Es gilt:

\begin{align*}
\lim _{n\to \infty }f(x_{n})=\ldots =f(x_{0})
\end{align*}

\end{importantparagraph*}

\subsection{Beispielaufgabe}

\begin{exercise*}[Stetigkeit der Quadratfunktion]
Zeige, dass die Quadratfunktion $f:\mathbb {R} \to \mathbb {R} :x\mapsto x^{2}$ stetig ist.

\end{exercise*}

\begin{proof*}[Stetigkeit der Quadratfunktion]
Sei $f:\mathbb {R} \to \mathbb {R} :x\mapsto x^{2}$. Wir betrachten nun eine beliebige Folge $(x_{n})_{n\in \mathbb {N} }$, die gegen $x$ konvergiert. Es ist

\begin{align*}
\lim _{n\to \infty }f(x_{n})&=\lim _{n\to \infty }x_{n}^{2}\\&=\lim _{n\to \infty }x_{n}\cdot x_{n}\\[0.5em]&{\color {OliveGreen}\left\downarrow \ \lim _{n\to \infty }a_{n}\cdot b_{n}=\lim _{n\to \infty }a_{n}\cdot \lim _{n\to \infty }b_{n}\right.}\\[0.5em]&=\left(\lim _{n\to \infty }x_{n}\right)\cdot \left(\lim _{n\to \infty }x_{n}\right)\\&{\color {OliveGreen}\left\downarrow \ {\text{Voraussetzung}}\lim _{n\to \infty }x_{n}=x\right.}\\[0.5em]&=x\cdot x\\&=x^{2}\\&=f(x)
\end{align*}

Bei der Quadratfunktion kann der Limes also immer hineingezogen werden, womit diese Funktion stetig ist.

\end{proof*}

\section{Epsilon-Delta-Kriterium}

\subsection{Allgemeine Beweisstruktur}

In Quantorenschreibweise lautet die Epsilon-Delta-Definition der Stetigkeit einer Funktion $f$ an der Stelle $x_{0}$:

\begin{align*}
{\color {Red}\forall \epsilon >0}\ {\color {Orange}\exists \delta >0}\ {\color {OliveGreen}\forall x\in D:|x-x_{0}|<\delta }{\color {Blue}{}\implies |f(x)-f(x_{0})|<\epsilon }
\end{align*}

Diese Aussageform gibt eine allgemeine Beweisstruktur für Stetigkeitsbeweise mit dem Epsilon-Delta-Kriterium vor:

\begin{importantparagraph*}
{\textcolor{Red}{Sei ${\color {Red}\epsilon >0}$ beliebig.}} {\textcolor{Orange}{Wähle ${\color {Orange}\delta =\ldots }$ in Abhängigkeit von ${\color {Orange}x_{0}}$ und ${\color {Orange}\epsilon .}$}} {\textcolor{OliveGreen}{Sei ${\color {OliveGreen}x\in D}$ mit ${\color {OliveGreen}|x-x_{0}|<\delta .}$}} {\textcolor{Blue}{Dann ist:}}

\begin{align*}
{\color {Blue}|f(x)-f(x_{0})|<\ldots <\epsilon }
\end{align*}

\end{importantparagraph*}

Die Farben kennzeichnen korrespondierende Elemente zwischen der Quantorenschreibweise und dem Beweis.

\subsection{Beispielaufgaben und allgemeines Vorgehen}

\begin{exercise*}[Stetigkeit der Quadratfunktion]
Beweise, dass die Funktion $f:\mathbb {R} \to \mathbb {R} $ mit $f(x)=x^{2}$ stetig ist.

\end{exercise*}

\begin{solutionprocess*}[Stetigkeit der Quadratfunktion]
Für den Beweis müssen wir zeigen, dass die Quadratfunktion an jeder Stelle $x_{0}\in \mathbb {R} $ stetig ist. Nach der allgemeinen Beweisstruktur des Epsilon-Delta-Kriteriums wird ein beliebiges $\epsilon >0$ vorgegeben. Wir müssen dann ein geeignetes $\delta >0$ finden, sodass die Ungleichung $|f(x)-f(x_{0})|<\epsilon $ für alle $|x-x_{0}|<\delta $ erfüllt ist.

Um ein geeignetes $\delta $ zu finden, setzen wir zunächst in den Term $|f(x)-f(x_{0})|$ die bekannte Funktionszuordnung $f(x)=x^{2}$ ein:

\begin{align*}
|f(x)-f(x_{0})|=\left|x^{2}-x_{0}^{2}\right|
\end{align*}

Den Term $|x-x_{0}|$ können wir kontrollieren. Daher ist es sinnvoll, den Term $\left|x^{2}-x_{0}^{2}\right|$ so umzuformen bzw. nach oben abzuschätzen, dass $|x-x_{0}|$ auftaucht. Hierzu bietet sich die dritte binomische Formel an:

\begin{align*}
|x^{2}-x_{0}^{2}|=|x+x_{0}||x-x_{0}|
\end{align*}

Aus unserer Voraussetzung, dass $|x-x_{0}|<\delta $ gelten soll, können wir den Ausdruck nach oben abschätzen:

\begin{align*}
|x+x_{0}||x-x_{0}|<|x+x_{0}|\cdot \delta 
\end{align*}

Da das $\delta $, welches wir suchen, nur von $\epsilon $ und $x_{0}$ abhängen darf, stört uns die vorhandene Abhängigkeit von $x$ in $|x+x_{0}|\cdot \delta $. Um diese Abhängigkeit zu eliminieren, können wir den Faktor $|x-x_{0}|$ geschickt nach oben abschätzen. Dabei verwenden wir einen unscheinbaren – aber häufig verwendeten – {''}Trick{''}: Wir subtrahieren an geeigneter Stelle ein $x_{0}$ und addieren diesen Term wieder, so dass der Term $x-x_{0}$ entsteht:

\begin{align*}
|x+x_{0}|\cdot \delta =|x\underbrace {-x_{0}+x_{0}} _{=\ 0}+x_{0}|\cdot \delta =|x-x_{0}+2x_{0}|\cdot \delta 
\end{align*}

Damit wir den Betrag $|x-x_{0}|$ erhalten, nutzen wir die Dreiecksungleichung. Den Term $|x-x_{0}|$ können wir wieder nach oben durch $\delta $ abschätzen:

\begin{align*}
|x-x_{0}+2x_{0}|\cdot \delta \leq (|x-x_{0}|+|2x_{0}|)\cdot \delta <(\delta +2|x_{0}|)\cdot \delta 
\end{align*}

Durch geschicktes Umformen und Abschätzen haben wir so erhalten:

\begin{align*}
|f(x)-f(x_{0})|<(\delta +2|x_{0}|)\cdot \delta 
\end{align*}

Mit dieser Ungleichung sind wir fast am Ziel. Wenn wir $\delta $ so geschickt wählen, dass $(\delta +2|x_{0}|)\cdot \delta \leq \epsilon $ ist, haben wir unsere Zielungleichung $|f(x)-f(x_{0})|<\epsilon $ erfüllt. So können wir die Mitternachtsformel bei der quadratischen Gleichung $\delta ^{2}+2|x_{0}|\delta =\epsilon $ anwenden, um eine passende Wahl von $\delta $ zu finden. Der Term $(\delta +2|x_{0}|)\cdot \delta $ kann durch eine weitere Abschätzung jedoch vereinfacht werden. Hierzu können wir ausnutzen, dass wir beliebige Bedingungen an das $\delta $ stellen können. So folgt aus der Bedingung $\delta \leq 1$, dass $\delta +2|x_{0}|\leq 1+2|x_{0}|$ ist und damit gilt:

\begin{align*}
|f(x)-f(x_{0})|<(\underbrace {\delta +2|x_{0}|} _{\leq 1+2|x_{0}|})\cdot \delta \leq (1+2|x_{0}|)\cdot \delta 
\end{align*}

Somit führt auch $(1+2|x_{0}|)\cdot \delta \leq \epsilon $ zum Ziel. Diese Ungleichung können wir umstellen, um eine zweite Bedingung für $\delta $ zu finden (unsere erste Bedingung lautet $\delta \leq 1$):

\begin{align*}
(1+2|x_{0}|)\cdot \delta \leq \epsilon \iff \delta \leq {\frac {\epsilon }{1+2|x_{0}|}}
\end{align*}

Wir haben zwei Bedingungen für $\delta $ gefunden: $\delta \leq 1$ und $\delta ={\tfrac {\epsilon }{1+2|x_{0}|}}$. Beide Bedingungen sind erfüllt für $\delta :=\min \left\{1,{\tfrac {\epsilon }{1+2|x_{0}|}}\right\}$. Diese Wahl treffen wir im finalen Beweis und führen die Abschätzungen so, wie wir sie gerade gefunden haben.

\end{solutionprocess*}

\begin{proof*}[Stetigkeit der Quadratfunktion]
Sei $\epsilon >0$ beliebig und sei $\delta :=\min \left\{1,{\tfrac {\epsilon }{1+2|x_{0}|}}\right\}$. Wenn $\left|x-x_{0}\right|<\delta $ erfüllt ist, dann folgt:

\begin{align*}
|f(x)-f(x_{0})|&=|x^{2}-x_{0}^{2}|\\[0.5em]&=|x+x_{0}||x-x_{0}|\\[0.5em]&\quad {\color {OliveGreen}\left\downarrow \ |x-x_{0}|<\delta \right.}\\[0.5em]&<|x+x_{0}|\cdot \delta \\[0.5em]&=|x\underbrace {-x_{0}+x_{0}} _{=\ 0}+x_{0}|\cdot \delta \\[0.5em]&=|x-x_{0}+2x_{0}|\cdot \delta \\[0.5em]&\leq (|x-x_{0}|+2|x_{0}|)\cdot \delta \\[0.5em]&\quad {\color {OliveGreen}\left\downarrow \ |x-x_{0}|<\delta \right.}\\[0.5em]&<(\delta +2|x_{0}|)\cdot \delta \\[0.5em]&\quad {\color {OliveGreen}\left\downarrow \ \delta \leq 1\right.}\\[0.5em]&\leq (1+2|x_{0}|)\cdot \delta \\[0.5em]&\quad {\color {OliveGreen}\left\downarrow \ \delta ={\frac {\epsilon }{1+2|x_{0}|}}\right.}\\[0.5em]&=(1+2|x_{0}|)\cdot {\frac {\epsilon }{1+2|x_{0}|}}\\[0.5em]&=\epsilon 
\end{align*}

Damit haben wir gezeigt, dass die Quadratfunktion stetig ist.

\end{proof*}

\chapter{Unstetigkeit beweisen}

\section{Überblick}

Um die Unstetigkeit einer Funktion zu beweisen, muss man zeigen, dass diese mindestens eine Unstetigkeitsstelle besitzt. Für den Nachweis einer Unstetigkeitsstelle kann man eine von mehreren Methoden verwenden:

\begin{itemize}
\item \textbf{Folgenkriterium:} Man kann nachweisen, dass die Funktion an der betrachteten Stelle das Folgenkriterium nicht erfüllt.
\item \textbf{Betrachtung des links- und rechtsseitigen Grenzwert:} Man kann den linksseitigen und rechtsseitigen Grenzwert der Funktion an der betrachteten Stelle ausrechnen. Wenn entweder einer dieser beiden Grenzwerte nicht existiert oder wenn diese Grenzwerte unterschiedlich sind, dann ist die Funktion an der betrachteten Stelle unstetig.
\item \textbf{Epsilon-Delta-Kriterium:} Man kann nachweisen, dass die Funktion an der betrachteten Stelle das Epsilon-Delta-Kriterium nicht erfüllt.
\end{itemize}

\section{Folgenkriterium}

\subsection{Beweisskizze}

Um mit dem Folgenkriterium zu zeigen, dass eine Funktion $f:D\to \mathbb {R} $ an der Stelle $x_{0}\in D$ unstetig ist, muss man eine Argumentenfolge $(x_{n})_{n\in \mathbb {N} }$ mit $x_{n}\in D$ für alle $n\in \mathbb {N} $ und dem Grenzwert $x_{0}$ finden, so dass die Funktionswertfolge $\left(f(x_{n})\right)_{n\in \mathbb {N} }$ nicht gegen $f(x_{0})$ konvergiert. Es soll also $\lim _{n\to \infty }x_{n}=x_{0}$ und $\lim _{n\to \infty }f(x_{n})\neq f(x_{0})$ gelten. Für $\lim _{n\to \infty }f(x_{n})\neq f(x_{0})$ gibt es zwei Möglichkeiten:

\begin{itemize}
\item Die Funktionswertfolge $\left(f(x_{n})\right)_{n\in \mathbb {N} }$ divergiert.
\item Die Funktionswertfolge $\left(f(x_{n})\right)_{n\in \mathbb {N} }$ konvergiert, jedoch ist ihr Grenzwert ungleich $f(x_{0})$.
\end{itemize}

Ein Unstetigkeitsbeweis über das Folgenkriterium könnte zum Beispiel folgende Form aufweisen:

\begin{importantparagraph*}
Sei $f:\ldots $ eine Funktion mit $f(x)=\ldots $. Diese Funktion ist unstetig an der Stelle $x_{0}=\ldots $. Wählen wir nämlich die Folge $(x_{n})_{n\in \mathbb {N} }$ mit $x_{n}=\ldots $, so liegen alle Folgenglieder im Definitionsbereich von $f$, und wir haben

\begin{align*}
\lim _{n\to \infty }x_{n}=\ldots =x_{0}
\end{align*}

Jedoch ist $\lim _{n\to \infty }f(x_{n})\neq f(x_{0})$. Es ist nämlich ...\emph{Beweis, dass $\left(f(x_{n})\right)_{n\in \mathbb {N} }$ divergiert oder dass der Grenzwert von $\left(f(x_{n})\right)_{n\in \mathbb {N} }$ ungleich $f(x_{0})$ ist}...

\end{importantparagraph*}

\subsection{Beispielaufgabe}

\begin{exercise*}[Unstetigkeit der topologischen Sinusfunktion]
Beweise die Unstetigkeit der folgenden Funktion:

\begin{align*}
f:\mathbb {R} \to \mathbb {R} :x\mapsto {\begin{cases}\sin \left({\frac {1}{x}}\right)&x\neq 0\\0&x=0\end{cases}}
\end{align*}

\end{exercise*}

\begin{solutionprocess*}[Unstetigkeit der topologischen Sinusfunktion]
Damit $f$ eine unstetige Funktion ist, muss sie mindestens eine Unstetigkeitkeitsstelle besitzen. Für jedes $x\neq 0$ entspricht $f$ in einer hinreichend kleinen Umgebung von $x$ der Funktion $\sin \left({\tfrac {1}{x}}\right)$. Da die Funktion $\sin \left({\tfrac {1}{x}}\right)$ als Komposition stetiger Funktionen stetig ist, muss auch $f$ für alle $x\neq 0$ stetig sein. Damit muss die Unstetigkeitsstelle bei $x=0$ liegen.

Um mit dem Folgenkriterium zu zeigen, dass $f$ an der Stelle $x=0$ unstetig ist, müssen wir eine Argumentenfolge $(x_{n})_{n\in \mathbb {N} }$ mit $\lim _{n\to \infty }x_{n}=0$ und $\lim _{n\to \infty }f(x_{n})\neq f(0)$ finden. Um diese Argumentenfolge zu finden, schauen wir uns zunächst den Graphen der Funktion $f$ an:

\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:The_function_sin(1_over_x).svg}{\textbf{The\allowbreak\_function\allowbreak\_sin(1\allowbreak\_over\allowbreak\_x).svg}} by Stephan Kulla \textit{(CC0)}}\begin{center}
\adjincludegraphics[max width=0.5\textwidth,max height=0.2\textheight]{file58the95function95sin40195over95x419576029f13dd50d1445e4c830ce857aaebd4505a02}\end{center}

In der Graphik sehen wir, dass die Funktion $f$ in jeder Umgebung des Nullpunkts jeden Wert zwischen $-1$ und $1$ beliebig oft annimmt. Also können wir $(x_{n})_{n\in \mathbb {N} }$ so wählen, dass $f(x_{n})$ immer gleich $1$ ist. Dann ist nämlich garantiert, dass $\lim _{n\to \infty }f(x_{n})=1\neq 0=f(0)$ ist. Dabei wählen wir $x_{n}$ so, dass $x_{n}$ von oben gegen Null konvergiert.

In der folgenden Graphik sind neben dem Graphen von $f$ auch die Funktionswerte der Folge $x_{n}$ (grüne Punkte) eingetragen. Man sieht, dass für $x_{n}\to 0$ die Funktionswerte gegen $1$ konvergieren, was ungleich dem Funktionswert $f(0)=0$ (orangefarbener Punkt) ist:

\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Discontinuity_of_function_sin(1_over_x).svg}{\textbf{Discontinuity\allowbreak\_of\allowbreak\_function\allowbreak\_sin(1\allowbreak\_over\allowbreak\_x).svg}} by Stephan Kulla \textit{(CC0)}}\begin{center}
\adjincludegraphics[max width=0.5\textwidth,max height=0.2\textheight]{file58discontinuity95of95function95sin40195over95x4195861a09878386ba3c576ee36ae6b443a1780fbbac}\end{center}

Wie lauten die Werte für $x_{n}$? Formen wir hierzu die Gleichung $f(x)=1$ nach $x$ um:

\begin{align*}
{\begin{array}{rrrl}&&f(x)&=1\\[0.5em]{\overset {f(0)\neq 0}{\iff {}}}&&\sin \left({\frac {1}{x}}\right)&=1\\[0.5em]\iff {}&\exists k\in \mathbb {Z} :&{\frac {1}{x}}&={\frac {\pi }{2}}+2k\pi \\[0.5em]\iff {}&\exists k\in \mathbb {Z} :&x&={\frac {1}{{\frac {\pi }{2}}+2k\pi }}\end{array}}
\end{align*}

Für alle $x={\tfrac {1}{{\frac {\pi }{2}}+2k\pi }}$ mit $k\in \mathbb {Z} $ gilt also $f(x)=1$. Damit unsere Argumente $x_{n}$ positiv sind und von oben gegen Null konvergieren, wählen wir $x_{n}={\tfrac {1}{{\frac {\pi }{2}}+2n\pi }}$. Es gilt dann:

\begin{align*}
\lim _{n\to \infty }x_{n}=\lim _{n\to \infty }{\frac {1}{{\frac {\pi }{2}}+2n\pi }}=0
\end{align*}

Jedoch haben wir gesehen, dass $\lim _{n\to \infty }f(x_{n})=1\neq 0=f(0)$ ist. Wir haben also eine Argumentenfolge $(x_{n})_{n\in \mathbb {N} }$ gefunden, welche die Unstetigkeit von $f$ an der Stelle $x=0$ beweist.

\end{solutionprocess*}

\begin{proof*}[Unstetigkeit der topologischen Sinusfunktion]
Sei $f:\mathbb {R} \to \mathbb {R} $ mit $f(x)=\sin \left({\tfrac {1}{x}}\right)$ für $x\neq 0$ und $f(0)=0$. Wir berachten die Folge $(x_{n})_{n\in \mathbb {N} }$ mit $x_{n}={\tfrac {1}{{\frac {\pi }{2}}+2n\pi }}$. Für diese Folge ist:

\begin{align*}
\lim _{n\to \infty }x_{n}=\lim _{n\to \infty }{\frac {1}{{\frac {\pi }{2}}+2n\pi }}=0
\end{align*}

Außerdem gilt:

\begin{align*}
\lim _{n\to \infty }f(x_{n})&=\lim _{n\to \infty }f\left({\frac {1}{{\frac {\pi }{2}}+2n\pi }}\right)\\[0.5em]&=\lim _{n\to \infty }\sin \left({\frac {1}{\frac {1}{{\frac {\pi }{2}}+2n\pi }}}\right)\\[0.5em]&=\lim _{n\to \infty }\sin \left({\frac {\pi }{2}}+2n\pi \right)\\[0.5em]&=\lim _{n\to \infty }1\\[0.5em]&=1
\end{align*}

Damit ist $\lim _{n\to \infty }f(x_{n})=1\neq 0=f(0)$, obwohl $\lim _{n\to \infty }x_{n}=0$ ist. Dies beweist, dass $f$ an der Stelle $x=0$ und somit auch insgesamt unstetig ist.

\end{proof*}

\section{Epsilon-Delta-Kriterium}

\subsection{Allgemeine Beweisstruktur}

Epsilon-Delta-Kriterium der Unstetigkeit kann folgendermaßen in Prädikatenlogik formuliert werden:

\begin{align*}
{\color {Red}\exists \epsilon >0}\ {\color {Orange}\forall \delta >0}\ {\color {OliveGreen}\exists x\in D}:{\color {RedViolet}|x-x_{0}|<\delta }\land {\color {Blue}|f(x)-f(x_{0})|\geq \epsilon }
\end{align*}

Daraus ergibt sich ein Schema, mit dem die Unstetigkeit einer Funktion nach dem Epsilon-Delta-Kriterium bewiesen werden kann:

\begin{importantparagraph*}
{\textcolor{Red}{Wähle ${\color {Red}\epsilon =\ldots }$.}} {\textcolor{Orange}{Sei ${\color {Orange}\delta >0}$ beliebig.}} {\textcolor{OliveGreen}{Wähle ${\color {OliveGreen}x=\ldots }$. Die Zahl ${\color {OliveGreen}x}$ liegt in ${\color {OliveGreen}D}$, weil ...}} {\textcolor{RedViolet}{Es ist:}}

\begin{align*}
{\color {RedViolet}{\text{Beweis für }}|x-x_{0}|<\delta }
\end{align*}

Außerdem ist:

\begin{align*}
{\color {Blue}{\text{Beweis für }}|f(x)-f(x_{0})|\geq \epsilon }
\end{align*}

\end{importantparagraph*}

Die Farben kennzeichnen korrespondierende Elemente zwischen der Quantorenschreibweise und dem Beweis.

\subsection{Beispielaufgabe}

\begin{exercise*}[Unstetigkeit der topologischen Sinusfunktion]
Beweisen Sie die Unstetigkeit der folgenden Funktion an der Stelle $x_{0}=0$:

\begin{align*}
f:\mathbb {R} \to \mathbb {R} :x\mapsto {\begin{cases}\sin \left({\frac {1}{x}}\right)&x\neq 0\\0&x=0\end{cases}}
\end{align*}

\end{exercise*}

\begin{solutionprocess*}[Unstetigkeit der topologischen Sinusfunktion]
Bei dieser Aufgabe soll die Unstetigkeit einer Funktion gezeigt werden. Wir betrachten hierzu die Negation des Epsilon-Delta-Kriteriums. Unser Ziel ist es sowohl ein $\epsilon >0$ als auch ein $x\in \mathbb {R} $ so zu wählen, dass $|x-x_{0}|<\delta $ und $|f(x)-f(x_{0})|\geq \epsilon $ ist. Dabei darf $x$ in Abhänigigkeit von $\delta $ gewählt werden, während $\epsilon $ unabhängig für alle $\delta >0$ sein muss. Für eine Lösung können wir folgendermaßen vorgehen:

\proofstep{Schritt 1:}
 Zielungleichungen vereinfachen\begin{indentblock}
Zunächst können wir beide Ungleichungen, die erfüllt sein müssen, umschreiben, denn in unserem Fall ist $x_{0}=0$ und $f(x_{0})=0$. Dadurch können wir schreiben: $|x|<\delta $ und $|f(x)|\geq \epsilon $.

\end{indentblock}

\proofstep{Schritt 2:}
 Wahl eines geeigneten $\epsilon >0$\begin{indentblock}
Wir betrachten nun den Graphen der Funktion $f$ – dieser hilft uns nämlich unsere „Beweisbausteine“ zu finden:

\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:The_function_sin(1_over_x).svg}{\textbf{The\allowbreak\_function\allowbreak\_sin(1\allowbreak\_over\allowbreak\_x).svg}} by Stephan Kulla \textit{(CC0)}}\begin{center}
\adjincludegraphics[max width=0.5\textwidth,max height=0.2\textheight]{file58the95function95sin40195over95x419576029f13dd50d1445e4c830ce857aaebd4505a02}\end{center}

Wir müssen ein $\epsilon >0$ finden, so dass es stets Funktionswerte im Bereich $(x_{0}-\delta ,x_{0}+\delta )=(-\delta ,\delta )$ gibt, die einen Abstand größer gleich $\epsilon $ von $f(x_{0})=f(0)=0$ haben – egal wie klein $\delta $ ist. Sprich: Egal, welches $\delta $ wir wählen, es gibt immer Punkte die oberhalb oder unterhalb des $2\epsilon $-$2\delta $-Rechtecks liegen.

In der Grafik erkennen wir, dass die Funktion in der Nähe des Nullpunkts unendlich oft zwischen $-1$ und $1$ oszilliert. Damit bietet sich ein $\epsilon \leq 1$ an. Dann gibt es nämlich immer Funktionswerte in jeder noch so kleinen Umgebung von der Null mit $|f(x)|\geq 1$. Wir wählen $\epsilon ={\tfrac {1}{2}}$. In der folgenden Grafik ist dies illustriert:

\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:The_function_sin(1_over_x)_with_2epsilon-2delta-rectangle.svg}{\textbf{The\allowbreak\_function\allowbreak\_sin(1\allowbreak\_over\allowbreak\_x)\allowbreak\_with\allowbreak\_2epsilon\allowbreak-2delta\allowbreak-rectangle.svg}} by Stephan Kulla \textit{(CC0)}}\begin{center}
\adjincludegraphics[max width=0.5\textwidth,max height=0.2\textheight]{file58the95function95sin40195over95x4195with952epsilon452delta45rectangle9572ad606c60cff1dc34e0987f1866025b52630ab4}\end{center}

Im Beweis müssen wir nach der Wahl von $\epsilon $ das $\delta $ beliebig größer Null wählen. Das machen wir dann auch.

\end{indentblock}

\proofstep{Schritt 3:}
 Wahl eines geeigneten $x\in \mathbb {R} $\begin{indentblock}
Wir haben $\epsilon ={\tfrac {1}{2}}$ gesetzt. Es muss nun gelten $\left|\sin \left({\tfrac {1}{x}}\right)\right|\geq {\tfrac {1}{2}}$. Damit diese Bedingung erfüllt ist, können wir solche $x$ mit $\sin \left({\tfrac {1}{x}}\right)=1$ wählen. Nun ist $sin(a)=1$ genau dann, wenn $a={\tfrac {\pi }{2}}+2k\pi $ für ein $k\in \mathbb {Z} $ ist. Für die gesuchten $x$ gilt also:

\begin{align*}
{\frac {1}{x}}={\frac {\pi }{2}}+2k\pi \iff x={\frac {1}{{\frac {\pi }{2}}+2k\pi }}
\end{align*}

Damit haben wir verschiedene $x$ gefunden, für die $|f(x)|\geq \epsilon $ gilt. Jetzt muss noch die erste Bedingung $|x|<\delta $ beachtet werden. Unsere $x$ hängen von $k$ ab. Wir müssen ein geeignetes $k$ mit $k\in \mathbb {Z} $ finden, so dass $|x|<\delta $ erfüllt ist. Setzen wir also in diese Ungleichung $|x|<\delta $ die gefundene Gleichung $x={\tfrac {1}{{\tfrac {\pi }{2}}+2k\pi }}$ ein und stellen sie nach $k$ um:

\begin{align*}
\left|x\right|<\delta &\iff \left|{\frac {1}{{\frac {\pi }{2}}+2k\pi }}\right|<\delta \\[0.5em]&\iff {\frac {1}{{\frac {\pi }{2}}+2k\pi }}<\delta \\[0.5em]&\iff 2k\pi +{\frac {\pi }{2}}>{\frac {1}{\delta }}\\[0.5em]&\iff 2k\pi >{\frac {1}{\delta }}-{\frac {\pi }{2}}\\[0.5em]&\iff k>{\frac {1}{2\pi }}\left({\frac {1}{\delta }}-{\frac {\pi }{2}}\right)
\end{align*}

Dies liefert den Ausdruck $k>{\tfrac {1}{2\pi }}\left({\tfrac {1}{\delta }}-{\tfrac {\pi }{2}}\right)$. Wählen wir also eine natürliche Zahl $k$, die größer als ${\tfrac {1}{2\pi }}\left({\tfrac {1}{\delta }}-{\tfrac {\pi }{2}}\right)$ ist, so ist $|x|<\delta $ erfüllt. Ein solches $k$ muss nach dem archimedischen Axiom existieren. Wählen wir ein solches $k$ und definieren damit das $x$ über $x={\tfrac {1}{{\tfrac {\pi }{2}}+2k\pi }}$, haben wir sowohl $|x|<\delta $ als auch $|f(x)|\geq \epsilon $ gegeben. Damit sind alle Bausteine für den Beweis gefunden und dieser muss nur noch sauber aufgeschrieben werden.

\end{indentblock}

\end{solutionprocess*}

\begin{proof*}[Unstetigkeit der topologischen Sinusfunktion]
Wähle $\epsilon ={\tfrac {1}{2}}$ und sei $\delta >0$ beliebig. Wähle eine natürliche Zahl $k$, so dass $k>{\tfrac {1}{2\pi }}\left({\tfrac {1}{\delta }}-{\tfrac {\pi }{2}}\right)$. Eine solche natürliche Zahl $k$ muss nach dem Archimedischen Axiom existieren. Weiter sei $x={\tfrac {1}{{\frac {\pi }{2}}+2k\pi }}$. So gilt:

\begin{align*}
k>{\frac {1}{2\pi }}\left({\frac {1}{\delta }}-{\frac {\pi }{2}}\right)&\implies 2k\pi >{\frac {1}{\delta }}-{\frac {\pi }{2}}\\[0.5em]&\implies 2k\pi +{\frac {\pi }{2}}>{\frac {1}{\delta }}\\[0.5em]&\quad {\color {OliveGreen}\left\downarrow \ {\frac {1}{a}}>{\frac {1}{b}}>0\iff 0<a<b\right.}\\[0.5em]&\implies {\frac {1}{{\frac {\pi }{2}}+2k\pi }}<\delta \\[0.5em]&\quad {\color {OliveGreen}\left\downarrow \ {\frac {1}{{\frac {\pi }{2}}+2k\pi }}>0\right.}\\[0.5em]&\implies \left|{\frac {1}{{\frac {\pi }{2}}+2k\pi }}\right|<\delta \\[0.5em]&\quad {\color {OliveGreen}\left\downarrow \ x={\frac {1}{{\frac {\pi }{2}}+2k\pi }}\right.}\\[0.5em]&\implies \left|x\right|<\delta 
\end{align*}

Weiter ist:

\begin{align*}
\left|f(x)-f(0)\right|&=\left|\sin \left({\frac {1}{x}}\right)-0\right|\\[0.5em]&\quad {\color {OliveGreen}\left\downarrow \ x={\frac {1}{{\frac {\pi }{2}}+2k\pi }}\right.}\\[0.5em]&=\left|\sin \left({\frac {\pi }{2}}+2k\pi \right)\right|\\[0.5em]&\quad {\color {OliveGreen}\left\downarrow \ \sin \left({\frac {\pi }{2}}+2k\pi \right)=1\right.}\\[0.5em]&=\left|1\right|\geq {\frac {1}{2}}=\epsilon 
\end{align*}

Damit ist die Funktion unstetig an der Stelle $x_{0}=0$.

\end{proof*}

\chapter{Zwischenwertsatz}

Der Zwischenwertsatz besagt, dass jede stetige Funktion $f:[a,b]\to \mathbb {R} $ alle Werte zwischen $f(a)$ und $f(b)$ mindestens einmal annimmt. Stetige Funktionen nehmen also alle Zwischenwerte zwischen $f(a)$ und $f(b)$ an (wenn es zwischen $a$ und $b$ keine Lücken im Definitionsbereich gibt). Der Zwischenwertsatz kann somit genutzt werden, um die Existenz von Funktionswerten zu beweisen.

\section{Motivation}

Sei $f:[a,b]\to \mathbb {R} $ eine beliebige stetige Funktion. An der Stelle $a$ besitzt sie den Funktionswert $f(a)$ und an der Stelle $b$ den Funktionswert $f(b)$. Nehmen wir an, dass $f(a)\leq f(b)$ ist. Sei außerdem $s$ ein beliebiger Wert zwischen $f(a)$ und $f(b)$, also $f(a)\leq s\leq f(b)$:

\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Illustration_for_the_intermediate_value_theorem_(initial_situation).svg}{\textbf{Illustration\allowbreak\_for\allowbreak\_the\allowbreak\_intermediate\allowbreak\_value\allowbreak\_theorem\allowbreak\_(initial\allowbreak\_situation).svg}} by Lukasstockner, Stephan Kulla \textit{(CC0)}}\begin{center}
\adjincludegraphics[max width=0.4\textwidth,max height=0.2\textheight]{file58illustration95for95the95intermediate95value95theorem9540initial95situation419540a768b86af84209bfd4d904827679b7d112ada8}\end{center}

Nach unserer Vorstellung besitzen stetige Funktionen innerhalb des Definitionsbereichs keine Sprünge. Da $f$ auf dem gesamten Intervall $[a,b]$ definiert ist und somit ihr Definitionsbereich zusammenhängend ist, verbindet der Graph die Punkte $(a,f(a))$ und $(b,f(b))$ ohne Sprünge. Wenn wir $f(a)$ und $f(b)$ ohne Absetzen des Stifts verbinden, müssen wir irgendwann die Gerade $y=s$ kreuzen. Es gibt also mindestens einen Schnittpunkt zwischen der Geraden $y=s$ und dem Graphen von $f$:

\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Illustration_for_the_intermediate_value_theorem.svg}{\textbf{Illustration\allowbreak\_for\allowbreak\_the\allowbreak\_intermediate\allowbreak\_value\allowbreak\_theorem.svg}} by Autorenkollektiv „Auswahlaxiom“ (Charlotte Dietze, Matthias Paulsen, Anne Reif), Lukasstockner, Stephan Kulla \textit{(CC0)}}\begin{center}
\adjincludegraphics[max width=0.4\textwidth,max height=0.2\textheight]{file58illustration95for95the95intermediate95value95theorem959b86216ddb144128f2c46cc6413ebf0857fc5ea0}\end{center}

Für die $x$-Werte ${\tilde {x}}$ der Schnittpunkte gilt $f({\tilde {x}})=s$. Der Zwischenwert $s$ wird also mindestens einmal durch die Funktion $f$ angenommen. Wir haben intuitiv gesehen, dass stetige Funktionen alle Werte zwischen zwei Funktionswerten mindestens einmal annehmen, wenn der Definitionsbereich keine Lücken zwischen den beiden Argumenten besitzt.

\section{Der Zwischenwertsatz}

\begin{theorem*}[Zwischenwertsatz]
Sei $f:[a,b]\to \mathbb {R} $ eine stetige Funktion mit $a,b\in \mathbb {R} $ und $a<b$. Sei $s$ ein Wert zwischen den beiden Funktionswerten $f(a)$ und $f(b)$. Es gilt also entweder $f(a)\leq s\leq f(b)$ oder $f(b)\leq s\leq f(a)$. Dann gibt es mindestens eine reelle Zahl $x\in [a,b]$ mit $f(x)=s$. Der Zwischenwert $s$ wird also mindestens einmal von der Funktion $f$ angenommen.

\end{theorem*}

\section{Nullstellensatz von Bolzano}

Für den Beweis des Zwischenwertsatzes reicht es aus, diesen nur für den Spezialfall $s=0$ zu beweisen. Dieser Spezialfall wird auch „Nullstellensatz von Bolzano” genannt:

\begin{theorem*}[Nullstellensatz von Bolzano]
Sei $h:[a,b]\to \mathbb {R} $ eine stetige Funktion mit $a,b\in \mathbb {R} $ und $a<b$. Sei außerdem die Null ein Zwischenwert von $h(a)$ und $h(b)$, also $h(a)\leq 0\leq h(b)$ oder $h(a)\geq 0\geq h(b)$. Dann besitzt $h$ mindestens eine Nullstelle. Es gibt also mindestens ein Argument ${\tilde {x}}\in [a,b]$ mit $h({\tilde {x}})=0$.

\end{theorem*}

Wieso reicht es, nur diesen Spezialfall zu betrachten? Nehmen wir eine stetige Funktion $f:[a,b]\to \mathbb {R} $ und einen Wert $s\in \mathbb {R} $ zwischen den Funktionswerten $f(a)$ und $f(b)$. Nach dem Zwischenwertsatz müssen wir nun ein ${\tilde {x}}\in [a,b]$ mit $f({\tilde {x}})=s$ finden. Nun gilt:

\begin{align*}
f({\tilde {x}})=s\iff f({\tilde {x}})-s=0
\end{align*}

Damit ist genau dann $f({\tilde {x}})=s$, wenn $f({\tilde {x}})-s=0$ ist. Wir definieren nun die Hilfsfunktion $h:[a,b]\to \mathbb {R} $ mit $h(x)=f(x)-s$. Wie wir gerade festgestellt haben, ist genau im Fall $f({\tilde {x}})=s$ die Gleichung $h({\tilde {x}})=0$ erfüllt. Wenn wir also eine Nullstelle von $h$ finden, dann nimmt auch die Funktion $f$ den Wert $s$ an.

Nun erfüllt die Funktion $h$ alle Voraussetzungen des Nullstellensatz von Bolzano. Es ist eine Funktion der Form $[a,b]\to \mathbb {R} $ mit dem abgeschlossenen Intervall $[a,b]$ als Definitionsbereich. Als Verkettung stetiger Funktionen ist die Funktion $h$ stetig. Im Fall $f(a)\leq s\leq f(b)$ ist:

\begin{align*}
f(a)\leq s\iff f(a)-s\leq 0\iff h(a)\leq 0\\[0.3em]f(b)\geq s\iff f(b)-s\geq 0\iff h(b)\geq 0
\end{align*}

Damit folgt aus $f(a)\leq s\leq f(b)$ die Ungleichungskette $h(a)\leq 0\leq h(b)$. Betrachten wir nun den Fall $f(a)\geq s\geq f(b)$:

\begin{align*}
f(a)\geq s\iff f(a)-s\geq 0\iff h(a)\geq 0\\[0.3em]f(b)\leq s\iff f(b)-s\leq 0\iff h(b)\leq 0
\end{align*}

Es folgt insgesamt, dass Null ein Zwischenwert von $h(a)$ und $h(b)$ ist. Somit erfüllt $h$ die Voraussetzungen des Nullstellensatz von Bolzano. Nach diesem Nullstellensatz gibt es ein ${\tilde {x}}\in [a,b]$ mit $h({\tilde {x}})=0$. Für dieses ${\tilde {x}}$ ist dann $f({\tilde {x}})=s$. Dies zeigt, dass aus dem Nullstellensatz von Bolzano der allgemeinere Zwischenwertsatz folgt. Wir müssen also nur den Nullstellensatz von Bolzano beweisen.

\section{Beweis des Nullstellensatz von Bolzano}

\begin{proof*}[Nullstellensatz von Bolzano]
Sei $h:[a,b]\to \mathbb {R} $ eine stetige Funktion mit $h(a)\leq 0\leq h(b)$ oder $h(a)\geq 0\geq h(b)$. Im Folgenden betrachten wir den Fall $h(a)\leq 0\leq h(b)$. Der andere Fall $h(a)\geq 0\geq h(b)$ kann analog bewiesen werden. Wir müssen nun eine Nullstelle von $h$ finden. Dies kann durch eine geeignete Intervallschachtelung gezeigt werden. Als Startintervall wählen wir $[a_{1},b_{1}]=[a,b]$, also $a_{1}=a$ und $b_{1}=b$. Wir wissen nämlich, dass sich im Intervall $[a,b]$ die gesuchte Nullstelle befinden muss.

Für $h(a_{1})=0$ oder $h(b_{1})=0$ haben wir bereits eine Nullstelle bei $x=a_{1}$ bzw. $x=b_{1}$ gefunden und sind fertig. Falls $h(a_{1})<0$ und $h(b_{1})>0$ ist, verkleinern wir unser Intervall. Wir betrachten hierzu den Mittelpunkt ${\tfrac {a_{1}+b_{1}}{2}}$ des Startintervalls. Ist der Wert von $h$ an diesem Punkt gleich Null, so haben wir wieder eine Nullstelle gefunden und sind fertig.

Wenn $h\left({\tfrac {a_{1}+b_{1}}{2}}\right)\neq 0$ ist, so wählen wir nun ein anderes Intervall, in dem sich eine Nullstelle befinden muss. Nehmen wir an, es sei $h\left({\tfrac {a_{1}+b_{1}}{2}}\right)>0$. Dann ergibt sich folgendes Bild:

\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Bolzano3.png}{\textbf{Bolzano3.png}} by Alexander Sedlmayr \textit{(CC0)}}\begin{center}
\adjincludegraphics[max width=0.5\textwidth,max height=0.2\textheight]{file58bolzano395b490d7327dad0f2e891983081fb9043c9cefcb3d}\end{center}

Wir sehen, dass der Graph im ersten Intervall von $a_{1}$ bis ${\tfrac {a_{1}+b_{1}}{2}}$ die $x$-Achse überqueren muss. Da $h$ als stetige Funktion keine Sprünge aufweist und in diesem Intervall keine Definitionslücken aufweist, sollte sich in diesem Intervall also eine Nullstelle von $h$ befinden. Da beide Funktionswerte $h\left({\tfrac {a_{1}+b_{1}}{2}}\right)$ und $h(b_{1})$ positiv sind, können wir nicht sagen, ob sich im Bereich von ${\tfrac {a_{1}+b_{1}}{2}}$ bis $b_{1}$ eine Nullstelle befindet oder nicht. Deswegen wählen wir als zweites Intervall $[a_{2},b_{2}]=\left[a_{1},{\tfrac {a_{1}+b_{1}}{2}}\right]$.

Wenn demgegenüber $h\left({\tfrac {a_{1}+b_{1}}{2}}\right)$ kleiner als Null ist, muss der Graph von $h$ im zweiten Intervall von ${\tfrac {a_{1}+b_{1}}{2}}$ bis $h(b_{1})$ einen Vorzeichenwechsel vollführen. Dementsprechend sollte sich dort eine Nullstelle befinden und wir wählen in diesem Fall $\left[{\tfrac {a_{1}+b_{1}}{2}},b_{1}\right]$ als zweites Intervall $[a_{2},b_{2}]$:

\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:BolzanoN.svg}{\textbf{BolzanoN.svg}} by Alexander Sedlmayr \textit{(CC0)}}\begin{center}
\adjincludegraphics[max width=0.5\textwidth,max height=0.2\textheight]{file58bolzanon9530fd5299cc76314f2e198223c320f9a5e8037480}\end{center}

Insgesamt bestimmen wir $[a_{2},b_{2}]$ folgendermaßen:

\begin{align*}
[a_{2},b_{2}]={\begin{cases}\left[a_{1},{\frac {a_{1}+b_{1}}{2}}\right]&;h\left({\frac {a_{1}+b_{1}}{2}}\right)>0\\\left[{\frac {a_{1}+b_{1}}{2}},b_{1}\right]&;h\left({\frac {a_{1}+b_{1}}{2}}\right)<0\end{cases}}
\end{align*}

Diesen Vorgang wiederholen wir nun immer wieder: Im $n$-ten Schritt berechnet wir den Mittelpunkt ${\tfrac {a_{n}+b_{n}}{2}}$ des Intervalls $[a_{n},b_{n}]$. Nimmt $h$ hier den Wert $0$ an, sind wir fertig und können ${\tfrac {a_{n}+b_{n}}{2}}$ als Nullstelle zurückgeben. Ansonsten wählen wir analog zu vorher ein neues Intervall $[a_{n+1},b_{n+1}]$ mit folgender Definition

\begin{align*}
[a_{n+1},b_{n+1}]={\begin{cases}\left[a_{n},{\frac {a_{n}+b_{n}}{2}}\right]&;h\left({\frac {a_{n}+b_{n}}{2}}\right)>0\\\left[{\frac {a_{n}+b_{n}}{2}},b_{n}\right]&;h\left({\frac {a_{n}+b_{n}}{2}}\right)<0\end{cases}}
\end{align*}

Durch dieses Vorgehen erhalten wir entweder nach irgendeinem Schritt $n$ eine gesuchte Nullstelle, oder wir bekommen eine Folge von Intervallen $([a_{n},b_{n}])_{n\in \mathbb {N} }$. So wie wir die Folgenglieder gewählt haben, ist die Folge $(a_{n})_{n\in \mathbb {N} }$ monoton wachsend, und die Folge $(b_{n})_{n\in \mathbb {N} }$ monoton fallend. Da jedes Folgenglied im Intervall $[a,b]$ liegt, sind die Folgen auch beschränkt. Daraus können wir nach dem \href{https://de.wikibooks.org/wiki/Mathe\_für\_Nicht-Freaks:\_Monotoniekriterium\_für\_Folgen}
{Monotoniekriterium für Folgen} folgern, dass beide Folgen konvergieren. Beachte, dass die Länge des Intervalls bei jedem Schritt halbiert wird. Das heißt:

\begin{align*}
\lim _{n\rightarrow \infty }{(b_{n}-a_{n})}&=\lim _{n\rightarrow \infty }{{\frac {1}{2}}\cdot (b_{n-1}-a_{n-1})}\\[0.3em]&=\lim _{n\rightarrow \infty }{{\frac {1}{2}}\cdot {\frac {1}{2}}\cdot (b_{n-2}-a_{n-2})}\\[0.3em]&\vdots \\[0.3em]&=\lim _{n\rightarrow \infty }{{\frac {1}{2^{n}}}\cdot (b_{1}-a_{1})}=0
\end{align*}

Und damit folgt: $\lim _{n\rightarrow \infty }{a_{n}}=\lim _{n\rightarrow \infty }{b_{n}}$. Damit konvergieren die Folgen der unteren bzw. oberen Intervallgrenzen gegen den gleichen Wert $c\in [a;b]$. Außerdem gilt mit unserer Wahl, dass $h(a_{n})<0$ sowie $h(b_{n})>0$ für alle $n\in \mathbb {N} $ gilt. Deswegen gilt für die Grenzwerte: $\lim _{n\rightarrow \infty }{h(a_{n})}\leq 0$ und $\lim _{n\rightarrow \infty }{h(b_{n})}\geq 0$. Weil $h$ stetig ist, gilt

\begin{align*}
\lim _{n\rightarrow \infty }{h(a_{n})}=h\left(\lim _{n\rightarrow \infty }{a_{n}}\right)=h(c)=h\left(\lim _{n\rightarrow \infty }{b_{n}}\right)=\lim _{n\rightarrow \infty }{h(b_{n})}
\end{align*}

Mit der oberen Zeile folgt also $h(c)\leq 0$ und $h(c)\geq 0$, damit muss $h(c)=0$ gelten und wir haben auch in diesem Fall eine Nullstelle der Funktion $h$ gefunden.

\end{proof*}

\section{Übungsaufgabe: Fixpunktsatz}

\subsection{Beweis eines Fixpunktsatzes}

In der folgenden Aufgabe beweisen wir einen Fixpunktsatz. Fixpunkte sind Argumente $x$ einer Funktion $f$, die die Gleichung $f(x)=x$ erfüllt. Fixpunkte werden also durch eine Funktion nicht verändert. Fixpunktsätze sind wiederum Sätze, die die Existenz von Fixpunkten in gewissen Situationen beweisen. Für die Mathematik sind solche Sätze wichtig, weil manchmal die Existenz eines Objekts auf die Existenz eines Fixpunktes zurückgeführt werden kann. Beispielsweise ist das Argument $x$ genau dann Nullstelle einer Funktion $f:\mathbb {R} \to \mathbb {R} $, wenn die Funktion $g:\mathbb {R} \to \mathbb {R} $ mit der Zuordnungsvorschrift $g(x)=x-f(x)$ einen Fixpunkt besitzt. Aus der Existenz eines Fixpunkts der Funktion $g$, folgt die Existenz einer Nullstelle für $f$. In der folgenden Aufgabe werden wir einen Zwischenwertsatz beweisen:

\begin{exercise*}[Fixpunktsatz]
Sei $f:[a,b]\to [a,b]$ eine stetige Funktion. Beweise, dass $f$ mindestens einen Fixpunkt besitzt. Fixpunkte sind Stellen $x\in [a,b]$ mit $f(x)=x$.

\end{exercise*}

\begin{solutionprocess*}[Fixpunktsatz]
Durch Umstellung der Gleichung $f(x)=x$ erhalten wir $f(x)-x=0$. Damit ist $x$ genau dann ein Fixpunkt von $f$, wenn $x$ eine Nullstelle der Funktion $h(x)=f(x)-x$ ist. Definieren also die Hilfsfunktion $h:[a,b]\to \mathbb {R} $ mit $h(x)=f(x)-x$. Wie sich herausstellt, können wir den Nullstellensatz von Bolzano einsetzen, um die Existenz einer Nullstelle zu beweisen. Hierfür müssen wir die Voraussetzungen dieses Satzes beweisen:

\begin{itemize}
\item $h$ ist stetig.
\item Null ist ein Zwischenwert von $h(a)$ und $h(b)$.
\end{itemize}

$h$ ist stetig als Verknüpfung stetiger Funktionen und wir können außerdem beweisen, dass $h(a)\leq 0\leq h(b)$ ist. Der Nullstellensatz liefert damit die Existenz des Grenzwerts.

\end{solutionprocess*}

\begin{proof*}[Fixpunktsatz]
Sei $f:[a,b]\to [a,b]$ eine stetige Funktion. Wir definieren die Hilfsfunktion $h:[a,b]\to \mathbb {R} :x\mapsto h(x)=f(x)-x$. Für diese gilt:

\begin{itemize}
\item $h$ ist stetig auf $[a,b]$ als Differenz der stetigen Funktionen $f$ und ${\text{id}}:[a,b]\to [a,b]:x\mapsto x$.
\item Es ist $h(b)\leq 0\leq h(a)$, weil: \begin{itemize}
\item $h(a)=\underbrace {f(a)} _{\in [a,b]}-a\geq 0$
\item $h(b)=\underbrace {f(b)} _{\in [a,b]}-b\leq 0$.
\end{itemize}


\end{itemize}

Damit erfüllt $h$ die Voraussetzungen des Nullstellensatzes. Es gibt daher ein ${\tilde {x}}\in [a,b]$ mit $h({\tilde {x}})=f({\tilde {x}})-{\tilde {x}}=0$. Es folgt $f({\tilde {x}})={\tilde {x}}$. Also besitzt $f$ einen Fixpunkt.

\end{proof*}

\chapter{Satz vom Minimum und Maximum}

Im Folgenden werden wir uns mit stetigen Funktionen auf kompakten Intervallen beschäftigen. Dies sind Intervalle, die abgeschlossen und beschränkt, also von der Form $[a,b]$, sind. Wir werden sehen, dass solche Funktionen immer beschränkt sind und ihr Maximum und Minimum annehmen. Dieser Satz wird \emph{Satz vom Minimum und Maximum} genannt. Er wird in der Mathematik verwendet, die Existenz von Extrema stetiger Funktionen zu beweisen.

\section{Motivation}

Nehmen wir eine stetige Funktion $f$, die auf einem kompakten Intervall $[a,b]$ definiert ist. Wir betrachten also eine Funktion $f:[a,b]\to \mathbb {R} $. Diese Funktion besitzt an der Stelle $a$ den Funktionswert $f(a)$ und an der Stelle $b$ den Funktionswert $f(b)$.

\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Beispiel_für_den_Satz_vom_Maximum_und_Minimum_1.svg}{\textbf{Beispiel\allowbreak\_für\allowbreak\_den\allowbreak\_Satz\allowbreak\_vom\allowbreak\_Maximum\allowbreak\_und\allowbreak\_Minimum\allowbreak\_1.svg}} by Stephan Kulla \textit{(CC0)}}\begin{center}
\adjincludegraphics[max width=0.5\textwidth,max height=0.2\textheight]{file58beispiel95f252r95den95satz95vom95maximum95und95minimum95195b471658a39221ac3175a71d109eb407038a1549c}\end{center}

Nun ist $f$ für jede Stelle zwischen $a$ und $b$ definiert. Nun können Graphen stetiger Funktionen, die keine Unterbrechungen im Definitionsbereich besitzen, ohne Absetzen des Stifts gezeichnet werden. Der Graph von $f$ verbindet also die beiden Punkte $(a,f(a))$ und $(b,f(b))$ durch einen durchgehenden Pfad ohne Sprünge. Der folgende Graph zeigt ein Beispiel für eine mögliche Funktion $f$:

\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Beispiel_für_den_Satz_vom_Maximum_und_Minimum_2.svg}{\textbf{Beispiel\allowbreak\_für\allowbreak\_den\allowbreak\_Satz\allowbreak\_vom\allowbreak\_Maximum\allowbreak\_und\allowbreak\_Minimum\allowbreak\_2.svg}} by Stephan Kulla \textit{(CC0)}}\begin{center}
\adjincludegraphics[max width=0.5\textwidth,max height=0.2\textheight]{file58beispiel95f252r95den95satz95vom95maximum95und95minimum9529565377090b50d6a3f1d55b5eee16ad3c5751bd37e}\end{center}

Wir stellen fest, dass im obigen Beispiel die Funktionen $f$ beschränkt ist. Auch nimmt $f$ ihr Maximum und ihr Minimum als Funktionswerte an:

\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Beispiel_für_den_Satz_vom_Maximum_und_Minimum_3.svg}{\textbf{Beispiel\allowbreak\_für\allowbreak\_den\allowbreak\_Satz\allowbreak\_vom\allowbreak\_Maximum\allowbreak\_und\allowbreak\_Minimum\allowbreak\_3.svg}} by Stephan Kulla \textit{(CC0)}}\begin{center}
\adjincludegraphics[max width=0.5\textwidth,max height=0.2\textheight]{file58beispiel95f252r95den95satz95vom95maximum95und95minimum9539560b0507ad18c6b2234772ab88f956b55e3c35ed4}\end{center}

Ist dies immer so? Probiere selbst, die Punkte $(a,f(a))$ und $(b,f(b))$ über einen Graphen zu verbinden, wobei du beim Zeichnen den Stift nicht absetzen darfst. Ist es möglich, so den Graphen einer unbeschränkten Funktion zu zeichnen?

Nein. Egal wie groß oder wie klein die Funktionswerte werden, irgendwann muss man umkehren, um den Endpunkt des Graphen zu erreichen. So bleibt die Funktion beschränkt. In der folgenden Grafik sehen wir, dass der Graph von $f$ zwar sehr große Werte annimmt – jedoch bleibt er beschränkt. Um den Endpunkt zu erreichen, muss man beim Zeichnen des Graphen irgendwann umkehren, wodurch der Graph nicht ins Unendliche wachsen kann:

\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Beispiel_für_den_Satz_vom_Maximum_und_Minimum_4.svg}{\textbf{Beispiel\allowbreak\_für\allowbreak\_den\allowbreak\_Satz\allowbreak\_vom\allowbreak\_Maximum\allowbreak\_und\allowbreak\_Minimum\allowbreak\_4.svg}} by Stephan Kulla \textit{(CC0)}}\begin{center}
\adjincludegraphics[max width=0.5\textwidth,max height=0.2\textheight]{file58beispiel95f252r95den95satz95vom95maximum95und95minimum95495c5f7b4d1938c4978c5b88306468739f89c6aa713}\end{center}

Wenn wir die Punkte $(a,f(a))$ und $(b,f(b))$ ohne Absetzen des Stifts miteinander verbinden, dann bleibt nach unserer Überlegung die Funktion beschränkt. Außerdem scheint sie immer ihr Maximum und ihr Minimum anzunehmen. Weil die beiden Randpunkte $a$ und $b$ des Intervalls $[a,b]$ zum Definitionsbereich dazugehören, muss die Funktion dort einen konkreten Wert besitzen. Damit ist die Funktion am Rand „gefangen“ und kann dort nicht gegen Unendlich streben. Die Stetigkeit von $f$ verhindert wiederum ein Streben der Funktion innerhalb des Definitionsbereichs gegen Unendlich, weil der Graph zusammenhängend gezeichnet sein muss. Diese intuitive Erklärung ist natürlich noch weit von einem formalen Beweis entfernt. Für ein erstes Verständnis des Satzes ist sie aber nützlich.

\section{Satz vom Minimum und Maximum}

\begin{theorem*}[Satz vom Minimum und Maximum]
Jede auf einem kompakten Intervall $[a,b]$ definierte stetige Funktion ist beschränkt und nimmt ihr Maximum und Minimum an. Sei also $f:[a,b]\to \mathbb {R} $ mit $a,b\in \mathbb {R} $ und $a<b$ eine stetige Funktion. Es gibt Argumente ${\tilde {x}},{\hat {x}}\in [a,b]$, so dass für alle Argumente $x\in [a,b]$ die Ungleichung $f({\hat {x}})\leq f(x)\leq f({\tilde {x}})$ erfüllt ist.

\end{theorem*}

\begin{proof*}[Satz vom Minimum und Maximum]
Sei $f:[a,b]\to \mathbb {R} $ eine stetige Funktion mit $a,b\in \mathbb {R} $ und $a<b$. Zunächst werden wir beweisen, dass $f$ nach oben beschränkt ist und sein Maximum annimmt. Auf analoge Art und Weise kann die Beschränktheit nach unten und die Annahme des Minimums als Funktionswert gezeigt werden.

Hierzu betrachten wir das Bild $f([a,b])$. Dies ist die Menge aller Funktionswerte, die $f$ annimmt. Nun nehmen wir das Supremum $\sup f([a,b])$ der Menge $f([a,b])$, wobei wir das uneigentliche Supremum $\sup f([a,b])=\infty $ explizit erlauben. Wenn $f$ nach oben unbeschränkt ist, dann ist $\sup f([a,b])=\infty $ und ansonsten ist $\sup([a,b])\in \mathbb {R} $ (weil $f([a,b])\neq \emptyset $ ist, kann der Fall $\sup \emptyset =-\infty $ nicht auftreten).

Nun wissen wir, dass es eine Folge in $f([a,b])$ gibt, die gegen $\sup f([a,b])$ konvergiert (für jede nicht leere Menge $M$ gibt es eine Folge aus $M$, die gegen $\sup M$ konvergiert). Damit gibt es eine Folge $(x_{n})_{n\in \mathbb {N} }$ von Argumenten aus $[a,b]$ mit $\lim _{n\to \infty }f(x_{n})=\sup f([a,b])$.

Nun können wir den Satz von Bolzano-Weierstraß anwenden. Dieser besagt, dass jede Folge aus einem Intervall der Form $[a,b]$ mit $a,b\in \mathbb {R} $ und $a<b$ eine konvergente Teilfolge besitzt. Damit besitzt auch $(x_{n})_{n\in \mathbb {N} }$ eine konvergente Teilfolge $\left(x_{n_{k}}\right)_{k\in \mathbb {N} }$. Sei ${\tilde {x}}$ der Grenzwert der konvergenten Teilfolge $\left(x_{n_{k}}\right)_{k\in \mathbb {N} }$. Wegen $a\leq x_{n_{k}}\leq b$ für alle $k\in \mathbb {N} $ ist auch $a\leq {\tilde {x}}\leq b$ und damit ${\tilde {x}}\in [a,b]$. Also ist ${\tilde {x}}$ ein Argument der Funktion $f$. Weil die Funktion $f$ stetig ist, gilt nach dem Folgenkriterium der Stetigkeit

\begin{align*}
f\left({\tilde {x}}\right)=f\left(\lim _{k\to \infty }x_{n_{k}}\right)=\lim _{k\to \infty }f\left(x_{n_{k}}\right)=\sup f([a,b])
\end{align*}

$\sup f([a,b])$ ist ein Funktionswert von $f$ und damit eine reelle Zahl. Dies zeigt, dass $f$ nach oben beschränkt ist. Außerdem haben wir gezeigt, dass $f$ den Wert $\sup f([a,b])$ an der Stelle ${\tilde {x}}$ annimmt. Damit ist $f(x)\leq f({\tilde {x}})$ für alle $x\in [a,b]$ und somit $f({\tilde {x}})$ das Maximum aller Funktionswerte von $f$.

\end{proof*}

\chapter{Gleichmäßige Stetigkeit}

Die gleichmäßige Stetigkeit ist eine stärkere Form der Stetigkeit. Sie leitet sich aus dem Epsilon-Delta-Kriterium der Stetigkeit ab und spielt insbesondere bei der Approximation von Funktionen eine wichtige Rolle.

\section{Motivation}

\subsection{Herleitung der gleichmäßigen Stetigkeit}

Das Epsilon-Delta-Kriterium garantiert uns so die Approximierbarkeit einer stetigen Funktion $f:D\to \mathbb {R} $. Für jeden Maximalfehler $\epsilon >0$ und jede betrachtete Stützstelle ${\tilde {x}}$ finden wir ein $\delta _{\tilde {x}}>0$, so dass sich der Funktionswert $f(x)$ für jedes Argument $x$ im Deltabereich $({\tilde {x}}-\delta ,{\tilde {x}}+\delta )$ von $f({\tilde {x}})$ um maximal $\epsilon >0$ unterscheidet. Für jedes Argument $x$ mit ${\tilde {x}}-\delta <x<{\tilde {x}}+\delta $ kann $f({\tilde {x}})$ als Annäherung von $f(x)$ mit einem maximalen Fehler von $\epsilon $ verwendet werden. Folgende Abbildung illustriert dies für einige eingezeichnete Stellen $x_{i}$:

\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Nonuniform_approximation_of_a_function.svg}{\textbf{Nonuniform\allowbreak\_approximation\allowbreak\_of\allowbreak\_a\allowbreak\_function.svg}} by Stephan Kulla \textit{(CC0)}}\begin{center}
\adjincludegraphics[max width=0.38\textwidth,max height=0.2\textheight]{file58nonuniform95approximation95of95a95function95fc905bba583382c5ce96ebe638a1d7560b483a25}\end{center}

Jedoch hängen die gefundenen $\delta _{\tilde {x}}$-Werte von der betrachteten Stelle ${\tilde {x}}$ ab. Deswegen sind die Rechtecke in der obigen Grafik auch unterschiedlich groß. Um eine gleichmäßigere Approximation zu erhalten, können wir zusätzlich fordern, dass alle Rechtecke in der Approximation gleich groß sein sollen. D.h. der $\delta _{\tilde {x}}$-Wert soll für jedes ${\tilde {x}}$ gleich sein. Obige Abbildung sähe dann wie folgt aus:

\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Uniform_approximation_of_a_function.svg}{\textbf{Uniform\allowbreak\_approximation\allowbreak\_of\allowbreak\_a\allowbreak\_function.svg}} by Stephan Kulla \textit{(CC0)}}\begin{center}
\adjincludegraphics[max width=0.38\textwidth,max height=0.2\textheight]{file58uniform95approximation95of95a95function9557a9f0805806f62a1723dabee40b2114f93a5ae2}\end{center}

Dies ist die Kernidee der gleichmäßigen Stetigkeit. Bei ihr findet man für ein vorgegebenes $\epsilon >0$ ein \emph{globales} $\delta >0$, so dass egal welche Stelle ${\tilde {x}}\in D$ man betrachtet, jeder Funktionswert $f(x)$ aus dem Delta-Bereich $({\tilde {x}}-\delta ,{\tilde {x}}+\delta )$ einen Abstand kleiner als $\epsilon $ von $f({\tilde {x}})$ besitzt. Damit erhalten wir folgende Definition der gleichmäßigen Stetigkeit einer Funktion $f:D\to \mathbb {R} $, welche eine gleichmäßige Approximierbarkeit ermöglicht:

\begin{importantparagraph*}
Für jedes $\epsilon >0$ existiert ein $\delta >0$ (unabhängig von der Stelle ${\tilde {x}}$), so dass für jede Stelle ${\tilde {x}}\in D$ und jedes Argument $x\in D$ mit $|x-{\tilde {x}}|<\delta $ die Ungleichung $|f(x)-f({\tilde {x}})|<\epsilon $ erfüllt ist.

\end{importantparagraph*}

\section{Definition}

\subsection{Definition der gleichmäßigen Stetigkeit}

Damit können wir die formale Definition der gleichmäßigen Stetigkeit wie folgt aufschreiben:

\begin{definition*}[Gleichmäßige Stetigkeit]
Eine Funktion $f:D\to \mathbb {R} $ ist gleichmäßig stetig auf D, falls zu jedem $\epsilon >0$ ein $\delta >0$ existiert, so dass sich für alle Stellen ${\tilde {x}}\in D$ und für alle Argumente $x\in D$ mit einem Abstand kleiner als $\delta $ von ${\tilde {x}}$ die Funktionswerte $f(x)$ und $f({\tilde {x}})$ um weniger als $\epsilon $ unterscheiden. In Quantorenschreibweise lautet die Definition der gleichmäßigen Stetigkeit:

\begin{align*}
\forall \epsilon >0\,\exists \delta >0\,\forall {\tilde {x}}\in D\,\forall x\in D:|x-{\tilde {x}}|<\delta \implies |f(x)-f({\tilde {x}})|<\epsilon 
\end{align*}

\end{definition*}

Anders formuliert heißt dies, dass es zu jedem $\epsilon >0$ ein $\delta >0$ gibt, so dass alle Paare $x,{\tilde {x}}\in D$ mit $|x-{\tilde {x}}|<\delta $ die Ungleichung $|f(x)-f({\tilde {x}})|<\epsilon $ erfüllen.

\subsection{Negation der gleichmäßigen Stetigkeit}

\begin{definition*}[Nicht gleichmäßige Stetigkeit]
Eine Funktion $f:D\to \mathbb {R} $ heißt nicht gleichmäßig stetig, wenn es mindestens ein $\epsilon >0$ existiert, bei dem es egal für welches $\delta >0$ jeweils mindestens zwei Argumente ${\tilde {x}}$ und $x$ mit einem Abstand kleiner als $\delta $ gibt, so dass die Funktionswerte $f(x)$ und $f({\tilde {x}})$ mindestens einen Abstand von $\epsilon $ haben.

\end{definition*}

\section{Visualisierung}

\subsection{Visualisierung der gleichmäßigen Stetigkeit}

Bei der gleichmäßigen Stetigkeit ist das $\delta $ unabhängig von der betrachteten Stelle. Damit muss der Graph komplett im Inneren des Rechtecks verlaufen, egal mit welchen Punkt des Graphen als Mittelpunkt man es betrachtet. Sprich: Für jedes $\epsilon >0$ muss es ein $\delta >0$ geben, so dass man das $2\epsilon $-$2\delta $-Rechteck beliebig am kompletten Graphen entlang verschieben kann, ohne dass es Funktionswerte direkt ober- bzw. unterhalb des Rechtecks gibt:

\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Gleichmäßig_stetige_Funktion.svg}{\textbf{Gleichmäßig\allowbreak\_stetige\allowbreak\_Funktion.svg}} by Claudia Renner, Stephan Kulla \textit{(CC BY-SA 4.0)}}\begin{center}
\adjincludegraphics[max width=0.5\textwidth,max height=0.2\textheight]{file58gleichm228223ig95stetige95funktion95a17ca4e35cb86f162447afacc7b63b2d4216a8bd}\end{center}

Bei einer nicht gleichmäßig stetigen Funktion ist dies nicht möglich. Nehmen wir als Gegenbeispiel die Quadratfunktion. Für ein beliebiges $\epsilon >0$ können wir kein $\delta >0$ setzen, so dass der Graph überall komplett im Inneren des $2\epsilon $-$2\delta $-Rechtecks verläuft, egal wo wir dieses Rechteck ansetzen. Zwar kann bei $x$-Werten in der Nähe der Null der Graph im Inneren des Rechtecks liegen, weil sich dort die Quadratfunktion wenig ändert, aber je mehr wir das Rechteck nach rechts verschieben, desto stärker ist der Anstieg der Quadratfunktion. Irgendwann ist dieser so stark, dass Funktionswerte direkt oberhalb bzw. unterhalb des $2\epsilon $-$2\delta $-Rechtecks liegen. Damit ist die Quadratfunktion ein Beispiel einer stetigen Funktion, die nicht gleichmäßig stetig ist:

\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Nicht_gleichmäßig_stetige_Funktion.svg}{\textbf{Nicht\allowbreak\_gleichmäßig\allowbreak\_stetige\allowbreak\_Funktion.svg}} by FareedaObeid, Stephan Kulla \textit{(CC BY-SA 4.0)}}\begin{center}
\adjincludegraphics[max width=0.5\textwidth,max height=0.2\textheight]{file58nicht95gleichm228223ig95stetige95funktion95f15983e7c546a035f2d2904ea386d15d732e9995}\end{center}

\section{Beweisschema}

\subsection{Beweisschema: Gleichmäßige Stetigkeit}

In Quantorenschreibweise lautet die Definition der gleichmäßigen Stetigkeit:

\begin{align*}
{\color {Red}\forall \epsilon >0}\ {\color {Orange}\exists \delta >0}\ {\color {OliveGreen}\forall {\tilde {x}},x\in D\ {\big (}|x-{\tilde {x}}|<\delta }{\color {Blue}{}\implies |f(x)-f({\tilde {x}})|<\epsilon {\big )}}
\end{align*}

Aus dieser Aussage kann ein Schema zum Beweis der gleichmäßigen Stetigkeit abgeleitet werden:

\begin{importantparagraph*}
{\textcolor{Red}{Sei ${\color {Red}\epsilon >0}$ beliebig.}} {\textcolor{Orange}{Wähle ${\color {Orange}\delta =\ldots }$ in Abhängigkeit von ${\color {Orange}\epsilon .}$}} {\textcolor{OliveGreen}{Seien ${\color {OliveGreen}{\tilde {x}},x\in D}$ beliebig mit ${\color {OliveGreen}|x-{\tilde {x}}|<\delta .}$}} {\textcolor{Blue}{Dann ist:}}

\begin{align*}
{\color {Blue}|f(x)-f({\tilde {x}})|<\ldots <\epsilon }
\end{align*}

\end{importantparagraph*}

\subsection{Beweisschema: Nicht gleichmäßige Stetigkeit}

In Prädikatenlogik lautet die Definition einer nicht gleichmäßig stetigen Funktion $f$:

\begin{align*}
{\color {Red}\exists \epsilon >0}\ {\color {Orange}\forall \delta >0}\ {\color {OliveGreen}\exists {\tilde {x}},x\in D}:{\color {RedViolet}|x-{\tilde {x}}|<\delta }\land {\color {Blue}|f(x)-f({\tilde {x}})|\geq \epsilon }
\end{align*}

Daraus ergibt sich ein Schema für den Beweis, dass eine Funktion nicht gleichmäßig stetig ist:

\begin{importantparagraph*}
{\textcolor{Red}{Wähle ${\color {Red}\epsilon =\ldots }$.}} {\textcolor{Orange}{Sei ${\color {Orange}\delta >0}$ beliebig.}} {\textcolor{OliveGreen}{Wähle ${\color {OliveGreen}{\tilde {x}}=\ldots }$ und ${\color {OliveGreen}x=\ldots }$. Die Zahlen ${\color {OliveGreen}x}$ und ${\color {OliveGreen}{\tilde {x}}}$ liegen in ${\color {OliveGreen}D}$, weil ...}} {\textcolor{RedViolet}{Es ist:}}

\begin{align*}
{\color {RedViolet}{\text{Beweis für }}|x-{\tilde {x}}|<\delta }
\end{align*}

Außerdem ist:

\begin{align*}
{\color {Blue}{\text{Beweis für }}|f(x)-f({\tilde {x}})|\geq \epsilon }
\end{align*}

\end{importantparagraph*}

\section{Eigenschaften}

Wie wir gesehen haben, ist nicht jede stetige Funktion auch gleichmäßig stetig. Dies trifft jedoch zu, wenn wir den Definitionsbereich einer stetigen Funktion auf ein abgeschlossenes, kompaktes Intervall $[a,b]$ einschränken:

\begin{theorem*}[Heine-Cantor für $\mathbb {R} $]
Jede stetige Funktion $f:[a,b]\to \mathbb {R} $ ist gleichmäßig stetig.

\end{theorem*}

\begin{proof*}[Heine-Cantor für $\mathbb {R} $]
Wir wählen einen indirekten Beweis und nehmen an, die Funktion $f:[a,b]\to \mathbb {R} $ sei nicht gleichmäßig stetig. Das heißt, es gibt ein $\varepsilon >0$ und zu jedem $n\in \mathbb {N} $ gibt es zwei Punkte $x_{n},x'_{n}\in [a,b]$, so dass $|x_{n}-x'_{n}|<{\tfrac {1}{n}}$ und $|f(x_{n})-f(x'_{n})|\geq \varepsilon $.

Nach dem Satz von Bolzano-Weierstraß besitzt die beschränkte Folge $(x_{n})_{n\in \mathbb {N} }$ eine konvergente Teilfolge $(x_{n_{k}})_{k\in \mathbb {N} }$, deren Grenzwert $x$ im Intervall $[a,b]$ enthalten ist. Dieser ist wegen $|x_{n_{k}}-x'_{n_{k}}|<{\tfrac {1}{n_{k}}}$ ebenfalls Grenzwert der Folge $(x'_{n_{k}})_{k\in \mathbb {N} }$.

Aus der Stetigkeit von $f$ folgt $f(x_{n_{k}})\to f(x)$ und $f(x'_{n_{k}})\to f(x)$. Daher gibt es ein $k_{0}$, so dass $|f(x_{n_{k}})-f(x)|<{\tfrac {\varepsilon }{2}}$ und $|f(x'_{n_{k}})-f(x)|<{\tfrac {1}{n_{K}}}$ für alle $k\geq k_{0}$. Daraus folgt nun $|f(x_{n_{k}})-f(x'_{n_{k}})|=|(f(x_{n_{k}})-f(x))+(f(x)-f(x'_{n_{k}}))|\leq |(f(x_{n_{k}})-f(x))|+|(f(x)-f(x'_{n_{k}}))|<{\tfrac {\varepsilon }{2}}+{\tfrac {\varepsilon }{2}}=\varepsilon $ für alle $k\geq k_{0}$ im Widerspruch zu unserer Annahme $|f(x_{n_{k}})-f(x'_{n_{k}})|\geq \varepsilon $ für alle $k$. Daher war die gemachte Annahme falsch und es folgt die gleichmäßige Stetigkeit.

\end{proof*}

\part{Ableitung}

\addxcontentsline{lof}{part}[\arabic{part}]{Ableitung}\begin{authors}
Who2010, Stephan Kulla, Autorenkollektiv „Auswahlaxiom“ (Charlotte Dietze, Matthias Paulsen, Anne Reif), Matthias Greger, EulerschesPi, BeateAsenbeck, Sven Prüfer, Matheoldie, Meitnerium266, Fabian Wietschorke, Jetstune, Agnes Pauer, Menuja J. (MJ Studies), Alexander Sedlmayr, Paolo Martinoni, Werner Fröhlich, Dirk Hünniger, SerloBot, Lukasstockner, Talonnn, Esclear, Peter Gröbner, Claudia Renner, Jochen Burghardt\end{authors}

\chapter{Ableitung}

Mit der Ableitung werden wir eines der wichtigsten Konzepte der Analysis kennenlernen. Die Ableitung entspricht der Änderungsrate einer Funktion. Sie wird in den Naturwissenschaften oft genutzt, um in mathematischen Modellen die Veränderung eines Systems zu modellieren. Mit Hilfe der Ableitung können wir eine Funktion auf viele ihrer Eigenschaften untersuchen.

\section{Intuitionen der Ableitung}

Für die Ableitung gibt es mehrere Intuitionen, die alle eng zusammenhängen:

\begin{itemize}
\item \emph{Ableitung als momentane Änderungsrate:} Die Ableitung entspricht dem, was wir intuitiv als momentane Änderungsrate einer Funktion verstehen. Eine Änderungsrate beschreibt dabei, wie stark sich eine Größe bezüglich einer anderen Bezugsgröße ändert. Bei der momentanen Änderungsrate wird diese Bezugsgröße als „unendlich klein“ angenommen. Es wird also der Grenzwert der Änderungsrate betrachtet, wenn die Bezugsgröße gegen Null konvergiert. Ein Beispiel hierfür ist die Geschwindigkeit. Diese ist die momentane Änderungsrate des Ortes bezüglich der Zeit und gibt an wie stark sich der Ort eines Objekts zu einem bestimmten Zeitpunkt mit der Zeit ändert.
\item \emph{Ableitung als Tangentensteigung:} Die Ableitung entspricht der Steigung, die die Tangente des Graphen an der Stelle der Ableitung besitzt. Damit löst die Ableitung das geometrische Problem, die Tangente an einen Graphen durch einen Punkt zu bestimmen.
\item \emph{Ableitung als Steigung der lokal besten linearen Approximation:} Jede an einer Stelle ableitbare Funktion kann in einer Umgebung um diesen Punkt gut durch eine lineare Funktion approximiert werden. Die Ableitung entspricht der Steigung dieser linearen Funktion. Damit kann die Ableitung genutzt werden, um Funktionen lokal durch lineare Funktionen gut anzunähern.
\item \emph{Ableitung als verallgemeinerte Steigung:} Zunächst ist der Begriff der Steigung einer Funktion nur für lineare Funktionen definiert. Man kann die Ableitung aber benutzen, um die Steigung auch für nicht-lineare Funktionen zu definieren.
\end{itemize}

Diese Intuitionen werden wir im Folgenden detailliert besprechen und aus ihnen eine formale Definition der Ableitung herleiten. Außerdem werden wir sehen, dass ableitbare Funktionen „knickfrei“ sind, weshalb sie auch \emph{glatte Funktionen} genannt werden.

\section{Ableitung als momentane Änderungsrate}

\subsection{Berechnung der Ableitung}

Die Ableitung entspricht der momentanen Änderungsrate einer Funktion $f$. Wie kann diese momentane Änderungsrate einer Funktion bestimmt oder definiert werden? Sei zum Beispiel $f$ eine reellwertige Funktion, die folgenden Graph besitzt:

\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Illustration_of_differential_quotient_1.svg}{\textbf{Illustration\allowbreak\_of\allowbreak\_differential\allowbreak\_quotient\allowbreak\_1.svg}} by Stephan Kulla \textit{(CC0)}}\begin{center}
\adjincludegraphics[max width=0.5\textwidth,max height=0.2\textheight]{file58illustration95of95differential95quotient9519532b36165d946019d8bce85ed4c1994eb61fba093}\end{center}

So kann $f$ eine physikalische Größe in Abhängigkeit von einer anderen Größe beschreiben. Beispielsweise könnte $f(x)$ dem zurückgelegten Weg eines Objekts zum Zeitpunkt $x$ entsprechen. $f(x)$ könnte auch der Luftdruck in der Höhe $x$ oder die Populationsgröße einer Art zum Zeitpunkt $x$ sein. Nehmen wir nun das Argument ${\tilde {x}}$, an dem die Funktion den Funktionswert $f({\tilde {x}})$ besitzt:

\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Illustration_of_differential_quotient_2.svg}{\textbf{Illustration\allowbreak\_of\allowbreak\_differential\allowbreak\_quotient\allowbreak\_2.svg}} by Stephan Kulla \textit{(CC0)}}\begin{center}
\adjincludegraphics[max width=0.5\textwidth,max height=0.2\textheight]{file58illustration95of95differential95quotient9529566a76c0b62bc0bf2f0a6e8319da6e5101b72fd7a}\end{center}

Nehmen wir einmal an, dass $f(x)$ der zurückgelegte Weg eines Autos zum Zeitpunkt $x$ ist. Dann ist die momentane Änderungsrate von $f$ an der Stelle ${\tilde {x}}$ gleich der Geschwindigkeit des Objekts zum Zeitpunkt ${\tilde {x}}$. Wie kann diese Geschwindigkeit bestimmt werden?

Anstatt die Geschwindigkeit direkt zu berechnen, können wir sie schätzen. Wir nehmen einen Zeitpunkt $x_{1}$ in der Zukunft und schauen, welchen Weg das Auto im Zeitraum von ${\tilde {x}}$ bis $x_{1}$ zurückgelegt hat. Der in dieser Zeit zurückgelegte Weg ist gleich der Differenz $f(x_{1})-f({\tilde {x}})$, während die Zeitdifferenz gleich $x_{1}-{\tilde {x}}$ ist. Nun ist die Geschwindigkeit gleich dem Quotienten ${\tfrac {\text{Weg}}{\text{Zeit}}}$. Damit hat das Auto im Zeitraum von ${\tilde {x}}$ nach $x_{1}$ die durchschnittliche Geschwindigkeit

\begin{align*}
{\frac {f(x_{1})-f({\tilde {x}})}{x_{1}-{\tilde {x}}}}
\end{align*}

Dieser Quotient, der die durchschnittliche Änderungsrate von der Funktion $f$ im Intervall $[{\tilde {x}},x_{1}]$ angibt, wird \emph{Differenzenquotient} genannt. Entsprechend seines Namens ist er ein Quotient von zwei Differenzen. In folgender Abbildung sehen wir, dass dieser Differenzenquotient gleich der Steigung derjenigen Sekante ist, die durch die Punkte $({\tilde {x}},f({\tilde {x}}))$ und $(x_{1},f(x_{1}))$ geht:

\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Illustration_of_differential_quotient_3.svg}{\textbf{Illustration\allowbreak\_of\allowbreak\_differential\allowbreak\_quotient\allowbreak\_3.svg}} by Stephan Kulla \textit{(CC0)}}\begin{center}
\adjincludegraphics[max width=0.5\textwidth,max height=0.2\textheight]{file58illustration95of95differential95quotient95395fca769f9d5ec0bc4de57ac3aa7529c8964d6ae0e}\end{center}

Diese durchschnittliche Geschwindigkeit ist eine erste Approximation der aktuellen Geschwindigkeit unseres Autos zum Zeitpunkt ${\tilde {x}}$. Nun muss die Bewegung des Autos zwischen den Zeitpunkten ${\tilde {x}}$ und $x_{1}$ nicht gleichförmig verlaufen sein – es kann beschleunigen oder abbremsen. Die momentane Geschwindigkeit zum Zeitpunkt ${\tilde {x}}$ ist also im Allgemeinen eine andere als die durchschnittliche Geschwindigkeit im Zeitraum zwischen ${\tilde {x}}$ und $x_{1}$. Ein besseres Ergebnis sollten wir erhalten, wenn wir den Zeitraum für die Berechnung der durchschnittlichen Geschwindigkeit verkürzen. Wir betrachten also einen Zeitpunkt $x_{2}$, der näher an ${\tilde {x}}$ liegt, und bestimmen die durchschnittliche Geschwindigkeit ${\tfrac {f(x_{2})-f({\tilde {x}})}{x_{2}-{\tilde {x}}}}$ für den neuen Zeitraum zwischen ${\tilde {x}}$ und $x_{2}$:

\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Illustration_of_differential_quotient_4.svg}{\textbf{Illustration\allowbreak\_of\allowbreak\_differential\allowbreak\_quotient\allowbreak\_4.svg}} by Stephan Kulla \textit{(CC0)}}\begin{center}
\adjincludegraphics[max width=0.5\textwidth,max height=0.2\textheight]{file58illustration95of95differential95quotient95495669ebf5dce7b9460101e397d8eee40e842848a14}\end{center}

Diesen Prozess wiederholen wir beliebig oft. Wir betrachten also eine Folge $(x_{n})_{n\in \mathbb {N} }$ von Zeitpunkten, die alle von ${\tilde {x}}$ verschieden sind und die gegen ${\tilde {x}}$ konvergieren. Für jedes $x_{n}$ berechnen wir die durchschnittliche Geschwindigkeit ${\tfrac {f(x_{n})-f({\tilde {x}})}{x_{n}-{\tilde {x}}}}$ des Autos im Zeitraum von ${\tilde {x}}$ bis $x_{n}$. Je kürzer $x_{n}-{\tilde {x}}$ ist, desto weniger sollte das Auto in diesem Zeitraum beschleunigen oder abbremsen können und umso mehr entspricht dann die durchschnittliche Geschwindigkeit der momentanen Geschwindigkeit des Autos zum Zeitpunkt ${\tilde {x}}$:

\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Illustration_of_differential_quotient_5.svg}{\textbf{Illustration\allowbreak\_of\allowbreak\_differential\allowbreak\_quotient\allowbreak\_5.svg}} by Stephan Kulla \textit{(CC0)}}\begin{center}
\adjincludegraphics[max width=0.5\textwidth,max height=0.2\textheight]{file58illustration95of95differential95quotient955958b6f47682cdb928b0eee22d946a0f71a463bb20c}\end{center}

Weil der Zeitabstand $x_{n}-x$ beliebig klein wird (es ist $\lim _{n\to \infty }x_{n}-x=0$), sollte die Folge der Durchschnittsgeschwindigkeiten $\left({\tfrac {f(x_{n})-f({\tilde {x}})}{x_{n}-{\tilde {x}}}}\right)_{n\in \mathbb {N} }$ gleich der momentanen Geschwindigkeit des Autos zum Zeitpunkt ${\tilde {x}}$ sein.

Damit haben wir eine Methode gefunden, um die momentane Änderungsrate von $f$ an der Stelle ${\tilde {x}}$ zu bestimmen: Wir nehmen eine beliebige Folge von Argumenten $(x_{n})_{n\in \mathbb {N} }$, die alle verschieden von ${\tilde {x}}$ sind und für die $\lim _{n\to \infty }x_{n}={\tilde {x}}$ ist. Für jedes $x_{n}$ bestimmen wir den Quotienten ${\tfrac {f(x_{n})-f({\tilde {x}})}{x_{n}-{\tilde {x}}}}$. Die momentane Änderungsrate ist der Grenzwert dieser Quotienten:

\begin{align*}
{\text{Änderungsrate von }}f{\text{ an der Stelle }}{\tilde {x}}=\lim _{n\to \infty }{\frac {f(x_{n})-f({\tilde {x}})}{x_{n}-{\tilde {x}}}}
\end{align*}

Für die Ableitung von $f$ an der Stelle ${\tilde {x}}$ schreiben wir $f'({\tilde {x}})$. Damit können wir notieren:

\begin{align*}
f'({\tilde {x}})=\lim _{n\to \infty }{\frac {f(x_{n})-f({\tilde {x}})}{x_{n}-{\tilde {x}}}}
\end{align*}

Der dabei auftretende Grenzwert der Differenzenquotienten wird \emph{Differentialquotient} genannt.

\subsection{Konkretisierung}

Nun haben wir in unserem Beispiel stets Zeitpunkte in der Zukunft von ${\tilde {x}}$ betrachtet. Was passiert, wenn wir einen Zeitpunkt $x_{n}$ in der Vergangenheit von ${\tilde {x}}$ betrachten? Hier erhalten wir folgendes Bild:

\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Illustration_of_differential_quotient_6.svg}{\textbf{Illustration\allowbreak\_of\allowbreak\_differential\allowbreak\_quotient\allowbreak\_6.svg}} by Stephan Kulla \textit{(CC0)}}\begin{center}
\adjincludegraphics[max width=0.5\textwidth,max height=0.2\textheight]{file58illustration95of95differential95quotient956952bc306e8d08e5b4bc677ef550561f3f7ed555695}\end{center}

Die durchschnittliche Geschwindigkeit im Zeitraum von $x_{n}$ bis ${\tilde {x}}$ ist dann gleich ${\tfrac {f({\tilde {x}})-f(x_{n})}{{\tilde {x}}-x_{n}}}$. Wenn wir diesen Quotienten um $-1$ erweitern, erhalten wir:

\begin{align*}
{\frac {f({\tilde {x}})-f(x_{n})}{{\tilde {x}}-x_{n}}}={\frac {-\left(f({\tilde {x}})-f(x_{n})\right)}{-({\tilde {x}}-x_{n})}}={\frac {f(x_{n})-f({\tilde {x}})}{x_{n}-{\tilde {x}}}}
\end{align*}

Wir erhalten denselben Term wie im vorherigen Abschnitt. Dieser gibt die durchschnittliche Geschwindigkeit an, egal ob $x_{n}<{\tilde {x}}$ oder $x_{n}>{\tilde {x}}$ ist. Damit sollte dessen Wert im Fall $x_{n}<{\tilde {x}}$ auch nah an der momentanen Geschwindigkeit des Autos zum Zeitpunkt ${\tilde {x}}$ liegen, wenn $x_{n}$ nur hinreichend nah an ${\tilde {x}}$ liegt. Es ist also

\begin{align*}
f'({\tilde {x}})=\lim _{n\to \infty }{\frac {f(x_{n})-f({\tilde {x}})}{x_{n}-{\tilde {x}}}}
\end{align*}

wobei $(x_{n})_{n\in \mathbb {N} }$ eine beliebige Folge von Argumenten ungleich ${\tilde {x}}$ mit $\lim _{n\to \infty }x_{n}={\tilde {x}}$ ist. Die Folgenglieder von $(x_{n})_{n\in \mathbb {N} }$ können dabei je nach Index $n$ manchmal größer und manchmal kleiner als ${\tilde {x}}$ sein:

\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Illustration_of_differential_quotient_7.svg}{\textbf{Illustration\allowbreak\_of\allowbreak\_differential\allowbreak\_quotient\allowbreak\_7.svg}} by Stephan Kulla \textit{(CC0)}}\begin{center}
\adjincludegraphics[max width=0.5\textwidth,max height=0.2\textheight]{file58illustration95of95differential95quotient957950e19014982eda40035d3c161d664f3b148414714}\end{center}

\subsection{Verfeinerung der Definition}

Sei nun $f:D\to \mathbb {R} $ eine beliebige reellwertige Funktion und sei ${\tilde {x}}\in D$. Wie wir im obigen Abschnitt gesehen haben, ist

\begin{align*}
f'({\tilde {x}})=\lim _{n\to \infty }{\frac {f(x_{n})-f({\tilde {x}})}{x_{n}-{\tilde {x}}}}
\end{align*}

wobei $\left(x_{n}\right)_{n\in \mathbb {N} }$ eine Folge von Argumenten ungleich ${\tilde {x}}$ ist, die gegen ${\tilde {x}}$ konvergiert. Damit es mindestens eine solche Folge von Argumenten gibt, muss ${\tilde {x}}$ ein Häufungspunkt vom Definitionsbereich $D$ sein (eine Zahl ist genau dann Häufungspunkt einer Menge, wenn es eine Folge in dieser Menge ungleich dieser Zahl gibt, die gegen diese Zahl konvergiert). Das hört sich jetzt vielleicht komplizierter an, als es häufig ist. In den meisten Fällen ist $D\subseteq \mathbb {R} $ ein Intervall und dann ist jedes ${\tilde {x}}\in D$ ein Häufungspunkt von $D$. Für die Definition des Differentialquotienten soll es egal sein, welche Folge $(x_{n})_{n\in \mathbb {N} }$ wir wählen. Dementsprechend können wir die Ableitung definieren:

\begin{importantparagraph*}
Sei $f:D\to \mathbb {R} $ mit $D\subseteq \mathbb {R} $ und sei ${\tilde {x}}\in D$ ein Häufungspunkt von $D$. Die Funktion $f$ ist an der Stelle ${\tilde {x}}$ ableitbar mit der Ableitung $f'({\tilde {x}})$, wenn für jede Folge $(x_{n})_{n\in \mathbb {N} }$ von Argumenten ungleich ${\tilde {x}}$ und mit $\lim _{n\to \infty }x_{n}={\tilde {x}}$ gilt:

\begin{align*}
f'({\tilde {x}})=\lim _{n\to \infty }{\frac {f(x_{n})-f({\tilde {x}})}{x_{n}-{\tilde {x}}}}
\end{align*}

\end{importantparagraph*}

Nun können wir diese Definition abkürzen, indem wir die Grenzwertdefinition für Funktionen benutzen. Zur Erinnerung: Es ist nach Definition genau dann $\lim _{x\to c}g(x)=L$, wenn $\lim _{n\to \infty }g(x_{n})=L$ für alle Folgen $(x_{n})_{n\in \mathbb {N} }$ von Argumenten ungleich $c$ mit $\lim _{n\to \infty }x_{n}=c$ ist. Also:

\begin{importantparagraph*}
Sei $f:D\to \mathbb {R} $ mit $D\subseteq \mathbb {R} $ und sei ${\tilde {x}}\in D$ ein Häufungspunkt von $D$. Die Funktion $f$ ist an der Stelle ${\tilde {x}}$ ableitbar mit der Ableitung $f'({\tilde {x}})$, wenn gilt:

\begin{align*}
f'({\tilde {x}})=\lim _{x\to {\tilde {x}}}{\frac {f(x)-f({\tilde {x}})}{x-{\tilde {x}}}}
\end{align*}

\end{importantparagraph*}

\subsection{Die h-Methode}

Es gibt eine weitere Möglichkeit, die Ableitung zu definieren. Hierzu gehen wir vom Differentialquotienten $\lim _{x\to {\tilde {x}}}{\tfrac {f(x)-f({\tilde {x}})}{x-{\tilde {x}}}}$ aus und führen die Variablenersetzung $x={\tilde {x}}+h$ durch. Die neue Variable $h$ ist also der Unterschied zwischen der Stelle ${\tilde {x}}$, bei der die Ableitung bestimmt werden soll, zu dem Punkt, wo der Differenzenquotient gebildet wird. Für $x\to {\tilde {x}}$ geht $h\to 0$. Damit können wir die Ableitung auch definieren als

\begin{importantparagraph*}
Sei $f:D\to \mathbb {R} $ mit $D\subseteq \mathbb {R} $ und sei ${\tilde {x}}\in D$ ein Häufungspunkt von $D$. Die Funktion $f$ ist an der Stelle ${\tilde {x}}$ ableitbar mit der Ableitung $f'({\tilde {x}})$, wenn gilt:

\begin{align*}
f'({\tilde {x}})=\lim _{h\to 0}{\frac {f({\tilde {x}}+h)-f({\tilde {x}})}{h}}
\end{align*}

\end{importantparagraph*}

\subsection{Anwendungen in den Naturwissenschaften}

Die Ableitung haben wir als momentane Änderungsrate einer Größe kennengelernt. Als solche tritt sie in den Naturwissenschaften häufig auf. Folgende Größen sind beispielsweise als Änderungsraten definiert:

\begin{itemize}
\item \emph{Geschwindigkeit:} Die Geschwindigkeit ist die momentane Änderungsrate des zurückgelegten Wegs eines Objekts.
\item \emph{Beschleunigung:} Die Beschleunigung ist die momentane Änderungsrate der Geschwindigkeit eines Objekts.
\item \emph{Druckänderung:} Sei $p(h)$ der Luftdruck in der Höhe $h$. Die Ableitung $p'(h)$ ist die Änderungsrate des Luftdrucks mit der Höhe. Dieses Beispiel zeigt, dass die Änderungsrate nicht immer auf die Zeit bezogen sein muss. Es kann auch die Änderungsrate bezüglich einer anderen Größe, wie zum Beispiel der Höhe, sein.
\item \emph{Chemische Reaktionsrate:} Betrachten wir eine chemische Reaktion $A\to B$. Sei $d_{A}(t)$ die Konzentration des Stoffs $A$ zum Zeipunkt $t$. Die Ableitung $d_{A}'(t)$ ist die momentane Änderungsrate der Stoffkonzentration von $A$ und damit gibt sie an, wie viel des Stoffs $A$ in den Stoff $B$ umgesetzt wird. Damit gibt $d_{A}'(t)$ die chemische Reaktionsrate für die Reaktion $A\to B$ an.
\item \emph{Änderung der Population:} Oft betrachtet man die Anzahl an Individuen $N(t)$ in einer Population (zum Beispiel die Anzahl an Menschen auf dem Planeten, die Anzahl an Bakterien in einer Petrischale, die Anzahl an Tieren einer Gattung oder die Anzahl der Atome eines radioaktiven Stoffs). Die Ableitung $N'(t)$ gibt die momentane Änderungsrate der Individuen zum Zeitpunkt $t$ wieder.
\end{itemize}

\section{Definitionen}

\subsection{Ableitung und Differenzierbarkeit}

\begin{definition*}
Sei $f:D\to \mathbb {R} $ mit $D\subseteq \mathbb {R} $ und sei ${\tilde {x}}\in D$ ein Häufungspunkt von $D$. Die Funktion $f$ ist an der Stelle ${\tilde {x}}$ ableitbar mit der Ableitung $f'({\tilde {x}})$, wenn gilt:

\begin{align*}
f'({\tilde {x}})=\lim _{x\to {\tilde {x}}}{\frac {f(x)-f({\tilde {x}})}{x-{\tilde {x}}}}
\end{align*}

Äquivalent kann in der Definition auch gefordert werden:

\begin{align*}
f'({\tilde {x}})=\lim _{h\to 0}{\frac {f({\tilde {x}}+h)-f({\tilde {x}})}{h}}
\end{align*}

Eine an der Stelle ${\tilde {x}}$ ableitbare Funktion nennt man an der Stelle ${\tilde {x}}$ \emph{differenzierbar}. Eine Funktion heißt \emph{ableitbar} oder \emph{differenzierbar}, wenn an jeder Stelle ihres Definitionsbereichs der obige Grenzwert existiert. Differenzierbare Funktionen sind also überall, wo sie definiert sind, differenzierbar.

\end{definition*}

\subsection{Differenzenquotient und Differentialquotient}

Die Begriffe „Differenzenquotient“ und „Differentialquotient“ sind folgendermaßen definiert:

\begin{align*}
\underbrace {f'({\tilde {x}})} _{\text{Ableitung}}=\underbrace {\lim _{x\to {\tilde {x}}}\underbrace {\tfrac {f(x)-f({\tilde {x}})}{x-{\tilde {x}}}} _{\text{Differenzenquotient}}} _{\text{Differentialquotient}}
\end{align*}

\subsection{Ableitungsfunktion}

Ist eine Funktion $f:D\to \mathbb {R} $ mit $D\subseteq \mathbb {R} $ an jeder Stelle ihres Definitionsbereichs differenzierbar, so besitzt $f$ an jedem Punkt in $D$ eine Ableitung. Die Funktion, die jedem Argument ${\tilde {x}}$ ihre Ableitung $f'({\tilde {x}})$ zuordnet, heißt Ableitungsfunktion von $f$:

\begin{definition*}[Ableitungsfunktion]
Sei $f:D\to \mathbb {R} $ eine differenzierbare Funktion mit $D\subseteq \mathbb {R} $. Wir definieren die Ableitungsfunktion $f':D\to \mathbb {R} $ durch

\begin{align*}
f'({\tilde {x}}):=\lim _{x\to {\tilde {x}}}{\frac {f(x)-f({\tilde {x}})}{x-{\tilde {x}}}}
\end{align*}

Ist die Ableitungsfunktion $f'$ zusätzlich noch stetig, so nennt man $f$ \emph{stetig differenzierbar}.

\end{definition*}

\begin{warning*}
Die Begriffe „stetig differenzierbar“ und „differenzierbar“ sind nicht äquivalent. Die Stetigkeit der Ableitungsfunktion eine echte zusätzliche Forderung.

\end{warning*}

\section{Ableitung als Tangentensteigung}

Die Ableitung $f'({\tilde {x}})$ entspricht dem Grenzwert $\lim _{x\to {\tilde {x}}}{\tfrac {f(x)-f({\tilde {x}})}{x-{\tilde {x}}}}$. Dabei ist der Differenzenquotient ${\tfrac {f(x)-f({\tilde {x}})}{x-{\tilde {x}}}}$ die Steigung der Sekante zwischen den Punkten $({\tilde {x}},f({\tilde {x}}))$ und $(x,f(x))$. Bei der Grenzwertbildung $x\to {\tilde {x}}$ geht diese Sekante in die Tangente über, die den Graphen von $f$ im Punkt $({\tilde {x}},f({\tilde {x}}))$ berührt:

\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Differencial_quotient_of_a_function.svg}{\textbf{Differencial\allowbreak\_quotient\allowbreak\_of\allowbreak\_a\allowbreak\_function.svg}} by Johannes Schneider \textit{(CC BY-SA 4.0)}}\begin{center}
\adjincludegraphics[max width=0.5\textwidth,max height=0.2\textheight]{file58differencial95quotient95of95a95function95cdaced80fd5d0331f37228118cf6b8a39441a2b8}\end{center}

Damit ist die Ableitung $f'({\tilde {x}})$ gleich der Steigung der Tangente am Graphen durch den Punkt $({\tilde {x}},f({\tilde {x}}))$. Die Ableitung kann also genutzt werden, um die Tangente an einem Graphen zu bestimmen. Somit löst sie auch ein geometrisches Problem. Mit $f'({\tilde {x}})$ kennen wir die Steigung der Tangente und mit $({\tilde {x}},f({\tilde {x}}))$ einen Punkt auf der Tangente. Damit können wir die Funktionsgleichung dieser Tangente bestimmen.

\section{Ableitung als Steigung der lokal besten linearen Approximation}

\subsection{Approximation einer differenzierbaren Funktion}

Die Ableitung kann auch zur Approximation einer Funktion genutzt werden. Um diese Approximation zu finden gehen wir von der Grenzwertdefinition der Ableitung aus:

\begin{align*}
f'({\tilde {x}})=\lim _{x\to {\tilde {x}}}{\frac {f(x)-f({\tilde {x}})}{x-{\tilde {x}}}}
\end{align*}

Der Differenzenquotient ${\tfrac {f(x)-f({\tilde {x}})}{x-{\tilde {x}}}}$ liegt also beliebig nah an der Ableitung $f'({\tilde {x}})$, wenn $x$ hinreichend nah an ${\tilde {x}}$ ist. Für $x\approx {\tilde {x}}$ können wir schreiben:

\begin{align*}
f'({\tilde {x}})\approx {\frac {f(x)-f({\tilde {x}})}{x-{\tilde {x}}}}
\end{align*}

Im Folgenden nehmen wir an, dass der Ausdruck $x\approx {\tilde {x}}$ für „$x$ ist ungefähr so groß wie ${\tilde {x}}$“ wohldefiniert ist und den üblichen Rechengesetzen für Gleichungen gehorcht. Damit können wir diese Gleichung umstellen zu

\begin{align*}
&&f'({\tilde {x}})&\approx {\frac {f(x)-f({\tilde {x}})}{x-{\tilde {x}}}}\\[0.3em]&\implies {}&f'({\tilde {x}})\cdot (x-{\tilde {x}})&\approx f(x)-f({\tilde {x}})\\[0.3em]&\implies {}&f({\tilde {x}})+f'({\tilde {x}})\cdot (x-{\tilde {x}})&\approx f(x)\\[0.3em]&\implies {}&f(x)&\approx f({\tilde {x}})+f'({\tilde {x}})\cdot (x-{\tilde {x}})
\end{align*}

Wenn $x$ hinreichend nah an ${\tilde {x}}$ liegt, dann ist $f(x)$ ungefähr gleich dem Wert $f({\tilde {x}})+f'({\tilde {x}})\cdot (x-{\tilde {x}})$. Dieser Wert kann somit in der Nähe der Ableitungsstelle als Approximation von $f(x)$ verwendet werden. Dabei ist die Funktion mit der Zuordnungsvorschrift $x\mapsto f({\tilde {x}})+f'({\tilde {x}})\cdot (x-{\tilde {x}})$ eine lineare Funktion, da ${\tilde {x}}$ ein beliebiger aber fester Punkt ist.

Die Zuordnungsvorschrift $t(x)=f({\tilde {x}})+f'({\tilde {x}})\cdot (x-{\tilde {x}})$ beschreibt dabei die Tangente, die den Funktionsgraphen an der Stelle der Ableitung berührt. Die Tangente ist also in der Nähe des Berührungspunkts eine gute Approximation des Funktionsgraphen. Dies zeigt auch das folgende Diagramm. Wenn man in einer differenzierbaren Funktion an einer Stelle nah genug reinzoomt, so sieht der Funktionsgraph näherungsweise wie eine Gerade aus:

\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Approximation_of_cos_with_linear_functions_without_numbers.svg}{\textbf{Approximation\allowbreak\_of\allowbreak\_cos\allowbreak\_with\allowbreak\_linear\allowbreak\_functions\allowbreak\_without\allowbreak\_numbers.svg}} by Stephan Kulla \textit{(CC BY 3.0)}}\begin{center}
\adjincludegraphics[max width=0.5\textwidth,max height=0.2\textheight]{file58approximation95of95cos95with95linear95functions95without95numbers952065552f7f6eff855f1d9d543dc471448c88812b}\end{center}

Diese Gerade wird durch die Zuordnungsvorschrift $t(x)=f({\tilde {x}})+f'({\tilde {x}})\cdot (x-{\tilde {x}})$ beschrieben und entspricht der Tangente des Graphen an dieser Stelle.

\subsection{Qualität der Approximation}

Wie gut ist die Approximation $f(x)\approx f({\tilde {x}})+f'({\tilde {x}})\cdot (x-{\tilde {x}})$? Um dies zu beantworten, sei $\epsilon (x)$ derjenige Wert mit

\begin{align*}
{\frac {f(x)-f({\tilde {x}})}{x-{\tilde {x}}}}=f'({\tilde {x}})+\epsilon (x)
\end{align*}

Der Wert $\epsilon (x)$ ist damit der Unterschied zwischen dem Differenzenquotienten ${\tfrac {f(x)-f({\tilde {x}})}{x-{\tilde {x}}}}$ und der Ableitung $f'({\tilde {x}})$. Dieser Unterschied verschwindet für den Grenzübergang $x\to {\tilde {x}}$, weil für diesen Grenzübergang der Differenzenquotient in den Differentialquotienten, also der Ableitung $f'({\tilde {x}})$, übergeht. Es gilt also $\lim _{x\to {\tilde {x}}}\epsilon (x)=0$. Nun können wir die obige Gleichung umstellen und erhalten so

\begin{align*}
&&{\frac {f(x)-f({\tilde {x}})}{x-{\tilde {x}}}}&=f'({\tilde {x}})+\epsilon (x)\\[0.3em]&\implies {}&f(x)-f({\tilde {x}})&=f'({\tilde {x}})\cdot (x-{\tilde {x}})+\epsilon (x)\cdot (x-{\tilde {x}})\\[0.3em]&\implies {}&f(x)&=f({\tilde {x}})+f'({\tilde {x}})\cdot (x-{\tilde {x}})+\underbrace {\epsilon (x)\cdot (x-{\tilde {x}})} _{:=\ \delta (x)}\\[0.3em]&\implies {}&f(x)&=f({\tilde {x}})+f'({\tilde {x}})\cdot (x-{\tilde {x}})+\delta (x)
\end{align*}

Der Fehler zwischen $f(x)$ und $f({\tilde {x}})+f'({\tilde {x}})\cdot (x-{\tilde {x}})$ ist damit gleich dem Term $\delta (x)=\epsilon (x)\cdot (x-{\tilde {x}})$. Wegen $\lim _{x\to {\tilde {x}}}\epsilon (x)=0$ ist auch

\begin{align*}
\lim _{x\to {\tilde {x}}}\delta (x)=\lim _{x\to {\tilde {x}}}\underbrace {\epsilon (x)} _{\to 0}\cdot \underbrace {(x-{\tilde {x}})} _{\to 0}=0
\end{align*}

Der Fehler $\delta (x)$ verschwindet also für $x\to {\tilde {x}}$. Wir können aber noch mehr sagen: $\delta (x)$ fällt schneller als ein linearer Term gegen Null ab. Selbst wenn wir $\delta (x)$ durch $x-{\tilde {x}}$ teilen und so diesen Term in der Nähe von ${\tilde {x}}$ stark vergrößern, verschwindet ${\tfrac {\delta (x)}{x-{\tilde {x}}}}$ für $x\to {\tilde {x}}$. Es ist nämlich

\begin{align*}
\lim _{x\to {\tilde {x}}}{\frac {\delta (x)}{x-{\tilde {x}}}}=\lim _{x\to {\tilde {x}}}{\frac {\epsilon (x)(x-{\tilde {x}})}{x-{\tilde {x}}}}=\lim _{x\to {\tilde {x}}}\epsilon (x)=0
\end{align*}

Der Fehler $\delta (x)$ in der Approximation $f(x)\approx f({\tilde {x}})+f'({\tilde {x}})\cdot (x-{\tilde {x}})$ fällt also für $x\to {\tilde {x}}$ schneller als linear gegen Null ab. Fassen wir die bisherige Argumentation in einem Satz zusammen:

\begin{theorem*}[Approximation einer differenzierbaren Funktion]
Sei $f:D\to \mathbb {R} $ und sei ${\tilde {x}}\in D$ ein Häufungspunkt von $D$. Sei außerdem $f$ an der Stelle ${\tilde {x}}$ differenzierbar mit der Ableitung $f'({\tilde {x}})$. Seien $\epsilon $ und $\delta $ so definiert, dass für alle $x\in D$ gilt

\begin{align*}
f(x)&=f({\tilde {x}})+f'({\tilde {x}})\cdot (x-{\tilde {x}})+\epsilon (x)\cdot (x-{\tilde {x}})\\[0.3em]&=f({\tilde {x}})+f'({\tilde {x}})\cdot (x-{\tilde {x}})+\delta (x)
\end{align*}

Dann verschwindet der Fehlerterm $\epsilon (x)$ für $x\to {\tilde {x}}$, das heißt $\lim _{x\to {\tilde {x}}}\epsilon (x)=0$. Für $\delta (x)$ gilt dementsprechend $\lim _{x\to {\tilde {x}}}{\tfrac {\delta (x)}{x-{\tilde {x}}}}=0$.

\end{theorem*}

\subsection{Alternative Definition der Ableitung}

Dass differenzierbare Funktionen durch lineare Funktionen approximiert werden können, charakterisiert den Begriff der Ableitung. Jede Funktion $f$ ist an der Stelle ${\tilde {x}}$ ableitbar, wenn eine reelle Zahl $c\in \mathbb {R} $ sowie eine Funktion $\delta $ existieren, so dass $f(x)=f({\tilde {x}})+c\cdot (x-{\tilde {x}})+\delta (x)$ und $\lim _{x\to {\tilde {x}}}{\tfrac {\delta (x)}{x-{\tilde {x}}}}=0$ gelten. Ihre Ableitung ist dann $f'({\tilde {x}})=c$. Es gilt nämlich

\begin{align*}
f'({\tilde {x}})&=\lim _{x\to {\tilde {x}}}{\frac {f(x)-f({\tilde {x}})}{x-{\tilde {x}}}}\\[0.3em]&=\lim _{x\to {\tilde {x}}}{\frac {f({\tilde {x}})+c\cdot (x-{\tilde {x}})+\delta (x)-f({\tilde {x}})}{x-{\tilde {x}}}}\\[0.3em]&=\lim _{x\to {\tilde {x}}}{\frac {c\cdot (x-{\tilde {x}})+\delta (x)}{x-{\tilde {x}}}}\\[0.3em]&=\lim _{x\to {\tilde {x}}}c+\underbrace {\frac {\delta (x)}{x-{\tilde {x}}}} _{\to 0}\\[0.3em]&=c
\end{align*}

Somit können wir die Ableitung auch wie folgt definieren:

\begin{definition*}[Alternative Definition der Ableitung]
Sei $f:D\to \mathbb {R} $ eine Funktion und ${\tilde {x}}\in D$ ein Häufungspunkt von $D$. Die Funktion $f$ ist genau dann im Punkt ${\tilde {x}}$ differenzierbar mit der Ableitung $f'({\tilde {x}})\in \mathbb {R} $, wenn eine Funktion $\delta :D\to \mathbb {R} $ existiert, so dass

\begin{align*}
f(x)=f({\tilde {x}})+f'({\tilde {x}})\cdot (x-{\tilde {x}})+\delta (x)
\end{align*}

und $\lim _{x\to {\tilde {x}}}{\tfrac {\delta (x)}{x-{\tilde {x}}}}=0$ gelten.

\end{definition*}
\clearpage
\section{Beispiele}

\subsection{Beispiel einer differenzierbaren Funktion}

\begin{example*}[Quadratfunktion ist an der Stelle $3$ ableitbar]
Die Quadratfunktion $f:\mathbb {R} \to \mathbb {R} :x\mapsto x^{2}$ ist ableitbar an der Stelle $x_{0}=3$ mit der Ableitung $6$. Dieses Resultat erhalten wir, wenn wir den Differentialquotienten an der Stelle $x_{0}=3$ auswerten:

\begin{align*}
f'(3)&=\lim _{h\to 0}{\frac {f(3+h)-f(3)}{h}}=\lim _{h\to 0}{\frac {(3+h)^{2}-3^{2}}{h}}\\[0.3em]&=\lim _{h\to 0}{\frac {9+6h+h^{2}-9}{h}}=\lim _{h\to 0}{\frac {6h+h^{2}}{h}}=\lim _{h\to 0}{(6+h)}
\end{align*}

Der letzte Ausdruck zeigt, dass der Differenzenquotient gleich $6+h$ für $h\neq 0$ ist (für $h=0$ ist der Differenzenquotient nicht definiert, weil sonst durch Null geteilt wird). Nun müssen wir den Grenzwert von $6+h$ für $h\to 0$ bestimmen:

\begin{align*}
\lim _{h\to 0}{(6+h)}=6+0=6
\end{align*}

Damit ist die Ableitung von $f$ an der Stelle $x_{0}=3$ gleich $6$, also $f'(3)=6$. Analog können wir die Ableitung von $f$ an einer beliebigen Stelle ${\tilde {x}}\in \mathbb {R} $ bestimmen:

\begin{align*}
f'({\tilde {x}})&=\lim _{h\to 0}{\frac {f({\tilde {x}}+h)-f({\tilde {x}})}{h}}=\lim _{h\to 0}{\frac {({\tilde {x}}+h)^{2}-{\tilde {x}}^{2}}{h}}\\[0.3em]&=\lim _{h\to 0}{\frac {{\tilde {x}}^{2}+2{\tilde {x}}h+h^{2}-{\tilde {x}}^{2}}{h}}=\lim _{h\to 0}{\frac {2{\tilde {x}}h+h^{2}}{h}}\\[0.3em]&=\lim _{h\to 0}{(2{\tilde {x}}+h)}=2{\tilde {x}}
\end{align*}

Damit ist die Ableitung der Quadratfunktion an der Stelle ${\tilde {x}}$ gleich $f'({\tilde {x}})=2{\tilde {x}}$. Die Ableitungsfunktion von $f$ ist damit die Funktion $f':\mathbb {R} \to \mathbb {R} :x\mapsto 2x$.

\end{example*}

\subsection{Beispiel einer nicht differenzierbaren Funktion}

\begin{example*}[Betragsfunktion ist nicht differenzierbar]
Wir betrachten die Betragsfunktion $f:\mathbb {R} \to \mathbb {R} ,x\mapsto |x|$ und prüfen, ob sie an der Stelle $x_{0}=0$ ableitbar ist. Hier wählen wir die Folgen $(x_{n})_{n\in \mathbb {N} }$, $({\tilde {x}}_{n})_{n\in \mathbb {N} }$ und $({\hat {x}}_{n})_{n\in \mathbb {N} }$ mit

\begin{align*}
x_{n}={\frac {1}{n}},\quad {\tilde {x}}_{n}=-{\frac {1}{n}},\quad {\hat {x}}_{n}=(-1)^{n}{\frac {1}{n}}
\end{align*}

Diese konvergieren alle gegen $x_{0}=0$. Nun betrachten wir die Differentialquotienten zu den einzelnen Folgen. Für $(x_{n})_{n\in \mathbb {N} }$ ergibt sich:

\begin{align*}
\lim _{n\rightarrow \infty }{\frac {f(x_{n})-f(x_{0})}{x_{n}-x_{0}}}&=\lim _{n\rightarrow \infty }{\frac {|{\frac {1}{n}}|-|0|}{{\frac {1}{n}}-0}}=\lim _{n\rightarrow \infty }{\frac {\frac {1}{n}}{\frac {1}{n}}}\\[0.3em]&=\lim _{n\rightarrow \infty }{1}=1
\end{align*}

Für $({\tilde {x}}_{n})_{n\in \mathbb {N} }$ bekommen wir:

\begin{align*}
\lim _{n\rightarrow \infty }{\frac {f({\tilde {x}}_{n})-f(x_{0})}{{\tilde {x}}_{n}-x_{0}}}&=\lim _{n\rightarrow \infty }{\frac {|-{\tfrac {1}{n}}|-|0|}{-{\tfrac {1}{n}}-0}}=\lim _{n\rightarrow \infty }{\frac {\tfrac {1}{n}}{-{\tfrac {1}{n}}}}\\[0.3em]&=\lim _{n\rightarrow \infty }{-1}=-1
\end{align*}

Für $({\hat {x}}_{n})_{n\in \mathbb {N} }$ gilt:

\begin{align*}
\lim _{n\rightarrow \infty }{\frac {f({\hat {x}}_{n})-f(x_{0})}{{\hat {x}}_{n}-x_{0}}}&=\lim _{n\rightarrow \infty }{\frac {|(-1)^{n}{\tfrac {1}{n}}|-|0|}{(-1)^{n}{\tfrac {1}{n}}-0}}\\[0.3em]&=\lim _{n\rightarrow \infty }{\frac {\tfrac {1}{n}}{(-1)^{n}{\tfrac {1}{n}}}}=\lim _{n\rightarrow \infty }{(-1)^{n}}
\end{align*}

Dieser Grenzwert für die Folge $({\hat {x}}_{n})_{n\in \mathbb {N} }$ existiert nicht. Wir sehen daher, dass je nach gewählter Folge $(a_{n})_{n\in \mathbb {N} }$ der Grenzwert $\lim _{n\to \infty }{\tfrac {f(a_{n})-f(x_{0})}{a_{n}-x_{0}}}$ unterschiedlich ist oder nicht existiert. Damit existiert nach Definition auch nicht der Grenzwert $\lim _{x\to x_{0}}{\tfrac {f(x)-f(x_{0})}{x-x_{0}}}$, womit die Funktion $f$ an der Stelle $x_{0}=0$ nicht ableitbar ist. Die Betragsfunktion besitzt am Nullpunkt keine Ableitung.

\end{example*}

\section{Links- und rechtsseitige Ableitung}

\subsection{Definition}

Die Ableitung einer Funktion $f:D\to \mathbb {R} $ ist der Grenzwert des Differenzenquotienten ${\tfrac {f(x)-f({\tilde {x}})}{x-{\tilde {x}}}}$ für $x\to {\tilde {x}}$. Der Differenzenquotient kann dabei als eine Funktion $D\setminus \{{\tilde {x}}\}\to \mathbb {R} $ aufgefasst werden, die für alle $x\in D$ außer für $x={\tilde {x}}$ definiert ist. Damit handelt es sich beim Grenzwert $\lim _{x\to {\tilde {x}}}{\tfrac {f(x)-f({\tilde {x}})}{x-{\tilde {x}}}}$ um einen Grenzwert einer Funktion.

Die Begriffe \href{https://de.wikibooks.org/wiki/Mathe\_für\_Nicht-Freaks:\_Grenzwert\_von\_Funktionen}
{„linksseitiger und rechtsseitiger Grenzwert“} können auch für den Differenzenquotienten betrachtet werden. So erhalten wir die Begriffe „linksseitige“ beziehungsweise „rechtsseitige“ Ableitung. Bei der linksseitigen Ableitung werden nur Sekanten links von der betrachteten Stelle evaluiert. Es werden also nur Differenzenquotienten ${\tfrac {f(x)-f({\tilde {x}})}{x-{\tilde {x}}}}$ betrachtet, bei der $x<{\tilde {x}}$ ist. Dann wird überprüft, ob diese Differenzenquotienten für den Grenzübergang $x\to {\tilde {x}}$ gegen eine Zahl konvergieren. Wenn ja, dann ist diese Zahl der linksseitige Grenzwert. Also:

\begin{align*}
{f_{-}}'({\tilde {x}})=\lim _{x\uparrow {\tilde {x}}}{\frac {f(x)-f({\tilde {x}})}{x-{\tilde {x}}}}
\end{align*}

Dabei ist ${f_{-}}'({\tilde {x}})$ die Schreibweise für die linksseitige Ableitung von $f$ an der Stelle ${\tilde {x}}$. Damit dieser Grenzwert Sinn ergibt, muss es mindestens eine Folge $(x_{n})_{n\in \mathbb {N} }$ von Argumenten geben, die von links gegen ${\tilde {x}}$ konvergiert. Es muss also ${\tilde {x}}$ ein Häufungspunkt der Menge $D\cap (-\infty ,{\tilde {x}})=\{x\in D:x<{\tilde {x}}\}$ sein.

\begin{definition*}[Linksseitige Ableitung]
Sei $f:D\to \mathbb {R} $ eine Funktion und ${\tilde {x}}$ ein Häufungspunkt der Menge $\{x\in D:x<{\tilde {x}}\}$. Die Zahl ${f_{-}}'({\tilde {x}})$ ist die linksseitige Ableitung von $f$ an der Stelle ${\tilde {x}}$, wenn gilt

\begin{align*}
{f_{-}}'({\tilde {x}})=\lim _{x\uparrow {\tilde {x}}}{\frac {f(x)-f({\tilde {x}})}{x-{\tilde {x}}}}
\end{align*}

Dies ist äquivalent dazu, dass für alle Folgen $(x_{n})_{n\in \mathbb {N} }$ aus $\mathbb {N} $ mit $x_{n}\in D$ und $x_{n}<{\tilde {x}}$ sowie $\lim _{n\to \infty }x_{n}={\tilde {x}}$ gilt

\begin{align*}
{f_{-}}'({\tilde {x}})=\lim _{n\to \infty }{\frac {f(x_{n})-f({\tilde {x}})}{x_{n}-{\tilde {x}}}}
\end{align*}

\end{definition*}

Auf analoge Weise kann die rechtsseitige Ableitung folgendermaßen definiert werden:

\begin{definition*}[Rechtsseitige Ableitung]
Sei $f:D\to \mathbb {R} $ eine Funktion und ${\tilde {x}}$ ein Häufungspunkt der Menge $\{x\in D:x>{\tilde {x}}\}$. Die Zahl ${f_{+}}'({\tilde {x}})$ ist die rechtsseitige Ableitung von $f$ an der Stelle ${\tilde {x}}$, wenn gilt

\begin{align*}
{f_{+}}'({\tilde {x}})=\lim _{x\downarrow {\tilde {x}}}{\frac {f(x)-f({\tilde {x}})}{x-{\tilde {x}}}}
\end{align*}

Dies ist äquivalent dazu, dass für alle Folgen $(x_{n})_{n\in \mathbb {N} }$ aus $\mathbb {N} $ mit $x_{n}\in D$ und $x_{n}>{\tilde {x}}$ sowie $\lim _{n\to \infty }x_{n}={\tilde {x}}$ gilt

\begin{align*}
{f_{+}}'({\tilde {x}})=\lim _{n\to \infty }{\frac {f(x_{n})-f({\tilde {x}})}{x_{n}-{\tilde {x}}}}
\end{align*}

\end{definition*}

Funktionen besitzen an einer Stelle in ihrem Definitionsbereich nur dann einen Grenzwert, wenn sowohl der linksseitige als auch der rechtsseitige Grenzwert an dieser Stelle existieren und beide Grenzwerte übereinstimmen. Diesen Satz können wir direkt auf Ableitungen anwenden:

\begin{importantparagraph*}
Eine Funktion ist an einer Stelle in ihrem Definitionsbereich genau dann ableitbar, wenn dort sowohl die linksseitige als auch die rechtsseitige Ableitung existieren und beide Ableitungen übereinstimmen.

\end{importantparagraph*}

\subsection{Beispiel}

Wir haben bereits gezeigt, dass die Betragsfunktion $f:\mathbb {R} \to \mathbb {R} :x\mapsto |x|$ an der Stelle ${\tilde {x}}=0$ nicht differenzierbar ist. Jedoch können wir zeigen, dass die rechtsseitige Ableitung an dieser Stelle existiert und gleich $1$ ist:

\begin{align*}
{f_{+}}'(0)&=\lim _{x\downarrow 0}{\frac {f(x)-f(0)}{x-0}}=\lim _{x\downarrow 0}{\frac {|x|-|0|}{x}}\\[0.3em]&\ {\color {OliveGreen}\left\downarrow \ x>0\implies |x|=x\right.}\\[0.3em]&=\lim _{x\downarrow 0}{\frac {x-0}{x}}=\lim _{x\downarrow 0}1=1
\end{align*}

Analog können wir zeigen, dass die linksseitige Ableitung an derselben Stelle gleich $-1$ ist:

\begin{align*}
{f_{-}}'(0)&=\lim _{x\uparrow 0}{\frac {f(x)-f(0)}{x-0}}=\lim _{x\uparrow 0}{\frac {|x|-|0|}{x}}\\[0.3em]&\ {\color {OliveGreen}\left\downarrow \ x<0\implies |x|=-x\right.}\\[0.3em]&=\lim _{x\uparrow 0}{\frac {-x-0}{x}}=\lim _{x\uparrow 0}-1=-1
\end{align*}

Weil die rechtsseitige und die linksseitige Ableitung nicht übereinstimmen, ist die Betragsfunktion an der Stelle ${\tilde {x}}=0$ nicht ableitbar. Sie besitzt dort zwar links- und rechtsseitige Ableitungen, aber keine Ableitung.

\section{Zusammenhang zwischen Differenzierbarkeit, Stetigkeit und stetiger Differenzierbarkeit}

Stetige Differenzierbarkeit einer Funktion $f$ impliziert ihre Differenzierbarkeit, woraus wiederum ihre Stetigkeit folgt. Die Umkehrungen gelten im Allgemeinen nicht, wie wir im Laufe dieses Abschnitts sehen werden:

\begin{align*}
&{\text{Stetige Differenzierbarkeit}}\\\implies {}&{\text{Differenzierbarkeit}}\\\implies {}&{\text{Stetigkeit}}
\end{align*}

Die erste Implikation folgt direkt aus der Definition: Eine Funktion $f$ heißt genau dann stetig differenzierbar, wenn sie differenzierbar ist \textbf{und} die Ableitungsfunktion $f'$ stetig ist. Damit sind stetig differenzierbare Funktionen auch differenzierbar. Die zweite Implikation zeigen wir im Folgenden.
\subsection{Jede differenzierbare Funktion ist stetig}

Wir zeigen nun, dass jede an einer Stelle differenzierbare Funktion an dieser Stelle auch stetig ist. Damit ist Differenzierbarkeit eine stärkere Forderung an eine Funktion als Stetigkeit:
\clearpage

\begin{theorem*}
Sei $f:D\to \mathbb {R} $ mit $D\subseteq \mathbb {R} $ eine Funktion, die an der Stelle ${\tilde {x}}\in D$ differenzierbar ist. Dann ist $f$ im Punkt ${\tilde {x}}$ stetig. Damit gilt auch: Jede differenzierbare Funktion $f$ ist stetig.

\end{theorem*}

\begin{proof*}
Sei $(x_{n})_{n\in \mathbb {N} }$ eine beliebige Folge in $D$, die gegen ${\tilde {x}}$ konvergiert. Da $f$ in ${\tilde {x}}\in D$ differenzierbar ist, gibt es eine Funktion $\delta :D\to \mathbb {R} $ mit $\lim _{x\to {\tilde {x}}}{\tfrac {\delta (x)}{x-{\tilde {x}}}}=0$, so dass für alle $x$ in $D$ gilt

\begin{align*}
f(x)=f({\tilde {x}})+f'({\tilde {x}})\cdot (x-{\tilde {x}})+\delta (x)
\end{align*}

Wir haben uns bereits überlegt, dass dann auch $\lim _{x\to {\tilde {x}}}\delta (x)=0$ gilt. Wegen $\lim _{n\to \infty }{x_{n}}={\tilde {x}}$ muss also $\lim _{n\to \infty }{\delta (x_{n})}=0$ gelten. Insgesamt erhalten wir somit:

\begin{align*}
&\lim _{n\to \infty }{f(x_{n})}\\[0.3em]&{\color {OliveGreen}\left\downarrow \ f(x)=f({\tilde {x}})+f'({\tilde {x}})\cdot (x-{\tilde {x}})+\delta (x)\right.}\\[0.3em]=\ &\lim _{n\to \infty }{f({\tilde {x}})+f'({\tilde {x}})\cdot (x_{n}-{\tilde {x}})+\delta (x_{n})}\\[0.3em]&{\color {OliveGreen}\left\downarrow \ {\text{Limes auseinanderziehen}}\right.}\\[0.3em]=\ &\lim _{n\to \infty }\underbrace {f({\tilde {x}})} _{\to f({\tilde {x}})}+\lim _{n\to \infty }f'({\tilde {x}})\cdot \underbrace {(x_{n}-{\tilde {x}})} _{\to 0}+\lim _{n\to \infty }\underbrace {\delta (x_{n})} _{\to 0}\\[0.3em]=\ &f({\tilde {x}})+0+0\\[0.3em]=\ &f({\tilde {x}})
\end{align*}

Den Limes durften wir hier auseinanderziehen, da die Grenzwerte $\lim _{n\to \infty }{f({\tilde {x}})}=f({\tilde {x}})$, $\lim _{n\to \infty }{f'({\tilde {x}})\cdot (x_{n}-{\tilde {x}})}=0$ und $\lim _{n\to \infty }{\delta (x_{n})}=0$ existieren. Nach dem Folgenkriterium für Stetigkeit gilt wegen $\lim _{n\to \infty }{f(x_{n})}=f({\tilde {x}})$, dass $f$ an der Stelle ${\tilde {x}}$ stetig ist.

\end{proof*}

\subsection{Anwendung: Unstetige Funktionen sind nicht differenzierbar}

Aus dem vorherigen Abschnitt wissen wir, dass jede differenzierbare Abbildung stetig ist. Also:

\begin{align*}
{\text{Differenzierbarkeit}}\implies {\text{Stetigkeit}}
\end{align*}

Wenn wir auf diese Implikation das Prinzip der Kontraposition anwenden, dann folgt: Unstetige Funktionen sind nicht differenzierbar:

\begin{align*}
{\text{Unstetigkeit}}\implies {\text{Nichtdifferenzierbarkeit}}
\end{align*}

\subsection{Nicht jede differenzierbare Funktion ist stetig differenzierbar}

Im folgenden Beispiel greifen wir Kenntnisse über Ableitungsregeln vor, die wir erst im nächsten Kapitel ausführlicher behandeln werden. Da jene allerdings meist schon aus der Schule bekannt sind, führen wir das Beispiel bereits jetzt vor:

\begin{example*}[Beispiel einer differenzierbaren, aber nicht stetig differenzierbaren Funktion]
Wir werden zeigen, dass folgende Funktion überall differenzierbar ist, aber die Ableitungsfunktion nicht an jedem Punkt stetig ist:

\begin{align*}
f:\mathbb {R} \to \mathbb {R} :x\mapsto f(x)={\begin{cases}x^{2}\cdot \sin \left({\frac {1}{x}}\right)&x\neq 0\\0&x=0\end{cases}}
\end{align*}

Für ${\tilde {x}}\neq 0$ ist die Funktion nach der Produkt- und Kettenregel in jedem Punkt unendlich oft stetig differenzierbar. Wir werden nun die Differenzierbarkeit an der Stelle ${\tilde {x}}=0$ betrachten. Es gilt

\begin{align*}
f'(0)=\lim _{h\to 0}{\frac {h^{2}\sin \left({\frac {1}{h}}\right)-0}{h}}=\lim _{h\to 0}\underbrace {h} _{\to 0}\cdot \underbrace {\sin \left({\frac {1}{h}}\right)} _{\in [-1,1]}=0
\end{align*}

Also ist $f$ an der Stelle ${\tilde {x}}=0$ differenzierbar mit dem Ableitungswert $f'(0)=0$. Jedoch ist die Ableitungsfunktion $f'$ an der Stelle ${\tilde {x}}=0$ nicht stetig. Um dies zu zeigen, müssen wir die Ableitungsfunktion ermitteln. Für ${\tilde {x}}\neq 0$ folgt aus der Produkt- und Kettenregel:

\begin{align*}
f'({\tilde {x}})&=\left(x^{2}\cdot \sin \left({\frac {1}{x}}\right)\right)'\\[0.3em]&=2x\sin \left({\frac {1}{x}}\right)+x^{2}\cos \left({\frac {1}{x}}\right)\left(-{\frac {1}{x^{2}}}\right)\\[0.3em]&=2x\cdot \sin \left({\frac {1}{x}}\right)-\cos \left({\frac {1}{x}}\right)
\end{align*}

Zusammen mit dem Ableitungswert $f'(0)=0$ erhalten wir somit die Ableitungsfunktion

\begin{align*}
f':\mathbb {R} \to \mathbb {R} :x\mapsto f'(x)={\begin{cases}2x\cdot \sin \left({\frac {1}{x}}\right)-\cos \left({\frac {1}{x}}\right)&x\neq 0\\0&x=0\end{cases}}
\end{align*}

Um die Unstetigkeit von $f'$ bei ${\tilde {x}}=0$ zu zeigen, verwenden wir die Folgendefinition von Stetigkeit. Sei dazu $(x_{n})_{n\in \mathbb {N} }$ die Folge mit $x_{n}={\tfrac {1}{n\pi }}$. Es gilt $\lim _{n\to \infty }x_{n}=0$. Wenn $f'$ stetig wäre, müsste nach dem Folgenkriterium $\lim _{n\to \infty }f'(x_{n})=0=f'(0)=f'\left(\lim _{n\to \infty }x_{n}\right)$ gelten. Nun ist aber

\begin{align*}
\lim _{n\to \infty }f'(x_{n})&=\lim _{n\to \infty }\left(2x_{n}\cdot \sin \left({\frac {1}{x_{n}}}\right)-\cos \left({\frac {1}{x_{n}}}\right)\right)\\[0.3em]&=\lim _{n\to \infty }\left(2{\frac {1}{n\pi }}\cdot \sin(n\pi )-\cos(n\pi )\right)\\[0.3em]&=\lim _{n\to \infty }\left(2{\frac {1}{n\pi }}\cdot 0-(-1)^{n}\right)\\[0.3em]&=\lim _{n\to \infty }-(-1)^{n}
\end{align*}

Der Grenzwert $\lim _{n\to \infty }(-1)^{n}$ existiert nicht, denn die Folge $\left((-1)^{n}\right)_{n\in \mathbb {N} }$ besitzt die beiden Häufungspunkte $1$ und $-1$. Damit folgt, dass $f'$ an der Stelle ${\tilde {x}}=0$ nicht stetig ist. $f$ ist somit zwar differenzierbar, aber nicht stetig differenzierbar.

\end{example*}

\chapter{Ableitungsregeln}

Wir haben im letzten Kapitel die Ableitungsfunktion $f':D\to \mathbb {R} $ einer differenzierbaren Funktion $f:D\to \mathbb {R} $ folgendermaßen definiert: $f'({\tilde {x}}):=\lim _{x\rightarrow {\tilde {x}}}{\tfrac {f(x)-f({\tilde {x}})}{x-{\tilde {x}}}}$. Das ist jedoch oft eine sehr umständliche Art, die Ableitungsfunktion einer konkret gegebenen Funktion zu ermitteln. Nimm zum Beispiel die Funktion $g:\mathbb {R} \to \mathbb {R} $ mit $g(x)=x^{2}\cdot \ln(x)$. Zur Berechnung ihrer Ableitung müssten wir $\lim _{x\rightarrow {\tilde {x}}}{\tfrac {x^{2}\cdot \ln(x)-{\tilde {x}}^{2}\cdot \ln({\tilde {x}})}{x-{\tilde {x}}}}$ für jedes ${\tilde {x}}\in \mathbb {R} $ bestimmen.

Idealerweise finden wir eine Zuordnungsfunktion für die Ableitungsfunktion, mit der wir diese direkt berechnen können und uns den Weg über den Differentialquotienten sparen. Das Schöne ist, dass es Ableitungsgesetzte gibt, mit denen eine zusammengesetze Funktion auf Ableitungen ihrer Basisfunktionen zurückgeführt wird.

\section{Übersichtstabelle der Ableitungsregeln}

Sind $f$ und $g$ differenzierbare Funktionen, so dass die Kompositionen $af$ mit $a\in \mathbb {R} $, $f+g$, $fg$, ${\tfrac {f}{g}}$ und $f\circ g$ jeweils definiert und differenzierbar sind. Dann gelten die folgenden Ableitungsregeln:


\renewcommand{\arraystretch}{1.5}

\begin{longtabu} to \linewidth {X[l]X[l]} \\ \toprule 
Name & Regel \\ 
\midrule
Faktorregel & $(af)'=af'$ \\ 
Summen- / Differenzenregel & $(f\pm g)'=f'\pm g'$ \\ 
Produktregel & $(fg)'=f'g+fg'$ \\ 
Quotientenregel & $\left({\frac {f}{g}}\right)'={\frac {gf'-fg'}{g^{2}}}$ \\ 
Reziprokenregel & $\left({\frac {1}{g}}\right)'=-{\frac {g'}{g^{2}}}$ \\ 
Kettenregel & $(f\circ g)'=(f'\circ g)\cdot g'$ \\ 
Spezialfälle der Kettenregel & ${\begin{aligned}(f^{n})'&=nf^{n-1}\cdot f'\\({\sqrt {f}})'&={\frac {f'}{2{\sqrt {f}}}}\\(\exp \circ f)'&=(\exp \circ f)\cdot f'\\(\ln \circ f)'&={\tfrac {f'}{f}}\end{aligned}}$ \\ 
\href{https://de.wikibooks.org/wiki/Mathe\_für\_Nicht-Freaks:\_Ableitung\_der\_Umkehrfunktion}
{Inversenregel} & $(f^{-1})'={\tfrac {1}{f'\circ f^{-1}}}$ \\ 
\bottomrule
\end{longtabu}
\renewcommand{\arraystretch}{1.0}
\section{Merkregeln}

Folgende Regeln erleichtern das Merken der einzelnen Ableitungsregeln:

\begin{itemize}
\item \emph{Faktorregel $(af)'=af'$:} Die Ableitung ist linear und kann damit direkt in ein Produkt einer Funktion mit einer Zahl reingezogen werden.
\item \emph{Summen- und Differenzenregel $(f\pm g)'=f'\pm g'$:} Die Ableitung ist linear und kann damit direkt in die Summe zweier Funktionen reingezogen werden.
\item \emph{Produktregel $(fg)'=f'g+fg'$:} „Erste Funktion ableiten, zweite bleibt stehen plus zweite Funktion ableiten, erste bleibt stehen“
\item \emph{Quotientenregel $\left({\tfrac {f}{g}}\right)'={\tfrac {gf'-fg'}{g^{2}}}$:} NAZ-ZAN ist die Merkregel für den Zähler („Nenner Ableitung Zähler minus Zähler Ableitung Nenner“)
\item \emph{Reziprokenregel $\left({\frac {1}{g}}\right)'=-{\frac {g'}{g^{2}}}$:} Dies ist der Spezialfall der Quotientenregel mit $f\equiv 1$ (Zähler ist konstant $1$).
\item \emph{Kettenregel $(f\circ g)'=(f'\circ g)\cdot g'$:} „Ableitung äußere Funktion mal Ableitung innere Funktion“. Vorsicht, in die Ableitung der äußeren Funktion muss die innere Funktion eingesetzt werden. Auch darf das Nachdifferenzieren der inneren Funktion nicht vergessen werden.
\end{itemize}

\chapter{Ableitung der Umkehrfunktion}

Im folgenden Artikel werden wir untersuchen, unter welchen Voraussetzungen die Umkehrfunktion einer bijektiven Funktion in einem Punkt differenzierbar ist. Außerdem werden wir eine Formel herleiten, mit der wir die Ableitung der Umkehrfunktion explizit bestimmen können. Das praktische an dieser ist, dass wir damit die Ableitung an bestimmten Punkten bestimmen können, selbst wenn wir die Umkehrfunktion nicht explizit kennen.

\section{Motivation}

Betrachten wir zunächst als Beispiel eine lineare Funktion. Für diese ist es sehr einfach, die Ableitung der Umkehrfunktion zu bestimmen. Nicht-konstante lineare Funktionen sind nämlich auf ganz $\mathbb {R} $ bijektiv und damit umkehrbar. In diesem Fall können wir die Umkehrfunktion explizit berechnen und danach ableiten. Konkret wählen wir $f:\mathbb {R} \to \mathbb {R} $ mit $f(x)=2x-1$. Die Umkehrfunktion lautet

\begin{align*}
f^{-1}:\mathbb {R} \to \mathbb {R} :f^{-1}(y)={\tfrac {1}{2}}y+{\tfrac {1}{2}}
\end{align*}

$f^{-1}$ ist auf ganz $\mathbb {R} $ differenzierbar und $(f^{-1})'(y)={\frac {1}{2}}$ für alle $y\in \mathbb {R} $.

Betrachten wir als nächstes die Funktion $f(x)=x^{2}$. Hier müssen wir zunächst aufpassen, da sie nicht auf ganz $\mathbb {R} $ injektiv, und damit nicht umkehrbar ist. Schränken wir den Definitions- und Wertebereich jedoch auf $\mathbb {R} _{0}^{+}$ ein, so ist $f:\mathbb {R} _{0}^{+}\to \mathbb {R} _{0}^{+},\ f(x)=x^{2}$ bijektiv. Die Umkehrfunktion ist die Quadratwurzelfunktion

\begin{align*}
f^{-1}:\mathbb {R} _{0}^{+}\to \mathbb {R} _{0}^{+}:f^{-1}(y)={\sqrt {y}}
\end{align*}

Bei der Differenzierbarkeit müssen wir eine weitere Sache beachten: $f^{-1}$ ist in $y=0$ nicht differenzierbar. Dies können wir mit Hilfe des Differentialquotienten, oder auch durch die folgende Überlegung zeigen:

Da die Wurzelfunktion $f^{-1}$ die Umkehrfunktion der Quadratfunktion $f$ ist, gilt $f^{-1}\circ f={\text{id}}$. In null gilt damit insbesondere

\begin{align*}
\underbrace {f^{-1}(f(0))} _{=f^{-1}(0)}={\text{id}}(0)
\end{align*}

Wäre nun $f^{-1}$ in null differenzierbar, würde mit der Kettenregel

\begin{align*}
\underbrace {(f^{-1})'(f(0))\cdot \overbrace {f'(0)} ^{=0}} _{=0}=\underbrace {{\text{id}}'(0)} _{=1}
\end{align*}

gelten. Also kann $f^{-1}$ in null nicht differenzierbar sein. Auf $\mathbb {R} ^{+}$ ist $f^{-1}$ hingegen differenzierbar, und es gilt

\begin{align*}
(f^{-1})'(y)={\tfrac {1}{2{\sqrt {y}}}}
\end{align*}

Dieses Beispiel zeigt also, dass es vorkommen kann, dass $f^{-1}$ nicht auf dem gesamten Definitionsbereich differenzierbar ist, obwohl $f$ überall differenzierbar war. Konkret liegt das daran, dass $f'(f^{-1}(0))=f'(0)=0$ ist, wie wir später sehen werden.

In den beiden Beispielen war es also relativ einfach die Ableitung der Umkehrfunktion zu bestimmen. Wie sieht es aber mit komplizierteren Funktionen, zum Beispiel $\ln $ als Umkehrfunktion von $\exp $ aus? Hier können wir nicht so einfach die Ableitung der Umkehrfunktion berechnen. Oder was passiert, wenn sich eine bijektive Funktion gar nicht explizit umkehren lässt? Gibt es dann dennoch eine Möglichkeit die Ableitung der Umkehrfunktion zu bestimmen? In diesen Fällen wäre es natürlich gut, wenn wir eine allgemeine Formel hätten, mit der wir die Ableitung von $f^{-1}$ aus der Ableitung von $f$ bestimmen könnten. Wenn wir uns die Ableitung aus dem zweiten Beispiel nochmal ansehen, dann fällt Folgendes auf:

\begin{align*}
(f^{-1})'(y)={\tfrac {1}{2{\sqrt {y}}}}={\tfrac {1}{2f^{-1}(y)}}={\tfrac {1}{f'(f^{-1}(y))}}
\end{align*}

Da $f^{-1}(y)={\sqrt {y}}$ für alle $y\in \mathbb {R} ^{+}$ und $f'(x)=2x$ für alle $x\in \mathbb {R} ^{+}$ ist. Sehen wir uns das erste Beispiel nochmal an, so gilt dort ebenfalls

\begin{align*}
(f^{-1})'(y)={\tfrac {1}{2}}={\tfrac {1}{f'(f^{-1}(y))}}
\end{align*}

Die Frage ist nun, ob dies Zufall ist, oder ob diese Formel unter gewissen Voraussetzungen auch allgemein gilt? Setzen wir voraus, dass $f:D\to W$ in ${\tilde {x}}\in D$ und $f^{-1}:W\to D$ in ${\tilde {y}}=f({\tilde {x}})\in W$ differenzierbar ist, dann können wir uns die Formel allgemein herleiten. Dazu verwenden wir denselben Ansatz, den wir oben für die Nicht-Differenzierbarkeit der Quadratwurzelfunktion in null verwendet haben: Für alle $y\in W$ gilt

\begin{align*}
y=f(f^{-1}(y))
\end{align*}

Leiten wir nun auf beiden Seiten an der Stelle ${\tilde {y}}$ ab, so gilt nach der Kettenregel

\begin{align*}
1=f'(f^{-1}({\tilde {y}}))\cdot (f^{-1})'({\tilde {y}})
\end{align*}

Hierbei haben wir verwendet, dass $f$ in $f^{-1}({\tilde {y}})={\tilde {x}}$ und $f^{-1}$ in ${\tilde {y}}$ differenzierbar sind. Nun dividieren wir noch auf beiden Seiten durch $f'(f^{-1}({\tilde {y}}))$ (geht natürlich nur, wenn der Ausdruck ungleich null ist), und erhalten

\begin{align*}
(f^{-1})'({\tilde {y}})={\frac {1}{f'(f^{-1}({\tilde {y}}))}}
\end{align*}

beziehungsweise

\begin{align*}
(f^{-1})'(f({\tilde {x}}))={\frac {1}{f'({\tilde {x}})}}
\end{align*}

Die Formel gilt also unter diesen Voraussetzungen auch allgemein. Die Frage ist nun noch, unter welchen Bedingungen an $f$ die Ableitung von $f^{-1}$ sicher existiert.

\begin{itemize}
\item Zum einen muss die $f^{-1}$ existieren. Dies ist genau dann der Fall, wenn $f$ bijektiv ist, was wiederum genau dann der Fall ist, wenn $f$ surjektiv und streng monoton ist.
\item Wie wir oben gesehen haben muss $f$ im Punkt ${\tilde {x}}=f^{-1}({\tilde {y}})$ differenzierbar sein mit $f'({\tilde {x}})\neq 0$.
\item Wir werden sehen, dass wir noch eine weitere Voraussetzung benötigen, nämlich dass $f^{-1}$ in ${\tilde {y}}$ stetig ist. Ist der Definitionsbereich $D$ von $f$ ein Intervall, so ist dies nach dem Satz über die Stetigkeit der Umkehrfunktion immer erfüllt.
\end{itemize}

Unter genau diesen Voraussetzungen werden wir einen Satz formulieren und beweisen. Anschließend untersuchen wir noch ein paar Beispiele.

\section{Satz über die Ableitung der Umkehrfunktion}

\subsection{Satz und Beweis}

\begin{theorem*}[Ableitung der Umkehrfunktion]
Seien $D,W\subseteq \mathbb {R} $ und $D$ ein Intervall. Weiter sei $f:D\to W$ eine surjektive, streng monotone Funktion, die in ${\tilde {x}}\in D$ differenzierbar ist mit $f'({\tilde {x}})\neq 0$. Dann hat $f$ eine Umkehrfunktion $f^{-1}:W\to D$, die in ${\tilde {y}}:=f({\tilde {x}})$ differenzierbar ist, und es gilt:

\begin{align*}
(f^{-1})'({\tilde {y}})={\frac {1}{f'(f^{-1}({\tilde {y}}))}}
\end{align*}

\end{theorem*}

\begin{explanation*}[Ableitung der Umkehrfunktion]
Anmerkungen:

\begin{itemize}
\item Die Surjektivität von $f$ ist gleichwertig mit $W=f(D)$.
\item Ist $f$ auf ganz $D$ differenzierbar, so lässt sich nach dem \href{https://de.wikibooks.org/wiki/Mathe\_für\_Nicht-Freaks:\_Monotoniekriterium:\_Zusammenhang\_zwischen\_Monotonie\_und\_Ableitung\_einer\_Funktion}
{Monotoniekriterium} die strenge Monotonie am einfachsten duch $f'>0$ beziehungsweise $f'<0$ überprüfen.
\item Wie wir an der Ableitung der Quadratwurzelfunktion $y\mapsto {\sqrt {y}}$ in ${\tilde {y}}=f({\tilde {x}})=0$ oben gesehen haben, darf die Voraussetzung $f'({\tilde {x}})\neq 0$ auf keinen Fall weggelassen werden.
\item Der Satz gilt auch noch etwas allgemeiner, falls $D$ kein Intervall ist. Dann muss aber zusätzlich gefordert werden, dass $f^{-1}$ in ${\tilde {y}}$ stetig ist. Außerdem müssen ${\tilde {x}}$ beziehungsweise ${\tilde {y}}$ Häufungspunkte von $D$ beziehungsweise $W$ sein.
\item Ist $f$ zusätzlich noch stetig, so folgt, nach dem \href{https://de.wikibooks.org/wiki/Mathe\_für\_Nicht-Freaks:\_Stetigkeit\_der\_Umkehrfunktion\#Anker:Satz}
{Satz von der Stetigkeit der Umkehrfunktion}, dass $W$ ein Intervall ist.
\end{itemize}

\end{explanation*}

\begin{proofsummary*}[Ableitung der Umkehrfunktion]
Zunächst begründen wir, dass $f^{-1}$ existiert. Anschließend folgern wir mit Hilfe des Satzes über die Stetigkeit der Umkehrfunktion, dass $f$ stetig ist. Danach zeigen wir, dass der Differentialquotient $\lim \limits _{y\to {\tilde {y}}}{\tfrac {f^{-1}(y)-f^{-1}({\tilde {y}})}{y-{\tilde {y}}}}$ existiert, und den Wert ${\tfrac {1}{f'(f^{-1}({\tilde {y}}))}}$ hat. Das heißt, dass für jede Folge $(y_{n})$ mit $y_{n}\to {\tilde {y}}$ gilt $\lim \limits _{n\to \infty }{\tfrac {f^{-1}(y_{n})-f^{-1}({\tilde {y}})}{y_{n}-{\tilde {y}}}}={\tfrac {1}{f'(f^{-1}({\tilde {y}}))}}$.

\end{proofsummary*}

\begin{proof*}[Ableitung der Umkehrfunktion]
$f:D\to W$ ist surjektiv und streng monoton, also bijektiv. Also existiert die Umkehrfunktion $f^{-1}:W\to D$. Da wir angenommen haben, dass $D$ ein Intervall ist folgt, nach dem Satz über die Stetigkeit der Umkehrfunktion, dass $f^{-1}$ stetig auf $W$ ist. Es gilt damit $\lim \limits _{y\to {\tilde {y}}}{f^{-1}(y)}=f^{-1}({\tilde {y}})$ mit ${\tilde {y}}:=f({\tilde {x}})\in W$. Sei nun $(y_{n})_{n\in \mathbb {N} }=(f(x_{n}))_{n\in \mathbb {N} }$ eine Folge in $W$ mit $\lim _{n\to \infty }y_{n}={\tilde {y}}$, dann gilt

\begin{align*}
&\lim \limits _{n\to \infty }{\frac {f^{-1}(y_{n})-f^{-1}({\tilde {y}})}{y_{n}-{\tilde {y}}}}\\[0.3em]&{\color {OliveGreen}\left\downarrow \ f(f^{-1}(y_{n}))=y_{n}{\text{ und }}f(f^{-1}({\tilde {y}}))={\tilde {y}}\right.}\\[0.3em]=\ &\lim \limits _{n\to {\tilde {\infty }}}{\frac {f^{-1}(y_{n})-f^{-1}({\tilde {y}})}{f(f^{-1}(y_{n}))-f(f^{-1}({\tilde {y}}))}}\\[0.3em]=\ &\lim \limits _{n\to \infty }{\frac {1}{\frac {f(f^{-1}(y_{n}))-f(f^{-1}({\tilde {y}}))}{f^{-1}(y_{n})-f^{-1}({\tilde {y}})}}}\\[0.3em]&{\color {OliveGreen}\left\downarrow \ f^{-1}(y_{n})=x_{n}{\text{ und }}f^{-1}({\tilde {y}})={\tilde {x}}\right.}\\[0.3em]=\ &\lim \limits _{n\to \infty }{\frac {1}{\frac {f(x_{n})-f({\tilde {x}})}{x_{n}-{\tilde {x}}}}}\\[0.3em]=\ &{\frac {1}{\lim \limits _{n\to \infty }{\frac {f(x_{n})-f({\tilde {x}})}{x_{n}-{\tilde {x}}}}}}\\[0.3em]&{\color {OliveGreen}\left\downarrow \ f{\text{ differenzierbar in }}{\tilde {x}}\right.}\\[0.3em]=\ &{\frac {1}{f'({\tilde {x}})}}\\[0.3em]=\ &{\frac {1}{f'(f^{-1}({\tilde {y}}))}}
\end{align*}

Also ist $f^{-1}$ in ${\tilde {y}}$ differenzierbar und es gilt $(f^{-1})'({\tilde {y}})={\frac {1}{f'(f^{-1}({\tilde {y}})}}$.

\end{proof*}

\subsection{Merkregel und graphische Veranschaulichung zur Formel}

\begin{tabularx}{\linewidth}{XX}
\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Derivate_of_inverse_1.svg}{\textbf{Derivate\allowbreak\_of\allowbreak\_inverse\allowbreak\_1.svg}} by Who2010 \textit{(CC BY-SA 4.0)}}\begin{minipage}[t]{\linewidth}
\begin{figure}[H]
\begin{minipage}[t][0.2\textheight][c]{\linewidth}
\centering
\adjincludegraphics[max width=1.\linewidth, max height=0.2\textheight]{file58derivate95of95inverse951954406b364c47fa40e1c8bd83f5b1993a7a9d696a9}
\end{minipage}
\caption*{Graph von $f$ mit Ableitung $f'(x_{0})={\tfrac {dx}{dy}}=m$ (\arabic{imagelabel})}
\end{figure}

\end{minipage}
&
\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Derivate_of_inverse_2.svg}{\textbf{Derivate\allowbreak\_of\allowbreak\_inverse\allowbreak\_2.svg}} by Who2010 \textit{(CC BY-SA 4.0)}}\begin{minipage}[t]{\linewidth}
\begin{figure}[H]
\begin{minipage}[t][0.2\textheight][c]{\linewidth}
\centering
\adjincludegraphics[max width=1.\linewidth, max height=0.2\textheight]{file58derivate95of95inverse9529525a663a75ae725589297eb03e1fe63a1b2f60fc7}
\end{minipage}
\caption*{Graph von $f^{-1}$ mit Ableitung $(f^{-1})'(y_{0})={\tfrac {dy}{dx}}={\tfrac {1}{m}}$ (\arabic{imagelabel})}
\end{figure}

\end{minipage}
\end{tabularx}

Mit Hilfe der Leibnizschen Notation für die Ableitung lässt sich die Formel der Ableitung der Umkehrfunktion durch einen einfachen Bruchrechentrick veranschaulichen: Für $f^{-1}(y)=x$ und $f(x)=y$ gilt

\begin{align*}
{\frac {\mathrm {d} x}{\mathrm {d} y}}={\frac {1}{\frac {\mathrm {d} y}{\mathrm {d} x}}}
\end{align*}

Auch graphisch können wir die Formel klar machen: Ist die Funktion $f$ im Punkt $x_{0}$ differenzierbar, so entspricht $f'(x_{0})$ der Steigung der Tangente an dem Graphen in $(x_{0}|f(x_{0}))$. Es gilt daher

\begin{align*}
f'(x_{0})={\frac {\mathrm {d} y}{\mathrm {d} x}}=m
\end{align*}

Den Graphen der Umkehrfunktion erthalten wir nun in zwei Schritten:

\begin{enumerate}
\item Zunächst müssen wir den Graphen von $f$ um $90^{\circ }$ (im bzw. gegen den Uhrzeigersinn) drehen. Der daraus entstandene Graph hat im Punkt $x_{0}$ die Steigung $-{\tfrac {1}{m}}$, da die Tangente in diesem Punkt senkrecht auf der ursprünglichen Tangente steht.
\item Anschließend müssen wir den Graphen noch (horizontal bzw. vertikal) spiegeln. Dabei dreht sich das Vorzeichen der Tangentensteigung um.
\end{enumerate}

Insgesamt erhalten wir

\begin{align*}
(f^{-1})'(f(x_{0}))=(f^{-1})'(y_{0})={\frac {\mathrm {d} x}{\mathrm {d} y}}=-\left(-{\frac {1}{m}}\right)={\frac {1}{m}}
\end{align*}

\chapter{Beispiele für Ableitungen}

In diesem Kapitel wollen wir die wichtigsten Beispiele von Ableitungen zusammenfassen, die dir im Laufe des ersten Semsters begegnen werden. Mit Hilfe der \href{https://de.wikibooks.org/wiki/Mathe\_für\_Nicht-Freaks:\_Rechengesetze\_für\_die\_Ableitung}
{Rechengesetze für die Ableitung} kannst du dann viele weitere aus diesen Funktionen zusammengesetzte Funktionen ebenfalls ableiten.

\section{Tabelle wichtiger Ableitungen}

In der folgenden Tabelle ist $n\in \mathbb {N} $, $q\in \mathbb {Z} $ und ${\tilde {n}}\in \mathbb {N} _{0}$. Außerdem definieren wir $a,b,c\in \mathbb {R} $, $a_{k}\in \mathbb {R} $ und $p\in \mathbb {R} ^{+}$.


\renewcommand{\arraystretch}{1.5}

\begin{longtabu} to \linewidth {X[l]X[l]X[l]} \\ \toprule 
Funktionsterm & Term der Ableitungsfunktion & Definitionsbereich der Ableitung \\ 
\midrule
$c$ & $0$ & $\mathbb {R} $ \\ 
$x^{n}$ & $nx^{n-1}$ & $\mathbb {R} $ \\ 
$ax+b$ & $a$ & $\mathbb {R} $ \\ 
$ax^{2}+bx+c$ & $2ax+b$ & $\mathbb {R} $ \\ 
$\sum _{k=0}^{\tilde {n}}a_{k}x^{k}$ & $\sum _{k=1}^{\tilde {n}}a_{k}kx^{k-1}$ & $\mathbb {R} $ \\ 
${\frac {1}{x}}$ & $-{\frac {1}{x^{2}}}$ & $\mathbb {R} \setminus \{0\}$ \\ 
$x^{-n}={\frac {1}{x^{n}}}$ & $-{\frac {n}{x^{n+1}}}$ & $\mathbb {R} \setminus \{0\}$ \\ 
$x^{q}$ & $qx^{q-1}$ & ${\begin{cases}\mathbb {R} &q\in \mathbb {Z} _{0}^{+}\\\mathbb {R} \setminus \{0\}&q\in \mathbb {Z} ^{-}\end{cases}}$ \\ 
${\sqrt {x}}$ & ${\frac {1}{2{\sqrt {x}}}}$ & $\mathbb {R} ^{+}$ \\ 
${\sqrt[{n}]{x}}$ & ${\frac {1}{n{\sqrt[{n}]{x^{n-1}}}}}$ & $\mathbb {R} ^{+}$ \\ 
${\sqrt[{n}]{x^{q}}}$ & ${\frac {q}{n}}{\sqrt[{n}]{x^{q-n}}}$ & $\mathbb {R} ^{+}$ \\ 
$\exp(x)=e^{x}$ & $\exp(x)$ & $\mathbb {R} $ \\ 
$p^{x}=\exp(x\ln p)$ & $\ln(p)\cdot p^{x}$ & $\mathbb {R} $ \\ 
$x^{a}=\exp(a\ln x)$ & $ax^{a-1}$ & ${\begin{cases}\mathbb {R} &a\geq 0\\\mathbb {R} \setminus \{0\}&a<0\end{cases}}$ \\ 
$\ln |x|$ & ${\frac {1}{x}}$ & $\mathbb {R} \setminus \{0\}$ \\ 
$\log _{p}|x|={\frac {\ln |x|}{\ln p}}$ & ${\frac {1}{\ln(p)\cdot x}}$ & $\mathbb {R} \setminus \{0\}$ \\ 
$\sin(x)$ & $\cos(x)$ & $\mathbb {R} $ \\ 
$\cos(x)$ & $-\sin(x)$ & $\mathbb {R} $ \\ 
$\tan(x)={\frac {\sin x}{\cos x}}$ & ${\frac {1}{\cos ^{2}(x)}}=1+\tan ^{2}(x)$ & $\mathbb {R} \setminus \{{\frac {\pi }{2}}+k\pi |k\in \mathbb {Z} \}$ \\ 
$\sec(x)={\frac {1}{\cos(x)}}$ & ${\frac {\sin(x)}{\cos ^{2}(x)}}$ & $\mathbb {R} \setminus \{{\frac {\pi }{2}}+k\pi |k\in \mathbb {Z} \}$ \\ 
$\csc(x)={\frac {1}{\sin(x)}}$ & $-{\frac {\cos(x)}{\sin ^{2}(x)}}$ & $\mathbb {R} \setminus \{k\pi |k\in \mathbb {Z} \}$ \\ 
$\cot(x)={\frac {\cos x}{\sin x}}$ & $-{\frac {1}{\sin ^{2}(x)}}=-1-\cot ^{2}(x)$ & $\mathbb {R} \setminus \{k\pi |k\in \mathbb {Z} \}$ \\ 
$\arcsin(x)$ & ${\frac {1}{\sqrt {1-x^{2}}}}$ & $(-1,1)$ \\ 
$\arccos(x)$ & $-{\frac {1}{\sqrt {1-x^{2}}}}$ & $(-1,1)$ \\ 
$\arctan(x)$ & ${\frac {1}{1+x^{2}}}$ & $\mathbb {R} $ \\ 
${\text{arcot}}(x)$ & $-{\frac {1}{1+x^{2}}}$ & $\mathbb {R} $ \\ 
$\sinh(x)={\frac {e^{x}-e^{-x}}{2}}$ & $\cosh(x)$ & $\mathbb {R} $ \\ 
$\cosh(x)={\frac {e^{x}+e^{-x}}{2}}$ & $\sinh(x)$ & $\mathbb {R} $ \\ 
$\tanh(x)={\frac {\sinh x}{\cosh x}}$ & ${\frac {1}{\cosh ^{2}(x)}}=1-\tanh ^{2}(x)$ & $\mathbb {R} $ \\ 
$\operatorname {arsinh} (x)$ & ${\frac {1}{\sqrt {x^{2}+1}}}$ & $\mathbb {R} $ \\ 
$\operatorname {arcosh} (x)$ & ${\frac {1}{\sqrt {x^{2}-1}}}$ & $(1,\infty )$ \\ 
$\operatorname {artanh} (x)$ & ${\frac {1}{1-x^{2}}}$ & $(-1,1)$ \\ 
\bottomrule
\end{longtabu}
\renewcommand{\arraystretch}{1.0}
\chapter{Ableitung höherer Ordnung}

\section{Motivation}

\begin{figure}[h]
\vspace{\baselineskip}
\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Schematic_diagram_of_Jerk,_Acceleration,_and_Speed.svg}{\textbf{Schematic\allowbreak\_diagram\allowbreak\_of\allowbreak\_Jerk,\allowbreak\_Acceleration,\allowbreak\_and\allowbreak\_Speed.svg}} by Autopilot \textit{(CC BY-SA 4.0)}}\centering
\adjincludegraphics[max width=.5\textwidth, max height=0.2\textheight]{file58schematic95diagram95of95jerk4495acceleration4495and95speed957affce30f20957a14e416e0546e015d4d1561990}
\caption*{Diagramm für Ort, Geschwindigkeit, Beschleunigung und Ruck eines Objekts. Der Ort ist die türkise Linie. Die Geschwindigkeit (violett) steigt, ist dann zwischen $x=3$ und $x=4$ konstant und fällt dann wieder auf Null ab. Sobald die Geschwindigkeit abfällt, wird die Beschleunigung (grün) negativ. Ein Ruck spielt sich nur in nicht konstant beschleunigten Bereichen ab und ist eine Stufenfunktion. Deshalb ist die Ableitung des Rucks auch Null (bei den Sprungstellen ist die Ableitung nicht deffiniert). (\arabic{imagelabel})}
\end{figure}
Die Ableitung $f'$ beschreibt die momentane Änderungsrate der Funktion $f$. Nun kann man die abgeleitete Funktion $f'$ wieder ableiten, vorausgesetzt, dass diese wieder differenzierbar ist. Die gewonne Ableitung der Ableitung wird \emph{zweite Ableitung} bzw. \emph{Ableitung zweiter Ordnung} genannt und mit $f''$ oder $f^{(2)}$ bezeichnet. Dies lässt sich beliebig oft durchführen. Wenn die zweite Ableitung wiederum differenzierbar ist, so erhält man die dritte Ableitung $f^{(3)}$, danach die vierte Ableitung $f^{(4)}$ usw..

Diese höheren Ableitungen gestatten Aussagen über den Verlauf eines Funktionsgraphen. Die zweite Ableitung sagt zum Beispiel aus, ob ein Graph oben gekrümmt („konvex“) oder nach nach unten gekrümmt („konkav“) ist. Bei konvexen Graphen von differenzierbaren Funktionen nimmt seine Steigung kontinuierlich zu. Hierfür ist $f''(x)>0$ eine hinreichende Bedingung. Wenn nämlich die zweite Ableitung stets positiv ist, dann muss die erste Ableitung kontinuierlich wachsen. Analog folgt aus $f''(x)<0$, dass der Graph konkav ist und die Ableitung monoton fällt.

Um die Aussagekraft höherer Ableitungen genauer zu verdeutlichen betrachten wir die Funktion $f:[a,b]\to \mathbb {R} $ mit $f(t)=t^{3}+2$, welche den Ort $f(t)$ eines Autos zum Zeitpunkt $t$ angeben soll. Wir wissen schon, dass wir die Geschwindigkeit des Autos zum Zeitpunkt $t$ mit der ersten Ableitung berechnen können: $f'(t)=f^{(1)}(t)=3t^{2}$. Was sagt nun die Ableitung $f''(t)$ von $f'$ aus? Diese ist die momentane Änderungsrate der Geschwindigkeit und damit die Beschleunigung des Autos. Es beschleunigt mit $f''(t)=f^{(2)}(t)=6t$.

Nun kann man diese zweite Ableitung wieder ableiten, wodurch wir die momentane Änderungsrate der Beschleunigung $f'''(t)=f^{(3)}(t)=6$ erhalten. Diese wird in der Fahrdynamik Ruck genannt und sagt aus, wie schnell ein Auto die Beschleunigung erhöht oder wie schnell es die Bremsung einleitet. Ein großer Ruck entsteht zum Beispiel bei einer Notbremsung. Da $f^{(3)}(t)<0$ bei einer Notbremsung ist, ist der Graph der Geschwindigkeit $f'$ konvex – die Geschwindigkeit fällt immer stärker. Die vierte Ableitung $f^{4}(t)=0$ sagt uns wiederum, dass der Ruck keine momentane Änderungsrate hat.

\section{Definition}

\begin{definition*}[Ableitungen höherer Ordnung]
Sei $f:D\to W$ mit $D,W\subseteq \mathbb {R} $ eine reellwertige Funktion. Wir setzen $f^{(0)}:=f$ und im Fall der Differenzierbarkeit $f^{(1)}=f'$. Wir definieren die \emph{zweite Ableitung} über $f^{(2)}=\left(f^{(1)}\right)'$, die \emph{dritte Ableitung} über $f^{(3)}=\left(f^{(2)}\right)'$ usw., wenn diese höheren Ableitungen existieren. Insgesamt definieren wir rekursiv für $k\in \mathbb {N} _{0}$:

\begin{align*}
f^{(k+1)}:=\left(f^{(k)}\right)'
\end{align*}

Wir sagen, $f$ ist $k$-Mal differenzierbar, wenn die $k$-te Ableitung $f^{(k)}$ von $f$ existiert. $f$ heißt $k$-Mal stetig differenzierbar, falls $f^{(k)}$ stetig ist.

\end{definition*}

Die Menge aller $k$-fach stetig differenzierbaren Funktionen mit Definitionsbereich $D$ und Wertebereich $W$ wird mit $C^{k}(D,W)$ notiert. Insbesondere besteht $C^{0}(D,W)=C(D,W)$ aus den stetigen Funktionen. Falls wir die Funktion $f$ beliebig oft ableiten können, so schreiben wir $f\in C^{\infty }(D,W)$. Ist $W=\mathbb {R} $, so können wir kürzer $C^{k}(D)$ beziehungsweise $C^{\infty }(D)$ schreiben. Es gilt:

\begin{align*}
C(D,W)\supseteq C^{1}(D,W)\supseteq C^{2}(D,W)\supseteq \ldots \supseteq C^{k}(D,W)\supseteq C^{k+1}(D,W)
\end{align*}

\chapter{Satz von Rolle}

\section{Motivation}

Wir wissen bereits vom \href{https://de.wikibooks.org/wiki/Mathe\_für\_Nicht-Freaks:\_Satz\_vom\_Minimum\_und\_Maximum}
{Satz vom Minimum und Maximum}, dass eine stetige Funktion $f$ auf einem abgeschlossenen Intervall $[a,b]$ ein Maximum und ein Minimum annimmt:

\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Beispiel_für_den_Satz_vom_Maximum_und_Minimum_3.svg}{\textbf{Beispiel\allowbreak\_für\allowbreak\_den\allowbreak\_Satz\allowbreak\_vom\allowbreak\_Maximum\allowbreak\_und\allowbreak\_Minimum\allowbreak\_3.svg}} by Stephan Kulla \textit{(CC0)}}\begin{center}
\adjincludegraphics[max width=0.5\textwidth,max height=0.2\textheight]{file58beispiel95f252r95den95satz95vom95maximum95und95minimum9539560b0507ad18c6b2234772ab88f956b55e3c35ed4}\end{center}

Dies gilt natürlich auch, wenn $f(a)=f(b)$ ist. In diesem Fall muss es (wenn die Funktion nicht konstant ist) ein Maximum oder ein Minimum im Inneren des Definitionsbereichs geben. In folgender Abbildung liegt sowohl das Maximum als auch das Minimum im Inneren von $[a,b]$, also im offenen Intervall $(a,b)$:

\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Spezialfall_Satz_vom_Maximum_und_Minimum.svg}{\textbf{Spezialfall\allowbreak\_Satz\allowbreak\_vom\allowbreak\_Maximum\allowbreak\_und\allowbreak\_Minimum.svg}} by Who2010 \textit{(CC BY-SA 4.0)}}\begin{center}
\adjincludegraphics[max width=0.5\textwidth,max height=0.2\textheight]{file58spezialfall95satz95vom95maximum95und95minimum95cd7e5b6ba4a2720cf15d021d2e03a975a3bd3da4}\end{center}

Nehmen wir nun zusätzlich an, dass $f$ auf $(a,b)$ differenzierbar ist. Sei $\xi $ die Maximal- bzw. Minimalstelle. Wenn $\xi $ im Inneren des Definitionsbereichs liegt, wenn also $\xi \in (a,b)$ ist, dann ist $f'(\xi )=0$ nach dem notwendigen Hauptkriterium für Extrema einer differenzierbaren Funktion. Anschaulich bedeutet dies, dass die Tangente an $f$ in $\xi $ waagrecht liegt. Genau dies besagt der \emph{Satz von Rolle}: Für jede stetige Funktion $f:[a,b]\to \mathbb {R} $ mit $f(a)=f(b)$, die in $(a,b)$ differenzierbar ist, gibt es ein Argument $\xi \in (a,b)$ mit $f'(\xi )=0$.

\begin{tabularx}{\linewidth}{XX}
\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Satz von Rolle.svg}{\textbf{Satz von Rolle.svg}} by Who2010 \textit{(CC BY-SA 4.0)}}\begin{minipage}[t]{\linewidth}
\begin{figure}[H]
\begin{minipage}[t][0.2\textheight][c]{\linewidth}
\centering
\adjincludegraphics[max width=1.\linewidth, max height=0.2\textheight]{file58satz32von32rolle95eead608d05202d2c893e953b7aeb1127057e5974}
\end{minipage}
\caption*{Die Ableitung im Maximum von $f$ ist null. (\arabic{imagelabel})}
\end{figure}

\end{minipage}
&
\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Satz von Rolle2.svg}{\textbf{Satz von Rolle2.svg}} by Who2010 \textit{(CC BY-SA 4.0)}}\begin{minipage}[t]{\linewidth}
\begin{figure}[H]
\begin{minipage}[t][0.2\textheight][c]{\linewidth}
\centering
\adjincludegraphics[max width=1.\linewidth, max height=0.2\textheight]{file58satz32von32rolle295653f29b65fff4984f24e868ce1d449d712ad9fdd}
\end{minipage}
\caption*{Die Ableitung im Minimum von $f$ ist null. (\arabic{imagelabel})}
\end{figure}

\end{minipage}
\end{tabularx}

Natürlich kann $f$ in $(a,b)$ auch mehrere (teils lokale) Maximal- und Minimalstellen annehmen. Außerdem kann es sein, dass $f$ in $(a,b)$ nur ein Maximum (und kein Minimum) oder ein Minimum (und kein Maximum) im Inneren des Definitionsbereichs annimmt:

\begin{tabularx}{\linewidth}{XX}
\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Satz von Rolle3.svg}{\textbf{Satz von Rolle3.svg}} by Lukasstockner, Who2010 \textit{(CC BY-SA 4.0)}}\begin{minipage}[t]{\linewidth}
\begin{figure}[H]
\begin{minipage}[t][0.2\textheight][c]{\linewidth}
\centering
\adjincludegraphics[max width=1.\linewidth, max height=0.2\textheight]{file58satz32von32rolle395944139232b0b7042caf2ef0d94ab09dd2a8eb2f8}
\end{minipage}
\caption*{Die Funktion $f$ besitzt im Inneren des Definitionsbereichs nur ein Maximum und kein Minimum. An dieser Stelle ist die Ableitung null. (\arabic{imagelabel})}
\end{figure}

\end{minipage}
&
\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Satz von Rolle4.svg}{\textbf{Satz von Rolle4.svg}} by Who2010 \textit{(CC BY-SA 4.0)}}\begin{minipage}[t]{\linewidth}
\begin{figure}[H]
\begin{minipage}[t][0.2\textheight][c]{\linewidth}
\centering
\adjincludegraphics[max width=1.\linewidth, max height=0.2\textheight]{file58satz32von32rolle49592095678905742a366ac3cfc4b547dbac57c8e8f}
\end{minipage}
\caption*{Die Funktion $f$ besitzt im Inneren des Definitionsbereichs nur ein Minimum und kein Maximum. An dieser Stelle ist die Ableitung null. (\arabic{imagelabel})}
\end{figure}

\end{minipage}
\end{tabularx}

Ein Sonderfall ist der, dass $f$ konstant auf $[a,b]$ ist. In diesem Fall gilt $f'(x)=0$ für alle $x\in (a,b)$:

\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Satz_von_Rolle5.svg}{\textbf{Satz\allowbreak\_von\allowbreak\_Rolle5.svg}} by Who2010 \textit{(CC BY-SA 4.0)}}\begin{center}
\adjincludegraphics[max width=0.5\textwidth,max height=0.2\textheight]{file58satz95von95rolle595cc3f4564f9938de84ff197dd100311cf9e5ee5b0}\end{center}

Egal welchen Fall wir uns angeschaut haben, immer gab es mindestens eine Stelle im Inneren des Definitionsbereichs, wo die Ableitung der Funktion gleich null ist.

\section{Satz von Rolle}

Der nach Michel Rolle (1652-1719) benannte Satz stellt einen Spezialfall des Mittelwertsatzes der Differentialrechnung dar und lautet wie folgt:

\begin{theorem*}[Satz von Rolle]
Sei $f:[a,b]\to \mathbb {R} $ eine stetige Funktion mit $a<b$ und $f(a)=f(b)$. Außerdem sei $f$ auf dem offenen Intervall $(a,b)$ differenzierbar. Dann existiert ein $\xi \in (a,b)$ mit $f'(\xi )=0$.

\end{theorem*}

\begin{explanation*}[Satz von Rolle]
Ist $f$ auf $(a,b)$ differenzierbar, so ist $f$ auf $(a,b)$ stetig. Daher genügt es zur Prüfung der Voraussetzungen, die Stetigkeit von $f$ in den Randpunkten $a$ und $b$ nachzuweisen.

\end{explanation*}

\begin{example*}[Satz von Rolle]
Betrachten wir die Funktion $f:[0,2]\to \mathbb {R} $ mit $f(x)=x^{2}-2x-3$. Es ist

\begin{itemize}
\item $f$ ist als Polynom stetig auf $[0,2]$
\item $f(0)=-3=f(2)$
\item $f$ ist als Polynom differenzierbar auf $(0,2)$
\end{itemize}

Der Satz von Rolle besagt nun: Es gibt mindestens ein $\xi \in (0,2)$ mit $f'(\xi )=0$.

\end{example*}

\section{Beweis}

\begin{proofsummary*}[Satz von Rolle]
Wir betrachten zunächst den Spezialfall, dass $f$ eine konstante Funktion ist. Hier ist die Ableitung überall gleich null. Wenn $f$ nicht konstant ist, benutzen wir den Satz vom Maximum und Minimum, um ein Maximum oder Minimum im Inneren des Definitionsbereichs zu finden. Dort gilt nach dem notwendigen Kriterium für die Existenz eines Extremums, dass die Ableitung in der Extremstelle verschwindet.

\end{proofsummary*}

\begin{proof*}[Satz von Rolle]
Sei $f:[a,b]\to \mathbb {R} $ eine stetige Funktion mit $a<b$, die auf $(a,b)$ differenzierbar ist. Sei außerdem $f(a)=f(b)$.

\proofcase{1}{$f$ ist konstant.}
\begin{indentblock}
Sei $f$ konstant. Dann gilt $f'(\xi )=0$ für alle $\xi \in (a,b)$. Damit gibt es mindestens ein $\xi \in (a,b)$ mit $f'(\xi )=0$ (es kann ein beliebiges $\xi $ aus $(a,b)$ gewählt werden). Der Satz von Rolle ist erfüllt.

\end{indentblock}

\proofcase{2}{$f$ ist nicht konstant.}
\begin{indentblock}
Sei $f$ ist nicht konstant. Nach dem Satz vom Maximum und Minimum nimmt $f$ auf dem kompakten Intervall $[a,b]$ sowohl Maximum, als auch Minimum an. Das Maximum oder das Minimum von $f$ muss von $f(a)=f(b)$ verschieden sein, da sonst $f$ konstant wäre. Damit wird (mindestens) ein Extremum an einer Stelle $\xi \in (a,b)$ angenommen.

Weil $f$ auf $(a,b)$ differenzierbar ist, ist auch $f$ an der Extremstelle $\xi $ differenzierbar. Hier ist nach dem notwendigem Kriterium für Extrema $f'(\xi )=0$. Somit existiert mindestens ein $\xi \in (a,b)$, wo die Ableitung gleich null ist. Der Satz von Rolle ist auch in diesem Fall bewiesen.

\end{indentblock}

\end{proof*}

\section{Anwendung: Nullstellen von Funktionen}

Der Satz von Rolle kann auch in Existenzbeweisen von Nullstellen eingesetzt werden. Mit diesem lässt sich nämlich zeigen, dass eine Funktion auf einem Intervall \emph{höchstens} eine Nullstelle besitzt. Andererseits lässt sich mit dem \href{https://de.wikibooks.org/wiki/Mathe\_für\_Nicht-Freaks:\_Zwischenwertsatz}
{Zwischenwertsatz} zeigen, dass eine Funktion in einem Intervall \emph{mindestens} eine Nullstelle hat. Zusammen kann so die Existenz von \emph{genau} einer Nullstelle gezeigt werden.

\begin{example*}[Nullstelle eines Polynoms]
Betrachten wir das Polynom $p(x)=x^{3}+x+1$ auf dem Intervall $[-1,0]$. Für dieses gilt

\begin{itemize}
\item $p$ ist stetig auf $[-1,0]$. Außerdem ist $p(-1)=-1<0$ und $p(0)=1>0$. Nach dem Zwischenwertsatz hat das Polynom auf $[-1,0]$ \emph{mindestens} eine Nullstelle.
\item $p$ ist differenzierbar auf $(-1,0)$ mit $p'(x)=3x^{2}+1$. Wir nehmen nun an, dass $p$ auf $[-1,0]$ zwei Nullstellen $x_{1}$ und $x_{2}$ hat. Sei dabei $x_{1}<x_{2}$. Es gilt also $p(x_{1})=0=p(x_{2})$. Da $p$ auf $[x_{1},x_{2}]\subseteq [-1,0]$ stetig und auf $(x_{1},x_{2})\subseteq (-1,0)$ differenzierbar ist, kann der Satz von Rolle angewandt werden. Es müsste daher ein $\xi \in (x_{1},x_{2})$ mit $p'(\xi )=3\xi ^{2}+1=0$ geben. Nun hat aber $p'$ wegen $p'(x)=\underbrace {3x^{2}} _{\geq 0}+1\geq 1$ keine Nullstellen. Also kann $p$ in $[-1,0]$ nicht mehr als eine Nullstelle besitzen.
\end{itemize}

Aus beiden Punkten ergibt sich insgesamt, dass $p$ in $[-1,0]$ \emph{genau} eine Nullstelle hat.

\end{example*}

\chapter{Mittelwertsatz}

Der Mittelwertsatz ist einer der zentralen Sätze der Differentialrechnung und besagt (grob gesprochen), dass die Steigung der Sekante zwischen zwei verschiedenen Punkten einer differenzierbaren Funktion irgendwo zwischen diesen beiden Punkten als Ableitung angenommen wird. So verknüpft der Mittelwertsatz die Sekantensteigung mit der Ableitung einer Funktion. Globale Eigenschaften, die mit Hilfe der Sekantensteigung ausgedrückt werden können, sind so mit Hilfe des Mittelwertsatzes auf Eigenschaften der Ableitung zurückführbar. Im Abschnitt \href{https://de.wikibooks.org/wiki/Mathe\_für\_Nicht-Freaks:\_Mittelwertsatz\#Anker:Schrankensatz}
{„Schrankensatz“} werden wir eine nützliche Anwendung untersuchen. Weitere folgen dann in den Kapiteln \href{https://de.wikibooks.org/wiki/Mathe\_für\_Nicht-Freaks:\_Konstanzkriterium:\_Zusammenhang\_zwischen\_Konstanz\_einer\_Funktion\_und\_ihrer\_Ableitung}
{„Kriterium für Konstanz und Monotoniekriterium“}, \href{https://de.wikibooks.org/wiki/Mathe\_für\_Nicht-Freaks:\_Ableitung\_und\_lokale\_Extrema}
{„Ableitung und lokale Extrema“} und \href{https://de.wikibooks.org/wiki/Mathe\_für\_Nicht-Freaks:\_Regel\_von\_L'Hospital}
{„Regel von L'Hospital“}. Auch der Hauptsatz der Differential- und Integralrechnung basiert auf dem Mittelwertsatz.

\section{Motivation}

Wir haben uns bereits mit dem \href{https://de.wikibooks.org/wiki/Mathe\_für\_Nicht-Freaks:\_Satz\_von\_Rolle}
{Satz von Rolle} beschäftigt. Zur Wiederholung: Der Satz von Rolle besagt, dass es für jede stetige Funktion $f:[a,b]\to \mathbb {R} $, die in $(a,b)$ differenzierbar ist und für die $f(a)=f(b)$ gilt, ein Argument $\xi \in (a,b)$ geben muss, welches $f'(\xi )=0$ erfüllt:

\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Satz_von_Rolle3.svg}{\textbf{Satz\allowbreak\_von\allowbreak\_Rolle3.svg}} by Lukasstockner, Who2010 \textit{(CC BY-SA 4.0)}}\begin{center}
\adjincludegraphics[max width=0.5\textwidth,max height=0.2\textheight]{file58satz95von95rolle395944139232b0b7042caf2ef0d94ab09dd2a8eb2f8}\end{center}

Wie können wir diesen Satz für den Fall $f(a)\neq f(b)$ verallgemeinern? Muss die Ableitung $f'(\xi )$ für ein $\xi \in (a,b)$ auch einen bestimmten Wert annehmen? Zunächst fällt auf, dass $f'(\xi )$ nicht zwangsläufig $0$ sein muss:

\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Mittelwertsatz1.svg}{\textbf{Mittelwertsatz1.svg}} by Lukasstockner, Who2010 \textit{(CC BY-SA 4.0)}}\begin{center}
\adjincludegraphics[max width=0.5\textwidth,max height=0.2\textheight]{file58mittelwertsatz1956a3f9877d4b22661f08dc2cad3a89578a045f9b0}\end{center}

Überlegen wir uns nochmal, wie die Situation beim Satz von Rolle war. Zum einen ist die Steigung der Tangente an den Graphen von $f$ in $(\xi ,f(\xi ))$ gleich $f'(\xi )=0$. Zum anderen ist aber auch die Steigung der Sekante durch die beiden Randpunkte $(a,f(a))$ und $(b,f(b))$ von $f$ gleich ${\tfrac {f(b)-f(a)}{b-a}}=0$, da $f(a)=f(b)$ und damit $f(b)-f(a)=0$ ist. Die Sekante zwischen den Punkten $(a,f(a))$ und $(b,f(b))$ und die Tangante im Punkt $(\xi ,f(\xi ))$ liegen damit parallel:

\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Mittelwertsatz2.svg}{\textbf{Mittelwertsatz2.svg}} by Lukasstockner, Who2010 \textit{(CC BY-SA 4.0)}}\begin{center}
\adjincludegraphics[max width=0.5\textwidth,max height=0.2\textheight]{file58mittelwertsatz29562bddd9c1649d94bb50c09a690acdbb946ce04ad}\end{center}

Sei nun allgemeiner $f(a)\neq f(b)$. Betrachten wir die Sekantensteigung ${\tfrac {f(b)-f(a)}{b-a}}$ zwischen den Punkten $(a,f(a))$ und $(b,f(b))$. Diese ist ungleich Null und entspricht der mittleren Steigung von $f$ im Intervall $[a,b]$. Fassen wir beispielsweise die Funktion als Ortsfunktion eines Autos in Abhängigkeit von der Zeit auf, so entspricht die mittlere Steigung der Durchschnittsgeschwindigkeit ${\overline {v}}$ des Autos in der Zeit von $a$ bis $b$.

Wenn das Auto zum Zeitpunkt $a$ schneller als ${\overline {v}}$ fährt (sprich: Die Ableitung $f'(a)$ ist größer als die Sekantensteigung ${\tfrac {f(b)-f(a)}{b-a}}$), so muss es bis zum Zeitpunkt $b$ Zeiten gegeben haben, an denen es langsamer als ${\overline {v}}$ gefahren ist, sonst kann es die Durchschnittsgeschwindigkeit ${\overline {v}}$ nicht erreichen. Bei einem Beschleunigungs- oder Bremsvorgang nimmt das Auto alle Geschwindigkeiten zwischen Anfangs- und Endgeschwindigkeit an und springt nicht einfach von der Anfangs- auf die Endgeschwindigkeit (hier nehmen wir an, dass die Geschwindigkeitsfunktion stetig ist). Da das Auto mal schneller und mal langsamer als ${\overline {v}}$ war, muss es einen Zeitpunkt $\xi $ geben, an dem es genau die Geschwindigkeit ${\overline {v}}$ hat. Analog können wir argumentieren, wenn das Auto zum Zeitpunkt $a$ langsamer als ${\overline {v}}$ fährt. Für unsere Funktion $f$ bedeutet das, dass es tatsächlich ein $\xi \in (a,b)$ geben muss mit ${\overline {v}}=f'(\xi )={\tfrac {f(b)-f(a)}{b-a}}$. Dies ist die Aussage des Mittelwertsatzes:

\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Mittelwertsatz3.svg}{\textbf{Mittelwertsatz3.svg}} by Lukasstockner, Who2010 \textit{(CC BY-SA 4.0)}}\begin{center}
\adjincludegraphics[max width=0.5\textwidth,max height=0.2\textheight]{file58mittelwertsatz3950fed6c5dfb76399b55b5d44551862342eea6e913}\end{center}

Es scheint also ein $\xi \in (a,b)$ mit $f'(\xi )={\tfrac {f(b)-f(a)}{b-a}}$ zu geben. Diese Intuition wollen wir im Folgenden zu einem Satz formen und formal korrekt beweisen. In unserer Argumentation haben wir beispielsweise verwendet, dass die Ableitung stetig ist. Nun muss die betrachtete Funktion nicht stetig differenzierbar sein. Dass aber auch in diesem Fall der Mittelwertsatz erfüllt ist, werden wir im Beweis zeigen.

\section{Mittelwertsatz}

Der Mittelwertsatz der Differentialrechnung ist eine Verallgemeinerung des \href{https://de.wikibooks.org/wiki/Mathe\_für\_Nicht-Freaks:\_Satz\_von\_Rolle}
{Satzes von Rolle} und lautet wie folgt:

\begin{theorem*}[Mittelwertsatz]
Sei $f:[a,b]\to \mathbb {R} $ eine stetige Funktion mit $a<b$ und auf dem offenen Intervall $(a,b)$ differenzierbar. Dann existiert ein $\xi \in (a,b)$ mit $f'(\xi )={\frac {f(b)-f(a)}{b-a}}$.

\end{theorem*}

\section{Beweis}

\begin{solutionprocess*}[Mittelwertsatz]
Wie oben schon erwähnt, wollen wir den Mittelwertsatz mit Hilfe des Satzes von Rolle beweisen. Dazu müssen wir aus der gegeben Funktion $f:[a,b]\to \mathbb {R} $ eine Hilfsfunktion $H:[a,b]\to \mathbb {R} $ so konstruieren, dass wir auf diese den Satz von Rolle anwenden können. Wir benötigen hierzu eine auf $[a,b]$ stetige und auf $(a,b)$ differenzierbare Funktion $H$. Zudem sollte $H(a)=H(b)$ sein. Dann gilt mit dem Satz von Rolle $H'(\xi )=0$ für ein $\xi \in (a,b)$. Können wir außerdem die Hilfsfunktion so wählen, dass $H'(x)=f'(x)-{\tfrac {f(b)-f(a)}{b-a}}$ ist, so folgt aus $H'(\xi )=0$ die Gleichung $f'(\xi )-{\tfrac {f(b)-f(a)}{b-a}}=0$. Dies ist äquivalent zu $f'(\xi )={\tfrac {f(b)-f(a)}{b-a}}$, also der Formel des Mittelwertsatzes. Die Funktion

\begin{align*}
H:[a,b]\to \mathbb {R} ,\ H(x)=f(x)-{\tfrac {f(b)-f(a)}{b-a}}(x-a)
\end{align*}

besitzt die gewünschten Eigenschaften. Insbesondere ist

\begin{align*}
H(a)&=f(a)-{\frac {f(b)-f(a)}{b-a}}(a-a)\\[0.5em]&=f(a)\\[0.5em]&=f(b)-(f(b)-f(a))\\[0.5em]&=f(b)-{\frac {f(b)-f(a)}{b-a}}(b-a)=H(b)
\end{align*}

Die folgenden Grafiken illustrieren den Zusammenhang zwischen der Funktion $f$ und der Hilfsfunktion $H$:

\begin{tabularx}{\linewidth}{XX}
\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:BeweisMittelwertsatz1.svg}{\textbf{BeweisMittelwertsatz1.svg}} by Lukasstockner, Who2010 \textit{(CC BY-SA 4.0)}}\begin{minipage}[t]{\linewidth}
\begin{figure}[H]
\begin{minipage}[t][0.2\textheight][c]{\linewidth}
\centering
\adjincludegraphics[max width=1.\linewidth, max height=0.2\textheight]{file58beweismittelwertsatz195850c06d18544421289c48c7ab25f968bf4e1e119}
\end{minipage}
\caption*{Die Funktion $f$ aus dem Mittelwertsatz. (\arabic{imagelabel})}
\end{figure}

\end{minipage}
&
\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:BeweisMittelwertsatz2.svg}{\textbf{BeweisMittelwertsatz2.svg}} by Lukasstockner, Who2010 \textit{(CC BY-SA 4.0)}}\begin{minipage}[t]{\linewidth}
\begin{figure}[H]
\begin{minipage}[t][0.2\textheight][c]{\linewidth}
\centering
\adjincludegraphics[max width=1.\linewidth, max height=0.2\textheight]{file58beweismittelwertsatz295045e36c3cba1eee1fd0b039623f2518fbecc05c8}
\end{minipage}
\caption*{Die Hilfsfunktion $H$ für den Satz von Rolle. (\arabic{imagelabel})}
\end{figure}

\end{minipage}
\end{tabularx}

\end{solutionprocess*}

\begin{proof*}[Mittelwertsatz]
Sei $f:[a,b]\to \mathbb {R} $ eine stetige Funktion mit $a<b$ und auf $(a,b)$ differenzierbar. Eine passende Hilfsfunktion $H:[a,b]\to \mathbb {R} $ ist gegeben durch

\begin{align*}
H(x)=f(x)-{\frac {f(b)-f(a)}{b-a}}(x-a)
\end{align*}

$H$ ist auf $[a,b]$ stetig und auf $(a,b)$ differenzierbar, da $f$ nach Voraussetzung diese Bedingungen erfüllt und $H$ eine Komposition aus $f$ und dem Polynom ersten Grades ${\tfrac {f(b)-f(a)}{b-a}}(x-a)$ ist. Außerdem gilt

\begin{align*}
H(a)&=f(a)-{\frac {f(b)-f(a)}{b-a}}(a-a)\\[0.5em]&=f(a)\\[0.5em]&=f(b)-(f(b)-f(a))\\[0.5em]&=f(b)-{\frac {f(b)-f(a)}{b-a}}(b-a)=H(b)
\end{align*}

Nach dem Satz von Rolle existiert ein $\xi \in (a,b)$ mit $0=H'(\xi )=f'(\xi )-{\tfrac {f(b)-f(a)}{b-a}}$. Es wurde also ein $\xi \in (a,b)$ gefunden, das $f'(\xi )={\tfrac {f(b)-f(a)}{b-a}}$ erfüllt. Somit folgt die Behauptung des Mittelwertsatzes.

\end{proof*}

\section{Anwendung: Beweis von Ungleichungen}

Mit Hilfe des Mittelwertsatzes lassen sich häufig nützliche Ungleichungen beweisen. Der Trick dabei ist, zunächst den Mittelwertsatz auf eine Hilfsfunktion (die oftmals auf einer Seite der Ungleichung steht) anzuwenden. Anschließend schätzen wir dann den Ausdruck $f'(\xi )$ passend ab.

\subsection{Beispielaufgabe: Beweis einer Ungleichung}

\begin{exercise*}[Beweis einer Ungleichung mit dem Mittelwertsatz]
Beweise, dass folgende Ungleichung für alle $x>0$ gilt:

\begin{align*}
{\sqrt {1+x}}\leq 1+{\frac {x}{2}}
\end{align*}

\end{exercise*}

\begin{proof*}[Beweis einer Ungleichung mit dem Mittelwertsatz]
Wir wählen als Hilfsfunktion

\begin{align*}
f:\mathbb {R} ^{+}\to \mathbb {R} ,\ f(t)={\sqrt {1+t}}
\end{align*}

Sei $x\in \mathbb {R} ^{+}$ beliebig. Die Funktion $f$ ist als Komposition stetiger Funktionen auf $[0,x]$ stetig und auf $(0,x)$ differenzierbar. Nach dem Mittelwertsatz gibt es daher ein $\xi \in (0,x)$ mit

\begin{align*}
f'(\xi )={\frac {f(x)-f(0)}{x-0}}={\frac {{\sqrt {1+x}}-1}{x}}
\end{align*}

Nun können wir $f'(\xi )$ abschätzen:

\begin{align*}
f'(\xi )={\frac {1}{2{\sqrt {1+\xi }}}}{\overset {\xi >0}{\leq }}{\frac {1}{2}}
\end{align*}

Damit gilt die Ungleichung:

\begin{align*}
{\frac {{\sqrt {1+x}}-1}{x}}=f'(\xi )\leq {\frac {1}{2}}
\end{align*}

Diese Ungleichung ist äquivalent zur zu beweisenden Behauptung

\begin{align*}
{\sqrt {1+x}}\leq 1+{\frac {x}{2}}
\end{align*}

\end{proof*}

\section{Anwendung: Der Schrankensatz}

\subsection{Definition und Beweis des Schrankensatzes}

\begin{theorem*}[Schrankensatz]
Sei $f:[a,b]\to \mathbb {R} $ stetig und in $(a,b)$ differenzierbar. Weiter sei die Ableitungsfunktion $f':(a,b)\to \mathbb {R} $ beschränkt. Dann gibt es mit $L=\sup _{x\in (a,b)}|f'(x)|$ ein $L\in \mathbb {R} _{0}^{+}$, sodass $|f(x)-f(y)|\leq L|x-y|$ für alle $x,y\in [a,b]$ gilt. Insbesondere gilt die Abschätzung, falls $f$ auf $[a,b]$ stetig differenzierbar ist.

\end{theorem*}

\begin{proof*}[Schrankensatz]
Seien $x,y\in [a,b]$ beliebig gewählt mit $x<y$. Schränken wir nun den Definitionsbereich von $f$ auf $[x,y]$ ein, so bleiben Stetigkeit und Differenzierbarkeit erhalten. Der Mittelwertsatz ist anwendbar. Also gibt es ein $\xi \in (x,y)$ mit $f'(\xi )={\tfrac {f(x)-f(y)}{x-y}}$.

Da nach Voraussetzung $f'$ beschränkt ist, existiert $L=\sup _{x\in (a,b)}|f'(x)|$. Für dieses $L\in \mathbb {R} _{0}^{+}$ gilt $|f'(\xi )|\leq L$ für alle $\xi \in (x,y)$. Somit ist ${\tfrac {|f(x)-f(y)|}{|x-y|}}=|f'(\xi )|\leq L$. Dies ist äquivalent zur Behauptung.

Wenn $f$ stetig differenzierbar auf $[a,b]$ ist, so besitzt die stetige Ableitungsfunktion ein Maximum und Minimum. Sie ist damit beschränkt und der Satz kann dann auch für diese Funktion angewandt werden.

\end{proof*}

\includepdf[pages=-]{predesigned_pages/mfnf_participation}

\chapter{Konstanzkriterium}

In diesem Kapitel wollen wir eine nützliche Folgerung aus dem Mittelwertsatz besprechen, die bereits aus der Schulzeit bekannt ist: \emph{Das Kriterium für Konstanz}. Dieses besagt, dass eine Funktion konstant sein muss, wenn ihre Ableitung überall verschwindet (gleich Null ist).

\section{Kriterium für Konstanz}

\begin{theorem*}
Sei $I\subseteq \mathbb {R} $ ein Intervall und $f:I\to \mathbb {R} $ eine differenzierbare Funktion mit $f'(x)=0$ für alle $x\in I$. Dann ist $f$ konstant.

\end{theorem*}

\begin{proof*}
Seien $a,b\in I$ mit $a<b$ beliebig. Sei außerdem $f$ auf dem Intervall $[a,b]$ differenzierbar und für alle $\xi \in I$ gelte $f'(\xi )=0$. Nach dem Mittelwertsatz gibt es ein $\xi \in (a,b)$ mit

\begin{align*}
f'(\xi )={\frac {f(b)-f(a)}{b-a}}
\end{align*}

Wir wissen, dass $f'(\xi )=0$ gelten muss. Also:

\begin{align*}
0=f'(\xi )={\frac {f(b)-f(a)}{b-a}}
\end{align*}

Wegen $a<b$ ist $b-a\neq 0$. Nun multiplizieren wir beide Seiten mit $(b-a)$. Wir erhalten:

\begin{align*}
f(b)-f(a)=0\cdot (b-a)=0
\end{align*}

Es folgt $f(a)=f(b)$. Da dies für alle $a$ und $b$ in $I$ gilt, ist $f$ konstant.

\end{proof*}

\section{Identitätssatz der Differentialrechnung}

Die erste Folgerung besagt, dass Funktionen mit identischer Ableitung bis auf eine Konstante übereinstimmen. Dieses Ergebnis wird sich später beim Hauptsatz der Differential- und Integralrechnung als sehr nützlich erweisen.

\begin{theorem*}[Identitätssatz]
Seien $f,g:[a,b]\to \mathbb {R} $ zwei differenzierbare Funktionen mit $f'=g'$. Dann gilt $f(x)=g(x)+c$ für alle $x\in [a,b]$. Dabei ist $c\in \mathbb {R} $ eine konstante Zahl.

\end{theorem*}

\begin{proof*}[Identitätssatz]
Wir definieren die Hilfsfunktion

\begin{align*}
h:[a,b]\to \mathbb {R} ,\ h(x)=f(x)-g(x)
\end{align*}

Diese ist differenzierbar, da $f$ und $g$ differenzierbar sind, und es gilt

\begin{align*}
h'(x)=f'(x)-g'(x){\overset {f'=g'}{=}}0
\end{align*}

Nach dem Kriterium für Konstanz ist daher $h(x)=f(x)-g(x)=c$ für alle $x\in [a,b]$ mit einer konstanten Zahl $c\in \mathbb {R} $. Dies ist äquivalent zu

\begin{align*}
f(x)=g(x)+c
\end{align*}

\end{proof*}

\section{Übungsaufgaben}

\subsection{Übungsaufgabe zum Identitätssatz}

\begin{exercise*}[Logarithmus-Darstellung des Areasinus Hyperbolicus]
Zeige, dass für alle $x\in \mathbb {R} $ gilt

\begin{align*}
\operatorname {arsinh} (x)=\ln \left(x+{\sqrt {x^{2}+1}}\right)
\end{align*}

\end{exercise*}

\begin{proof*}[Logarithmus-Darstellung des Areasinus Hyperbolicus]
Die Funktion $f:\mathbb {R} \to \mathbb {R} ,\ f(x)=\operatorname {arsinh} (x)$ ist nach den \href{https://de.wikibooks.org/wiki/Mathe\_für\_Nicht-Freaks:\_Beispiele\_für\_Ableitungen}
{Beispielen für Ableitungen} auf ganz $\mathbb {R} $ differenzierbar. Ihre Ableitung ist

\begin{align*}
f'(x)={\frac {1}{\sqrt {x^{2}+1}}}
\end{align*}

Nach der \href{https://de.wikibooks.org/wiki/Mathe\_für\_Nicht-Freaks:\_Ableitungsregeln:\_Kettenregel,\_Quotientenregel,\_Produktregel,\_Summenregel,\_Faktorregel}
{Ketten- und Summenregel} ist auch $g:\mathbb {R} \to \mathbb {R} ,\ g(x)=\ln \left(x+{\sqrt {x^{2}+1}}\right)$ auf ganz $\mathbb {R} $ differenzierbar. Es gilt:

\begin{align*}
g'(x)&={\frac {1}{x+{\sqrt {x^{2}+1}}}}\cdot \left(1+{\frac {2x}{2{\sqrt {x^{2}+1}}}}\right)\\[0.3em]&={\frac {1}{x+{\sqrt {x^{2}+1}}}}\cdot \left({\frac {{\sqrt {x^{2}+1}}+x}{\sqrt {x^{2}+1}}}\right)\\[0.3em]&={\frac {1}{\sqrt {x^{2}+1}}}
\end{align*}

Es ist $f'(x)=g'(x)$ für alle $x\in \mathbb {R} $ und nach dem Identitätssatz ist daher $f(x)=g(x)+c$ mit einer Konstanten $c$. Nun ist aber wegen $\sinh(0)=0$:

\begin{align*}
f(0)=\operatorname {arsinh} (0)=0
\end{align*}

Außerdem ist

\begin{align*}
g(0)=\ln \left(0+{\sqrt {0^{2}+1}}\right)=0
\end{align*}

Also ist $c=0$ und damit folgt die Behauptung.

\end{proof*}

\chapter{Monotoniekriterium}

\section{Monotoniekriterium}

Das \emph{Monotoniekriterium} für die Ableitung wird bereits in der Schule behandelt. Ist die Ableitungsfunktion $f'$ einer differenzierbaren Funktion $f$ auf einem Intervall $(a,b)$ nicht-negativ beziehungsweise nicht-positiv, so ist $f$ auf $(a,b)$ monoton steigend beziehungsweise monoton fallend. Ist $f$ sogar echt positiv beziehungsweise echt negativ auf $(a,b)$, so ist $f$ dort streng monoton steigend beziehungsweise fallend. Im ersten Fall gilt auch die Umkehrung der Aussage. Sprich: Steigt eine differenzierbare Funktion auf $[a,b]$ monoton, so ist $f'(x)\geq 0$ und eine auf $[a,b]$ fallende und ableitbare Funktion besitzt eine negative Ableitung.

\begin{theorem*}[Monotoniekriterium für differenzierbare Funktionen]
Sei $f:[a,b]\to \mathbb {R} $ stetig und auf $(a,b)$ differenzierbar. Dann gilt

\begin{enumerate}
\item $f'\geq 0$ auf $(a,b)$ $\iff $ $f$ monoton steigend auf $[a,b]$
\item $f'\leq 0$ auf $(a,b)$ $\iff $ $f$ monoton fallend auf $[a,b]$
\item $f'>0$ auf $(a,b)$ $\Longrightarrow $ $f$ streng monoton steigend auf $[a,b]$
\item $f'<0$ auf $(a,b)$ $\Longrightarrow $ $f$ streng monoton fallend auf $[a,b]$
\end{enumerate}

\end{theorem*}

\section{Beispiele zum Monotoniekriterium}

\subsection{Exponential- und Logarithmusfunktion}

\begin{example*}[Monotonie der Exponential- und Logarithmusfunktion]
Für die Exponentialfunktion $f:\mathbb {R} \to \mathbb {R} ,\ f(x)=\exp(x)$ gilt für alle $x\in \mathbb {R} $:

\begin{align*}
f'(x)=\exp(x)>0
\end{align*}

Daher ist $\exp $ nach dem Monotoniekriterium auf ganz $\mathbb {R} $ streng monoton steigend. Für die (natürliche) Logarithmusfunktion $g:\mathbb {R} ^{+}\to \mathbb {R} ,\ g(x)=\ln(x)$ gilt für alle $x\in \mathbb {R} ^{+}$:

\begin{align*}
g'(x)={\frac {1}{x}}>0
\end{align*}

Somit ist $\ln $ auf $\mathbb {R} ^{+}$ ebenfalls streng monoton steigend.

\end{example*}

\chapter{Ableitung und lokale Extrema}

In diesem Kapitel werden wir mit Hilfe der Ableitung notwendige und hinreichende Kriterien für die Existenz von Extrema herleiten. In der Schule wird häufig der Satz verwendet, dass eine Funktion $f:D\to \mathbb {R} $ notwendigerweise $f'({\tilde {x}})=0$ erfüllen muss, damit $f$ in ${\tilde {x}}\in D$ ein (lokales) Extremum hat. Wechselt die Ableitungsfunktion $f'$ in ${\tilde {x}}$ zusätzlich noch das Vorzeichen, so folgt aus dieser Bedingung die Existenz eines Extremums. Der Vorzeichenwechsel der Ableitung ist damit ein hinreichendes Kriterium für die Ableitung. Diese und weitere Folgerungen werden wir nun herleiten, und an Hand zahlreicher Beispiele veranschaulichen. Zunächst werden wir jedoch sauber definieren, welche Art von Extrema es gibt.

\section{Typen von Extrema}

Eine Funktion $f:D\to \mathbb {R} $ kann zunächst einmal zwei Typen eines Extremums haben: Ein \emph{Maximum} oder ein \emph{Minimum}. Dieses kann wiederum \emph{lokal} oder \emph{global} sein. Wie die Bezeichnungen schon vermuten lassen, ist ein lokales Minimum beispielsweise ein Wert $f({\tilde {x}})$, der „lokal minimal“ ist. In einer Umgebung von ${\tilde {x}}$ gilt also $f\geq f({\tilde {x}})$. Sprich: Es gibt ein Intervall $({\tilde {x}}-\epsilon ,{\tilde {x}}+\epsilon )$ um ${\tilde {x}}$, so dass $f(x)\geq f({\tilde {x}})$ für alle Argumente $x$ gilt, die in $({\tilde {x}}-\epsilon ,{\tilde {x}}+\epsilon )$ liegen. Ein globales Maximum hingegen ist ein Wert $f({\hat {x}})$, der „global maximal“ ist. Das heißt, für alle Argumente $x$ aus dem gesamten Definitionsbereich muss $f(x)\leq f({\hat {x}})$ gelten. Diese intuitive Vorstellung ist in folgender Skizze veranschaulicht:

\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Extrema_example_de.svg}{\textbf{Extrema\allowbreak\_example\allowbreak\_de.svg}} by Georg-Johann \textit{(Public domain)}}\begin{center}
\adjincludegraphics[max width=0.5\textwidth,max height=0.2\textheight]{file58extrema95example95de95fbaa1a7ae82c6882d303efbbc6cd155ea1ad7b26}\end{center}

Bei lokalen Extrema wird außerdem noch zwischen strikten und nicht strikten unterschieden. Ein striktes lokales Minimum beispielsweise ist eines, das lokal nur „strikt“ in einem Punkt angenommen wird. Ein nicht striktes Extremum kann auf einem ganzen Teilintervall angenommen werden.

\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Extrema_example_de2.svg}{\textbf{Extrema\allowbreak\_example\allowbreak\_de2.svg}} by Who2010 \textit{(CC0)}}\begin{center}
\adjincludegraphics[max width=0.5\textwidth,max height=0.2\textheight]{file58extrema95example95de295cdf73178f81c670253f22e38b69888bd0466dbad}\end{center}

Die intuitiv erklärten Begriffe definieren wir nun formal:

\begin{definition*}[Extrema]
Sei $D\subseteq \mathbb {R} $ und $f:D\to \mathbb {R} $ eine Funktion. Dann hat $f$ in ${\tilde {x}}\in D$ ein

\begin{itemize}
\item \emph{lokales Maximum} bzw. \emph{Minimum}, falls es ein $\epsilon >0$ gibt, so dass $f({\tilde {x}})\geq f(x)$ (bzw. $f({\tilde {x}})\leq f(x)$) für alle $x\in D$ mit $|x-{\tilde {x}}|<\epsilon $ gilt.
\item \emph{striktes lokales Maximum} bzw. \emph{Minimum}, falls es ein $\epsilon >0$ gibt, so dass $f({\tilde {x}})>f(x)$ (bzw. $f({\tilde {x}})<f(x)$) für alle $x\in D\setminus \{{\tilde {x}}\}$ mit $|x-{\tilde {x}}|<\epsilon $ gilt.
\item \emph{globales Maximum} bzw. \emph{Minimum}, falls $f({\tilde {x}})\geq f(x)$ (bzw. $f({\tilde {x}})\leq f(x)$) für alle $x\in D$ gilt
\end{itemize}

\emph{Extremum} ist der Überbegriff für ein Maximum oder Minimum. ${\tilde {x}}\in D$ heißt \emph{Maximal-} oder \emph{Minimalstelle}.

\end{definition*}

Ein lokales Maximum/Minimum wird in der Literatur auch gelegentlich als \emph{relatives Maximum/Minimum}, und ein striktes Maximum/Minimum als \emph{isoliertes Maximum/Minimum} bezeichnet. Mit der Definition ist außerdem klar, dass jedes globale Extremum auch ein lokales ist. Ebenso ist jedes strikte lokale Extremum auch eines im gewöhnlichen Sinne. Im Folgenden wollen wir mit Hilfe der Ableitung notwendige und hinreichende Bedingungen für (strikte) lokale Extrema bestimmen. Zur Charakterisierung globaler Extrema reichen unsere Kriterien leider nicht aus.

\section{Notwendige Bedingung für Extrema}

\subsection{Satz und Beweis}

Damit eine Funktion an einer Stelle im Inneren Ihres Definitionsbereichs ein lokales Extremum haben kann, muss die Funktion dort eine waagrechte Tangente besitzen. Das heißt, die Ableitung an dieser Stelle muss gleich Null sein. Genau dies besagt der folgende Satz:

\begin{theorem*}[Notwendige Bedingung für Extrema]
Sei $a<b$ mit $a,b\in \mathbb {R} $. Sei ${\tilde {x}}\in (a,b)$ und $f:(a,b)\to \mathbb {R} $ in ${\tilde {x}}$ differenzierbar. Sei weiter ${\tilde {x}}$ ein lokales Minimum (bzw. Maximum). Dann gilt $f'({\tilde {x}})=0$.

\end{theorem*}

\begin{proof*}[Notwendige Bedingung für Extrema]
Wir betrachten den Fall, dass $f$ bei ${\tilde {x}}$ ein lokales Minimum hat. Der Beweis im Fall eines lokalen Maximums geht analog. Wir wollen zeigen, dass

\begin{align*}
f'({\tilde {x}})=\lim _{h\to 0}{\frac {f({\tilde {x}}+h)-f({\tilde {x}})}{h}}=0
\end{align*}

Da $f$ in ${\tilde {x}}$ differenzierbar ist, gilt

\begin{align*}
\lim _{h\downarrow 0}{\frac {f({\tilde {x}}+h)-f({\tilde {x}})}{h}}=f'({\tilde {x}})=\lim _{h\uparrow 0}{\frac {f({\tilde {x}}+h)-f({\tilde {x}})}{h}}
\end{align*}

Da $f$ in ${\tilde {x}}$ ein lokales Minimum hat, gibt es ein $\epsilon >0$, so dass für alle $h\in [0,\epsilon )$ gilt

\begin{align*}
f({\tilde {x}}+h)\geq f({\tilde {x}})\iff f({\tilde {x}}+h)-f({\tilde {x}})\geq 0
\end{align*}

Also ist auch

\begin{align*}
{\frac {f({\tilde {x}}+h)-f({\tilde {x}})}{h}}\geq 0
\end{align*}

Aus den Grenzwertregeln folgt

\begin{align*}
f'({\tilde {x}})=\lim _{h\downarrow 0}{\frac {f({\tilde {x}}+h)-f({\tilde {x}})}{h}}\geq 0
\end{align*}

Andererseits gibt es ein ${\tilde {\epsilon }}>0$, so dass für alle $h\in (-{\tilde {\epsilon }},0)$ gilt

\begin{align*}
f({\tilde {x}}+h)\geq f({\tilde {x}})\iff f({\tilde {x}}+h)-f({\tilde {x}})\geq 0
\end{align*}

Aus den Grenzwertregeln folgt dann

\begin{align*}
f'({\tilde {x}})=\lim _{h\uparrow 0}{\frac {f({\tilde {x}}+h)-f({\tilde {x}})}{h}}\leq 0
\end{align*}

Also ist $f'({\tilde {x}})\leq 0\leq f'({\tilde {x}})$ und daher $f'({\tilde {x}})=0$.

\end{proof*}

\subsection{Anwendung: Zwischenwerteigenschaft für Ableitungen}

Wir haben in den vergangenen Abschnitten bereits festgestellt, dass die Ableitungsfunktion einer differenzierbaren Funktion nicht zwingend stetig sein muss. Ein Beispiel hierfür ist folgende Funktion, die wir im Kapitel \href{https://de.wikibooks.org/wiki/Mathe\_für\_Nicht-Freaks:\_Ableitung\_höherer\_Ordnung\#Anker:Genau\_einmal\_differenzierbare\_Funktion}
{„Ableitung höherer Ordnung“} kennen gelernt haben:

\begin{align*}
f:\mathbb {R} \to \mathbb {R} ,\ f(x)={\begin{cases}x^{2}\sin({\tfrac {1}{x}})&{\text{ für }}x\neq 0,\\0&{\text{ für }}x=0.\end{cases}}
\end{align*}

Allerdings kann man zeigen, dass die Ableitungsfunktion immer die Zwischenwerteigenschaft erfüllt. Dass dies kein Widerspruch ist, liegt daran, dass die Stetigkeit eine stärkere Eigenschaft als die Zwischenwerteigenschaft ist. Zum Beweis werden wir unser notwendiges Kriterium aus dem vorangegangenen Satz verwenden. Dieses Resultat ist in der Literatur auch als „Satz von Darboux“ bekannt:

\begin{theorem*}[Satz von Darboux]
Sei $f:[a,b]\to \mathbb {R} $ differenzierbar. Weiter sei $f'(a)<f'(b)$ und $c\in (f'(a),f'(b))$. Dann existiert ein $x_{0}\in (a,b)$ mit $f'(x_{0})=c$.

\end{theorem*}

\begin{proof*}[Satz von Darboux]
Wir definieren die Hilfsfunktion

\begin{align*}
g:[a,b]\to \mathbb {R} ,\ g(x)=f(x)-cx
\end{align*}

Diese ist differenzierbar mit

\begin{align*}
g'(x)=f'(x)-c
\end{align*}

Damit gilt $g'(a)=f'(a)-c<0$ und $g'(b)=f'(b)-c>0$. Also ist

\begin{align*}
g'(a)=\lim _{x\to a}{\frac {g(x)-g(a)}{x-a}}<0
\end{align*}

Somit gibt es ein $x_{1}\in (a,b)$ mit

\begin{align*}
{\frac {g(x_{1})-g(a)}{x_{1}-a}}<0
\end{align*}

Da der Nenner $x_{1}-a$ positiv ist, folgt $g(x_{1})-g(a)<0\iff g(x_{1})<g(a)$. Analog gibt es ein $x_{2}\in (a,b)$ mit $g(x_{2})<g(b)$. Nach dem \href{https://de.wikibooks.org/wiki/Mathe\_für\_Nicht-Freaks:\_Satz\_vom\_Minimum\_und\_Maximum\#Anker:Satz\_vom\_Minimum\_und\_Maximum}
{Satz vom Minimum und Maximum} nimmt $g$ auf $[a,b]$ ein Minimum an. Da wir gezeigt haben, dass es $x_{1},x_{2}\in (a,b)$ mit $g(x_{1})<g(a)$ und $g(x_{2})<g(b)$, muss das Minimum in $(a,b)$ liegen. Sei $x_{0}\in (a,b)$ die Minimalstelle. Nach unserem notwendigen Kriterium für ein Extremum muss nun gelten

\begin{align*}
g'(x_{0})=f'(x_{0})-c=0
\end{align*}

Daraus folgt $f'(x_{0})=c$.

\end{proof*}

\section{Hinreichende Bedingung: Vorzeichenwechsel der Ableitung}

\subsection{Satz}

Bei vielen Funktionen ist es sehr mühsam, nur mit der notwendigen Bedingung $f({\tilde {x}})=0$ festzustellen, ob $f$ in ${\tilde {x}}$ ein Extremum hat. Daher suchen wir nun hinreichende Bedingungen dafür. Eine Möglichkeit ist, die Umgebung der möglichen Extremstelle ${\tilde {x}}$ zu untersuchen. Wenn die Funktion links von ${\tilde {x}}$ steigt und rechts fällt, gibt es ein Maximum. Wenn die Funktion erst fällt und dann steigt gibt es ein Minimum.

\begin{theorem*}[Hinreichende Bedingung für Extrema über Vorzeichenwechsel der Ableitung]
Sei $a<b$ und $f:(a,b)\to \mathbb {R} $ eine differenzierbare Abbildung. Und gelte $f'({\tilde {x}})=0$ für ein ${\tilde {x}}\in (a,b)$. Dann gilt

\begin{enumerate}
\item $f$ hat ein strenges Maximum in ${\tilde {x}}$, wenn es $\epsilon _{1},\epsilon _{2}>0$ gibt, so dass für alle $x\in ({\tilde {x}}-\epsilon _{1},{\tilde {x}})$ gilt $f'(x)>0$ und für alle $x\in ({\tilde {x}},{\tilde {x}}+\epsilon _{2})$ gilt $f'(x)<0$.
\item $f$ hat ein strenges Minimum in ${\tilde {x}}$, wenn es $\epsilon _{1},\epsilon _{2}>0$ gibt, so dass für alle $x\in ({\tilde {x}}-\epsilon _{1},{\tilde {x}})$ gilt $f'(x)<0$ und für alle $x\in ({\tilde {x}},{\tilde {x}}+\epsilon _{2})$ gilt $f'(x)>0$.
\end{enumerate}

\end{theorem*}

\begin{warning*}
Mit dem hinreichenden Kriterium können lediglich lokale Extrema gefunden werden. Ob diese auch globale sind, oder ob es an weiteren Stellen globale Extrema gibt, muss separat untersucht werden.

\end{warning*}

\subsection{Aufgaben}

\begin{exercise*}[Extremum einer Funktion]
Zeige, dass für $n\in \mathbb {N} $ die Funktion

\begin{align*}
f_{n}:\mathbb {R} _{0}^{+}\to \mathbb {R} ,\ f(x)=x^{n}e^{-x}
\end{align*}

ein lokales Maximum und Minimum besitzt, welches ein globales Maximum bzw. Minimum ist.

\end{exercise*}

\begin{solution*}[Extremum einer Funktion]
\proofstep{Beweisschritt:}
 $f_{n}$ hat lokales Maximum bei ${\tilde {x}}=n$\begin{indentblock}
$f_{n}$ ist auf $\mathbb {R} ^{+}$ nach der Produktregel differenzierbar mit

\begin{align*}
f_{n}'(x)=nx^{n-1}e^{-x}+x^{n}e^{-x}(-1)=x^{n-1}e^{-x}(n-x)
\end{align*}

Nach dem notwendigen Kriterium für die Existenz eines Maximums ${\tilde {x}}\in \mathbb {R} ^{+}$, muss für dieses $f_{n}'({\tilde {x}})=0$ gelten. Nun ist

\begin{align*}
f_{n}'(x)=\underbrace {x^{n-1}e^{-x}} _{\neq 0}(n-x)=0\iff n-x=0\iff x=n
\end{align*}

Also ist ${\tilde {x}}=n$ der einzige Kandidat für unser lokales Maximum in $\mathbb {R} ^{+}$. Weiter gilt

\begin{align*}
f_{n}'(x)=\underbrace {x^{n-1}e^{-x}} _{>0}(n-x)={\begin{cases}>0&\iff n-x>0\iff x<n,\\<0&\iff n-x<0\iff x>n\end{cases}}
\end{align*}

Damit gilt $f_{n}'(x)>0$ für alle $x\in (0,n)$ und $f_{n}'(x)<0$ für alle $x\in (n,n+1)$. Nach dem hinreichenden Kriterium ist daher ${\tilde {x}}=n$ ein (strenges) lokales Maximum von $f_{n}$.

\end{indentblock}

\proofstep{Beweisschritt:}
 $f_{n}$ hat globales Maximum bei ${\tilde {x}}=n$\begin{indentblock}
Da $\mathbb {R} ^{+}$ keine Randpunkte hat, kommt nach Teil 1 nur ${\tilde {x}}=n$ für ein globales Maximum von $f_{n}$ in Frage. Wir müssen dafür $f_{n}(x)\leq f_{n}(n)$ für alle $x\in \mathbb {R} ^{+}$ zeigen. Wegen $f_{n}'(x)>0$ für alle $x\in (0,n)$ ist $f_{n}$ nach dem Monotoniekriterium auf $(0,n]$ streng monoton steigend. Daher gilt für alle $x\in (0,n)$

\begin{align*}
f_{n}(x)<f_{n}(n)
\end{align*}

Analog folgt aus $f_{n}'(x)<0$ für alle $x\in (n,\infty )$, dass $f_{n}$ auf $[n,\infty )$ streng monoton fällt. Also ist für alle $x\in (n,\infty )$

\begin{align*}
f_{n}(x)<f_{n}(n)\iff f(n)>f(x)
\end{align*}

Insgesamt ist damit $f_{n}(n)\geq f_{n}(x)$ für alle $x\in \mathbb {R} ^{+}$. Somit ist ${\tilde {x}}=n$ ein globales Maximum von $f_{n}$. Genau wie im ersten Teil können wir außerdem begründen, dass ${\hat {x}}=0$ auch ein globales Minimum von $f_{n}$ ist.

\end{indentblock}

\proofstep{Beweisschritt:}
 $f_{n}$ hat ein globales Minimum bei ${\hat {x}}=0$\begin{indentblock}
Es gilt $f_{n}(0)=0$ und $f_{n}(x)>0$ für alle $x>0$. Also ist ${\hat {x}}=0$ ein globales und damit ein lokales Minimum von $f_{n}$.

\end{indentblock}

\end{solution*}

\section{Hinreichende Bedingung: Vorzeichen der zweiten Ableitung}

\subsection{Satz}

Ist $f$ zweimal differenzierbar, so können wir auch das folgende hinreichende Kriterium verwenden:

\begin{theorem*}[Hinreichende Bedingung für Extrema über zweite Ableitung]
Sei $f:(a,b)\to \mathbb {R} $ eine zweimal differenzierbare Abbildung, und gelte $f'({\tilde {x}})=0$ für ein ${\tilde {x}}\in (a,b)$. Dann gilt

\begin{enumerate}
\item $f$ hat ein stiktes Maximum in ${\tilde {x}}$, falls $f''({\tilde {x}})<0$ gilt.
\item $f$ hat ein stiktes Minimum in ${\tilde {x}}$, falls $f''({\tilde {x}})>0$ gilt.
\end{enumerate}

\end{theorem*}

\begin{warning*}
Auch dieses hinreichende Kriterium ist \emph{nicht} notwendig. Da wir es aus dem ersten Kriterium gefolgert hatten, ist es sogar schwächer als dieses. Ein Beispiel dafür ist die Funktion

\begin{align*}
f:\mathbb {R} \to \mathbb {R} ,\ f(x)=x^{4}
\end{align*}

Wie wir uns weiter oben schon überlegt hatten, bestitzt $f$ bei ${\tilde {x}}=0$ ein striktes lokales Minimum. Das zweite hinreichende Kriterium ist jedoch nicht anwendbar. Es gilt nämlich

\begin{align*}
f''({\tilde {x}})=12{\tilde {x}}^{2}=12\cdot 0^{2}=0
\end{align*}

Abhilfe schafft hier eine Erweiterung des zweiten hinreichenden Kriteriums, welches wir später diskutieren werden.

\end{warning*}

\subsection{Beispiel und Übungsaufgabe}

\begin{example*}[Überprüfen von Polynomfunktionen auf Extremstellen]
Wir betracheten wieder die Polynomfunktion $g:(-2,0)\to \mathbb {R} $ mit $g(x)=x^{3}-3x$. Wie wir bereits wissen gilt

\begin{align*}
g'(x)=3x^{2}-3=3(x-1)(x+1)
\end{align*}

Damit gilt $g'(x)=0\iff x=-1$ auf $(-2,0)$. Weiter ist

\begin{align*}
f''(x)=6x
\end{align*}

und daher $g''(-1)=-6<0$. Also hat $g$ ein striktes lokales Maximum bei ${\tilde {x}}=-1$.

\end{example*}
\clearpage
\begin{exercise*}[Bestimmung von Extrema einer Funktion]
Gegeben sei die Funktion

\begin{align*}
f:\left[-{\tfrac {1}{2}},\infty \right)\to \mathbb {R} ,\ f(x)={\tfrac {3}{2}}x^{2}-4x-{\tfrac {4}{x+1}}
\end{align*}

Bestimme alle lokalen und globalen Extrema von $f$.

\end{exercise*}

\begin{solution*}[Bestimmung von Extrema einer Funktion]
\proofstep{Schritt 1:}
 Bestimmung der lokalen Extrema von $f$\begin{indentblock}
$f$ ist auf $\left(-{\tfrac {1}{2}},\infty \right)$ differenzierbar mit

\begin{align*}
f'(x)=3x-4+{\tfrac {4}{(x+1)^{2}}}
\end{align*}

Für lokale Extrema in $\left(-{\tfrac {1}{2}},\infty \right)$ muss nun notwendigerweise $f(x)=0$ gelten. Nun ist

\begin{align*}
f(x)=0&\iff 3x-4+{\tfrac {4}{(x+1)^{2}}}=0\\&\iff 3x(x+1)^{2}-4(x+1)^{2}+4=0\\&\iff 3x(x^{2}+2x+1)-4(x^{2}+2x+1)+4=0\\&\iff 3x^{3}+6x^{2}+3x-4x^{2}-8x-4+4=0\\&\iff 3x^{3}+2x^{2}-5x=0\\&\iff 3x\left(x^{2}+{\tfrac {2}{3}}x-{\tfrac {5}{3}}\right)=0\\&\iff 3x\left(x+{\tfrac {5}{3}}\right)(x-1)=0
\end{align*}

Diese Gleichung ist in $\left(-{\tfrac {1}{2}},\infty \right)$ für $x_{1}=0$ und $x_{2}=1$ erfüllt. Also sind $x_{1}$ und $x_{2}$ Kandidaten für lokale Extrema. $f$ ist auf $\left(-{\tfrac {1}{2}},\infty \right)$ auch zweimal differenzierbar, mit

\begin{align*}
f''(x)=3-{\tfrac {8}{(x+1)^{3}}}
\end{align*}

Damit gilt

\begin{align*}
f''(0)=3-{\tfrac {8}{1}}=3-8=-5<0
\end{align*}

Nach unserem zweiten Kriterium hat $f$ bei $x_{1}=0$ ein striktes lokales Maximum. Außerdem ist

\begin{align*}
f''(1)=3-{\tfrac {8}{2^{3}}}=3-1=2>0
\end{align*}

Also hat $f$ bei $x_{2}=1$ ein striktes lokales Minimum. Nun müssen wir noch den Randpunkt $x_{3}=-{\tfrac {1}{2}}$ untersuchen, denn dort greifen unsere Kriterien nicht! Da $f$ in $x_{1}=0$ ein lokales Maximum hat, und auf $\left[-{\tfrac {1}{2}},0\right)$ keine weiteren Nullstellen von $f'$ liegen, ist $f$ auf $\left(-{\tfrac {1}{2}},0\right)$ streng monoton fallend. Also gilt $f\left(-{\tfrac {1}{2}}\right)<f(x)$ für alle $x\in \left(-{\tfrac {1}{2}},0\right)$. Daher hat $f$ in $x_{3}=-{\tfrac {1}{2}}$ ein striktes lokales Minimum.

\end{indentblock}

\proofstep{Schritt 2:}
 Bestimmung der globalen Extrema von $f$\begin{indentblock}
Aus dem ersten Beweisschritt ergibt sich die folgende Monotonietabelle für $f$:

\begin{align*}
{\begin{array}{|c|c|c|c|}\hline x\in &(-{\tfrac {1}{2}},0)&(0,1)&(1,\infty )\\\hline f&{\text{sms}}&{\text{smf}}&{\text{sms}}\\\hline \end{array}}
\end{align*}

Weiter gilt $f(-{\tfrac {1}{2}})=-5{\tfrac {5}{8}}<-4{\tfrac {1}{2}}=f(1)$. Damit ist $x_{3}=-{\tfrac {1}{2}}$ ein globales Minimum von $f$. Schließlich ist

\begin{align*}
\lim _{x\to \infty }f(x)&=\lim _{x\to \infty }{\tfrac {3}{2}}x^{2}-4x-{\tfrac {4}{x+1}}\\[0.3em]&=\lim _{x\to \infty }\underbrace {x^{2}} _{\to \infty }\cdot \underbrace {\left({\tfrac {3}{2}}-{\tfrac {4}{x}}-{\tfrac {4}{x^{2}(x+1)}}\right)} _{\to {\tfrac {3}{2}}}=\infty 
\end{align*}

Also ist $f$ nach oben unbeschränkt und besitzt somit \emph{kein} globales Maximum.

\end{indentblock}

\end{solution*}

\part{Integrale}

\addxcontentsline{lof}{part}[\arabic{part}]{Integrale}\begin{authors}
Autorenkollektiv „Auswahlaxiom“ (Charlotte Dietze, Matthias Paulsen, Anne Reif), Who2010, EulerschesPi, Stephan Kulla, GraffL, Michael D'Erchie, Sven Prüfer, Jenny Kilian, SerloBot, Matthias Greger, Meitnerium266, Boehm, Dirk Hünniger\end{authors}

\chapter{Das Integral}

Das Integral ist neben der Ableitung eines der wichtigsten Konzepte der Analysis. Es handelt sich um eine Art Umkehrung der Differentialrechnung. Mit ihm ist es möglich, viele interessante Fragestellungen aus der Flächenberechnung und der Physik zu beantworten.

Wir wollen in diesem Kapitel eine anschauliche Vorstellung des Integrals einführen, bevor wir diese im nächsten Kapitel mithilfe von Riemannintegralen präzisieren.

\section{Motivation}

Wir können das Integral im Grunde auf zwei verschiedenen Weisen motivieren. Die erste Möglichkeit sind Probleme aus der Flächenberechnung, zum Beispiel „Wie groß ist die Fläche einer Ellipse?“. Das heißt, wir wollen komplizierte krummlinige Flächeninhalte berechnen.

Beim zweiteren geht es um Fragestellungen wie „Wie groß ist der zurückgelegte Weg $s$ meines Autos, wenn die Geschwindigkeit $v(t)$ zu jedem Zeitpunkt bekannt ist?“. Wir wollen also, dass wir bei bekannter Ableitung Rückschlüsse auf die Gesamtänderung einer Funktion ziehen können.

Damit sind wir auch an einer Art Verallgemeinerung von Summen und Reihen interessiert. Würde sich nämlich die Geschwindigkeit deines Autos nicht von Moment zu Moment ändern, sondern nur alle 10 Zeiteinheiten, dann wäre es möglich, mithilfe einer Summe den zurückgelegten Weg auszurechnen. Ebenso wäre es möglich, von Flächen, die sich aus Rechtecken unterschiedlicher Größe zusammensetzen, den Flächeninhalt mit einer Summe zu berechnen. Während man also bei Summen über eine endliche Anzahl und bei Reihen über eine abzählbare Menge von Zahlen summiert, wollen wir nun über eine überabzählbare Menge, nämlich alle Funktionswerte einer Funktion, summieren. Dies ist aber mit den konventionellen Methoden nicht möglich. Wir benötigen daher ein neues mathematisches Instrument, das Integral.
\clearpage
\section{Intuitionen des Integrals}

Für das Integral gibt es mehrere Intuitionen, die alle eng zusammenhängen:

\begin{itemize}
\item \emph{Integral als orientierter Flächeninhalt:} Das Integral ist der orientierte Flächeninhalt zwischen dem Graphen einer Funktion und der $x$-Achse. Orientiert bedeutet, dass Flächenstücke oberhalb der $x$-Achse positiv und Flächenstücke unterhalb der $x$-Achse negativ gezählt werden.
\item \emph{Integral als Gesamtänderung einer Funktion:} Das Integral ist gleich der Gesamtänderung einer Funktion zwischen den Argumenten $a$ und $b$, die an jeder Stelle $x$ die Ableitung $f(x)$ besitzt.
\item \emph{Integral als verallgemeinerte Summe:} Das Integral ist eine Summe über eine überabzählbare Menge an Summanden.
\end{itemize}



\chapter{Riemannintegral}

\section{Warum Riemannintegrale?}

\subsection{Integrale als orientierter Flächeninhalt}

In der Schule wird der Ausdruck $\int _{a}^{b}f(x)\,\mathrm {d} x$ als orientierter Flächeninhalt zwischen dem Graphen von $f$ und der $x$-Achse im Intervall $[a,b]$ definiert. Dabei bedeutet „orientiert“, dass Flächeninhalte oberhalb der $x$-Achse positiv und Flächeninhalte unterhalb negativ gewertet werden:

\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Integral_example.svg}{\textbf{Integral\allowbreak\_example.svg}} by Actam, KSmrq~commonswiki \textit{(CC-BY-SA-3.0)}}\begin{center}
\adjincludegraphics[max width=0.5\textwidth,max height=0.2\textheight]{file58integral95example9587aac71933bba8ab9b6df19a1033313ce3904793}\end{center}

Diese Definition ist vordergründig ausreichend, zeigt aber bei genauerer Betrachtung Probleme. So ist der Begriff des „Flächeninhalts“ nicht mathematisch präzise definiert. Auch ist nicht klar, ob die Bestimmung des „Flächeninhalts unter dem Graphen“ immer funktioniert.

\subsection{Eine seltsame Funktion}

Wir definieren die Funktion $f:[0,1]\to \mathbb {R} $ für $x\in [0,1]$ folgendermaßen:

\begin{align*}
f(x)={\begin{cases}1&{\text{falls }}x\in \mathbb {Q} \\0&{\text{sonst}}\end{cases}}
\end{align*}

Dies ist die sogenannte „Dirichlet-Funktion“. Sie ist eingeschränkt auf das Intervall $[0,1]$. Sie nimmt bei allen rationalen Zahlen den Wert $1$ und bei allen irrationalen Zahlen den Wert $0$ an. Das Zeichnen des zugehörigen Funktionsgraphen stellt uns vor große Probleme. Der Funktionswert wechselt ständig zwischen $0$ und $1$ hin und her (damit ist die Funktion nirgends stetig). Da in jedem noch so kleinen Intervall $[a,b]$ mit $a<b$ sowohl rationale als auch irrationale Zahlen liegen, besteht der Graph von $f$ aus zwei Ansammlungen von Punkten – einmal auf Höhe $0$ und einmal auf Höhe $1$ – die wie zwei durchgängige Strecken aussehen:

\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Dirichlet-function.svg}{\textbf{Dirichlet\allowbreak-function.svg}} by MartinThoma \textit{(CC BY 3.0)}}\begin{center}
\adjincludegraphics[max width=0.5\textwidth,max height=0.2\textheight]{file58dirichlet45function95f5d5eb0594c15a206f301ae6c53d3cfbd1dfd029}\end{center}

Wir können nicht genau sagen, ob der Flächeninhalt zwischen der $x$-Achse und dem Graphen von $f$ den Wert $0$, $1$ oder etwas dazwischen haben sollte. Wenn wir das Riemannintegral eingeführt haben, werden wir tatsächlich sehen, dass diese Funktion nicht \emph{riemannintegrierbar} ist.

\subsection{Notwendigkeit einer präzisen Definition}

Am Beispiel der Dirichlet-Funktion sieht man, dass nicht bei jeder Funktion der Flächeninhalt unter dem Graphen bestimmt werden kann. Wir brauchen also eine Methode für die Entscheidung, ob der Flächeninhalt unter dem Graphen existiert und wie groß er ist. Eine solche Methode bietet das Riemannintegral. Es erlaubt uns zu entscheiden, welche Funktionen integrierbar sind (sprich: bei denen ein orientierter Flächeninhalt unter dem Graphen bestimmt werden kann). Ferner können wir mit ihm die Eigenschaften von Integralen beweisen.

\section{Herleitung des Riemannintegrals}

\subsection{Ein Verfahren zur Abschätzung des Integrals}

Sei $f:[a,b]\to \mathbb {R} $ eine stetige Funktion. Zunächst können wir versuchen, den Flächeninhalt von $f$ unter dem Graphen abzuschätzen. Da $f$ stetig ist, nimmt die Funktion ihr Maximum $M$ und Minimum $m$ an. Der gesuchte Flächeninhalt ist nicht größer als der Flächeninhalt des Rechtecks mit der Breite $b-a$ und der Höhe $M$. Auch ist er nicht kleiner als die Fläche des Rechtecks mit der Breite $b-a$ und der Höhe $m$:

\begin{tabularx}{\linewidth}{XX}
\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Mfnf-riemann-single-lo.svg}{\textbf{Mfnf\allowbreak-riemann\allowbreak-single\allowbreak-lo.svg}} by Autorenkollektiv „Auswahlaxiom“ (Charlotte Dietze, Matthias Paulsen, Anne Reif) \textit{(CC BY-SA 4.0)}}\begin{minipage}[t]{\linewidth}
\begin{figure}[H]
\begin{minipage}[t][0.2\textheight][c]{\linewidth}
\centering
\adjincludegraphics[max width=1.\linewidth, max height=0.2\textheight]{file58mfnf45riemann45single45lo95cfad8232da7e62b3af62dda792f3d041133b55e9}
\end{minipage}
\caption*{Untere Schranke an das Integral durch ein Rechteck der Breite $b-a$ (\arabic{imagelabel})}
\end{figure}

\end{minipage}
&
\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Mfnf-riemann-single-up.svg}{\textbf{Mfnf\allowbreak-riemann\allowbreak-single\allowbreak-up.svg}} by Autorenkollektiv „Auswahlaxiom“ (Charlotte Dietze, Matthias Paulsen, Anne Reif) \textit{(CC BY-SA 4.0)}}\begin{minipage}[t]{\linewidth}
\begin{figure}[H]
\begin{minipage}[t][0.2\textheight][c]{\linewidth}
\centering
\adjincludegraphics[max width=1.\linewidth, max height=0.2\textheight]{file58mfnf45riemann45single45up954b5dda0ae71e86d399109510fdce1fa5c26dbf0a}
\end{minipage}
\caption*{Obere Schranke an das Integral durch ein Rechteck der Breite $b-a$ (\arabic{imagelabel})}
\end{figure}

\end{minipage}
\end{tabularx}

Wir erhalten als Abschätzung:

\begin{align*}
{\color {WildStrawberry}\underbrace {(b-a)\cdot m} _{\text{kleines Rechteck}}}\leq \int _{a}^{b}f(x)\,\mathrm {d} x\leq {\color {NavyBlue}\underbrace {(b-a)\cdot M} _{\text{großes Rechteck}}}
\end{align*}

Diese Abschätzung ist noch nicht besonders gut. Besser wird sie durch eine Aufteilung des Intervalls in zwei Teilintervalle $\left[a,{\tfrac {a+b}{2}}\right]$ und $\left[{\tfrac {a+b}{2}},b\right]$. In beiden Teilintervallen kann der Flächeninhalt mit Hilfe des jeweiligen Minimums und Maximums abgeschätzt werden. Die Flächen der Rechtecke mit der Höhe des jeweiligen Funktionsmaximums und der halben Intervalllänge als Breite schätzen den Flächeninhalt nach oben ab. Mit Hilfe der analogen Rechtecke mit den Funktionsminima als Höhe kann der Flächeninhalt nach unten abgeschätzt werden:

\begin{tabularx}{\linewidth}{XX}
\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Mfnf-riemann-double-lo.svg}{\textbf{Mfnf\allowbreak-riemann\allowbreak-double\allowbreak-lo.svg}} by Autorenkollektiv „Auswahlaxiom“ (Charlotte Dietze, Matthias Paulsen, Anne Reif) \textit{(CC BY-SA 4.0)}}\begin{minipage}[t]{\linewidth}
\begin{figure}[H]
\begin{minipage}[t][0.2\textheight][c]{\linewidth}
\centering
\adjincludegraphics[max width=1.\linewidth, max height=0.2\textheight]{file58mfnf45riemann45double45lo9566c356f190c2d9bb83f0b115089a9e327f059d09}
\end{minipage}
\caption*{Untere Schranke an das Integral bei Aufteilung in zwei Teilintervalle (\arabic{imagelabel})}
\end{figure}

\end{minipage}
&
\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Mfnf-riemann-double-up.svg}{\textbf{Mfnf\allowbreak-riemann\allowbreak-double\allowbreak-up.svg}} by Autorenkollektiv „Auswahlaxiom“ (Charlotte Dietze, Matthias Paulsen, Anne Reif) \textit{(CC BY-SA 4.0)}}\begin{minipage}[t]{\linewidth}
\begin{figure}[H]
\begin{minipage}[t][0.2\textheight][c]{\linewidth}
\centering
\adjincludegraphics[max width=1.\linewidth, max height=0.2\textheight]{file58mfnf45riemann45double45up9597cbda674c6ec4762c819df5788582cb80399368}
\end{minipage}
\caption*{Obere Schranke an das Integral bei Aufteilung in zwei Teilintervalle (\arabic{imagelabel})}
\end{figure}

\end{minipage}
\end{tabularx}

Seien $M_{1}$ und $M_{2}$ die Maxima von $f$ auf den Intervallen $\left[a,{\tfrac {a+b}{2}}\right]$ und $\left[{\tfrac {a+b}{2}},b\right]$. $m_{1}$ und $m_{2}$ seien die jeweiligen Minima. Der orientierte Flächeninhalt unter $f$ kann nun folgendermaßen abgeschätzt werden:

\begin{align*}
{\color {WildStrawberry}\underbrace {{\frac {b-a}{2}}\cdot m_{1}} _{\text{kl. Rechteck 1}}+\underbrace {{\frac {b-a}{2}}\cdot m_{2}} _{\text{kl. Rechteck 2}}}\leq \int _{a}^{b}f(x)\,\mathrm {d} x\leq {\color {NavyBlue}\underbrace {{\frac {b-a}{2}}\cdot M_{1}} _{\text{gr. Rechteck 1}}+\underbrace {{\frac {b-a}{2}}\cdot M_{2}} _{\text{gr. Rechteck 1}}}
\end{align*}

Die dabei auftretende Summe ${\tfrac {b-a}{2}}\cdot M_{1}+{\tfrac {b-a}{2}}\cdot M_{2}$, die den Flächeninhalt unter dem Graphen von $f$ nach oben abschätzt, wird \emph{Obersumme} genannt. Entsprechend heißt die Summe für die Abschätzung nach unten \emph{Untersumme}. Noch besser wird die Abschätzung, wenn wir diesen Prozess fortführen und das Intervall in $4$, $8$, $16$, ... in Teilintervalle zerlegen. Bei $32$ Teilintervallen erhalten wir:

\begin{tabularx}{\linewidth}{XX}
\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Mfnf-riemann-uniform.svg}{\textbf{Mfnf\allowbreak-riemann\allowbreak-uniform.svg}} by Autorenkollektiv „Auswahlaxiom“ (Charlotte Dietze, Matthias Paulsen, Anne Reif) \textit{(CC BY-SA 4.0)}}\begin{minipage}[t]{\linewidth}
\begin{figure}[H]
\begin{minipage}[t][0.2\textheight][c]{\linewidth}
\centering
\adjincludegraphics[max width=1.\linewidth, max height=0.2\textheight]{file58mfnf45riemann45uniform95e6327c3d25a1a7bcfbcd92df75e696023e06f1c5}
\end{minipage}
\caption*{Untere Schranke bei gleichmäßiger Aufteilung des Grundintervalls in $2^{5}=32$ Teilintervalle (\arabic{imagelabel})}
\end{figure}

\end{minipage}
&
\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Mfnf-riemann-uniform2.svg}{\textbf{Mfnf\allowbreak-riemann\allowbreak-uniform2.svg}} by Autorenkollektiv „Auswahlaxiom“ (Charlotte Dietze, Matthias Paulsen, Anne Reif) \textit{(CC BY-SA 4.0)}}\begin{minipage}[t]{\linewidth}
\begin{figure}[H]
\begin{minipage}[t][0.2\textheight][c]{\linewidth}
\centering
\adjincludegraphics[max width=1.\linewidth, max height=0.2\textheight]{file58mfnf45riemann45uniform295a95d1ea8e32d7fa5d18721c11b4ffa57705a10ed}
\end{minipage}
\caption*{Obere Schranke bei gleichmäßiger Aufteilung des Grundintervalls in $2^{5}=32$ Teilintervalle (\arabic{imagelabel})}
\end{figure}

\end{minipage}
\end{tabularx}

Bei $2^{n}$ Teilintervallen erhalten wir die Intervalle $I_{k,n}=\left[a+(k-1){\tfrac {b-a}{2^{n}}},a+k{\tfrac {b-a}{2^{n}}}\right]$, wobei $k$ eine Zahl zwischen $1$ und $2^{n}$ ist. Sei $M_{k,n}$ das Maximum und $m_{k,n}$ das Minimum von $f$ im $k$-ten Intervall $I_{k,n}$. Der orientierte Flächeninhalt unter dem Graphen von $f$ kann nun abgeschätzt werden über:

\begin{align*}
{\color {WildStrawberry}\underbrace {\sum _{k=1}^{2^{n}}{\frac {b-a}{2^{n}}}\cdot m_{k,n}} _{\text{Untersumme}}}\leq \int _{a}^{b}f(x)\,\mathrm {d} x\leq {\color {NavyBlue}\underbrace {\sum _{k=1}^{2^{n}}{\frac {b-a}{2^{n}}}\cdot M_{k,n}} _{\text{Obersumme}}}
\end{align*}

Obige Abschätzung sollte mit wachsendem $n$ immer besser werden, da die Einteilung des Grundintervalls $[a,b]$ immer besser wird. Wir vermuten, dass mit der Anzahl der Unterteilungen der Fehler zwischen der Abschätzung nach oben bzw. nach unten und dem tatsächlichen Flächeninhalt immer kleiner wird. Im Grenzwert $n\to \infty $ sollte sowohl die Abschätzung nach oben als auch die Abschätzung nach unten gegen den orientierten Flächeninhalt konvergieren. Es sollte also gelten:

\begin{align*}
\lim _{n\to \infty }\sum _{k=1}^{2^{n}}{\frac {b-a}{2^{n}}}\cdot m_{k,n}=\int _{a}^{b}f(x)\,\mathrm {d} x=\lim _{n\to \infty }\sum _{k=1}^{2^{n}}{\frac {b-a}{2^{n}}}\cdot M_{k,n}
\end{align*}

Durch Unterteilung des Grundintervalls in $2^{n}$ konnten wir eine Ober- bzw. eine Untersumme bilden. Diese schätzen den orientierten Flächeninhalt unter dem Graphen von $f$ nach oben bzw. nach unten ab. Mit wachsendem $n$ wird diese Abschätzung immer besser und mit dem Grenzübergang $n\to \infty $ konvergiert sowohl die Ober- als auch die Untersumme gegen den tatsächlichen orientierten Flächeninhalt von $f$. Damit haben wir ein Verfahren gefunden, um den orientierten Flächeninhalt einer Funktion zu bestimmen.

\subsection{Zerlegungen}

Bisher haben wir das Grundintervall in $2^{n}$ gleich große Teilintervalle zerlegt. Jedoch kann bei einer beliebigen Unterteilung des Grundintervalls mit beliebig vielen und beliebig großen Teilintervallen eine Abschätzung nach oben und nach unten nach dem obigen Verfahren gebildet werden. Dadurch kann unser Verfahren verallgemeinert werden. Dies kann beispielsweise genutzt werden, um kleinere Teilintervalle in den Bereichen zu wählen, wo sich die Funktion stark ändert. Damit kann die Qualität der Abschätzung verbessert werden. Die folgende Abbildung zeigt eine Unterteilung von $[a,b]$ in zehn unterschiedlich große Teilintervalle:

\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Mfnf-riemann-zerl.svg}{\textbf{Mfnf\allowbreak-riemann\allowbreak-zerl.svg}} by Autorenkollektiv „Auswahlaxiom“ (Charlotte Dietze, Matthias Paulsen, Anne Reif) \textit{(CC BY-SA 4.0)}}\begin{center}
\adjincludegraphics[max width=0.5\textwidth,max height=0.2\textheight]{file58mfnf45riemann45zerl95404689a14779fbfd3d78e1f722fc9733d2116aba}\end{center}

Um eine solche Unterteilung zu definieren, reicht es, die Zahlen $x_{1},\ldots ,x_{n-1}$ anzugeben. Zusammen mit $a$ und $b$ bilden sie die Randpunkte der Teilintervalle. Die Zahlen $a,x_{1},\ldots ,x_{n-1},b$ werden deswegen \emph{Stützstellen} genannt. Für eine einheitliche Notation definiert man $x_{0}=a$ und $x_{n}=b$. Das Tupel aller Stützstellen $(x_{0},x_{1},\ldots ,x_{n})$ wird \emph{Zerlegung des Intervalls $[a,b]$} genannt.

Bei einer gegebenen Zerlegung $(x_{0},x_{1},\ldots ,x_{n})$ ist $[x_{k-1},x_{k}]$ mit $1\leq k\leq n$ das $k$-te Teilintervall. Seine Länge ist $x_{k}-x_{k-1}$. Fassen wir zusammen:

\begin{definition*}[Zerlegung]
Sei ein Intervall $[a,b]$ mit $a,b\in \mathbb {R} $ und $a\leq b$ gegeben. Ein $(n+1)$-Tupel $\Delta =(x_{0},x_{1},\ldots ,x_{n})$ ist genau dann eine Zerlegung des Intervalls $[a,b]$, wenn $a=x_{0}<x_{1}<\ldots <x_{n}=b$. Die Zahlen des Tupels werden Stützstellen der Zerlegung genannt.

\end{definition*}

Im obigen Verfahren haben wir die Teilintervalle der Zerlegung durch Hinzunahme von Stützstellen weiter unterteilt. Eine solche Zerlegung, die wir durch Hinzunahme von weiteren Stützstellen erhalten, wird \emph{Verfeinerung der Zerlegung} genannt:

\begin{definition*}[Verfeinerung einer Zerlegung]
Seien $a,b\in \mathbb {R} $ mit $a\leq b$. Seien $\Delta =(x_{0},x_{1},\ldots ,x_{n})$ und ${\tilde {\Delta }}=(y_{0},y_{1},\ldots ,y_{m})$ zwei Zerlegungen des Intervalls $[a,b]$. Dann heißt ${\tilde {\Delta }}$ eine \emph{Verfeinerung} von $\Delta $, wenn $\{x_{0},x_{1},\ldots ,x_{n}\}\subseteq \{y_{0},y_{1},\ldots y_{m}\}$ gilt. ${\tilde {\Delta }}$ enthält also (neben möglicherweise zusätzlichen Stützstellen) alle Stützstellen von $\Delta $.

\end{definition*}

Durch zusätzliche Stützstellen wollen wir die Approximation des orientierten Flächeninhalts durch Ober- und Untersumme verbessern. Dabei ist es notwendig, dass die Teilintervalle immer kleiner werden. Um insgesamt die Güte einer Zerlegung zu beurteilen, nennen wir die Länge des größten Teilintervalls die \emph{Feinheit der Zerlegung}. Diese sollte im Laufe der Abschätzung immer kleiner werden und im Grenzwert gegen Null konvergieren:

\begin{definition*}[Feinheit]
Es sei eine Zerlegung $\Delta =(x_{0},x_{1},\ldots ,x_{n})$ eines Intervalls $[a,b]$ mit $a,b\in \mathbb {R} $ und $a\leq b$ gegeben. Wir definieren die Feinheit $|\Delta |$ der Zerlegung $\Delta $ durch

\begin{align*}
|\Delta |:=\max\{x_{1}-x_{0},x_{2}-x_{1},\ldots ,x_{n}-x_{n-1}\}
\end{align*}

\end{definition*}

\subsection{Ober- und Untersummen}

Sei nun $f:[a,b]\to \mathbb {R} $ eine beliebige und nicht unbedingt stetige Funktion. Durch $\sup _{x\in [x_{k-1},x_{k}]}f(x)$ finden wir die (kleinste) obere Schranke für die Funktionswerte von $f$ im Teilintervall $[x_{k-1},x_{k}]$. Analog finden wir über $\inf _{x\in [x_{k-1},x_{k}]}f(x)$ eine Abschätzung nach unten für die Funktionswerte von $f$. Damit alle Suprema und Infima existieren, nehmen wir zusätzlich an, dass $f$ beschränkt ist. Diese Suprema können nun benutzt werden, um den Flächeninhalt nach oben und nach unten durch Rechtecke zu bestimmen:

\begin{tabularx}{\linewidth}{XX}
\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Mfnf-riemann-lower.svg}{\textbf{Mfnf\allowbreak-riemann\allowbreak-lower.svg}} by Autorenkollektiv „Auswahlaxiom“ (Charlotte Dietze, Matthias Paulsen, Anne Reif) \textit{(CC BY-SA 4.0)}}\begin{minipage}[t]{\linewidth}
\begin{figure}[H]
\begin{minipage}[t][0.2\textheight][c]{\linewidth}
\centering
\adjincludegraphics[max width=1.\linewidth, max height=0.2\textheight]{file58mfnf45riemann45lower95c4592814da133f76fbae9983b253e22315e24385}
\end{minipage}
\caption*{Der rote Flächeninhalt ist die Untersumme einer gegebenen Zerlegung mit zehn Teilintervallen (\arabic{imagelabel})}
\end{figure}

\end{minipage}
&
\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Mfnf-riemann-upper.svg}{\textbf{Mfnf\allowbreak-riemann\allowbreak-upper.svg}} by Autorenkollektiv „Auswahlaxiom“ (Charlotte Dietze, Matthias Paulsen, Anne Reif) \textit{(CC BY-SA 4.0)}}\begin{minipage}[t]{\linewidth}
\begin{figure}[H]
\begin{minipage}[t][0.2\textheight][c]{\linewidth}
\centering
\adjincludegraphics[max width=1.\linewidth, max height=0.2\textheight]{file58mfnf45riemann45upper957c1a35631411e7d3a729b68b9be0f9cbb37e7ad7}
\end{minipage}
\caption*{Der blaue Flächeninhalt ist die Obersumme einer gegebenen Zerlegung mit zehn Teilintervallen (\arabic{imagelabel})}
\end{figure}

\end{minipage}
\end{tabularx}

Das Produkt $(x_{k-1}-x_{k})\cdot \sup _{x\in [x_{k-1},x_{k}]}f(x)$ ist der Flächeninhalt des Rechtecks über dem Teilintervall $[x_{k-1},x_{k}]$ mit der Höhe $\sup _{x\in [x_{k-1},x_{k}]}f(x)$. Es ist eine Abschätzung nach oben für den Flächeninhalt unter $f$ eingeschränkt auf $[x_{k-1},x_{k}]$. Durch Summation dieser Produkte für alle Teilintervalle erhält man insgesamt die Abschätzung des Flächeninhalts nach oben für diese Zerlegung:

\begin{align*}
\int _{a}^{b}f(x)\,\mathrm {d} x\leq {\color {NavyBlue}\sum _{k=1}^{n}(x_{k}-x_{k-1})\sup _{x\in [x_{k-1},x_{k}]}f(x)}
\end{align*}

Analog können wir den Flächeninhalt auch nach unten abschätzen und erhalten so:

\begin{align*}
{\color {WildStrawberry}\underbrace {\sum _{k=1}^{n}(x_{k}-x_{k-1})\inf _{x\in [x_{k-1},x_{k}]}f(x)} _{\text{Untersumme}}}\leq \int _{a}^{b}f(x)\,\mathrm {d} x\leq {\color {NavyBlue}\underbrace {\sum _{k=1}^{n}(x_{k}-x_{k-1})\sup _{x\in [x_{k-1},x_{k}]}f(x)} _{\text{Obersumme}}}
\end{align*}

Die jeweiligen Summen werden Ober- und Untersumme genannt:

\begin{definition*}[Ober- und Untersummen]
Für eine Zerlegung $\Delta =(x_{0},x_{1},\ldots ,x_{n})$ des Intervalls $[a,b]$ und eine beschränkte Funktion $f:[a,b]\to \mathbb {R} $ definieren wir die Obersumme

\begin{align*}
O(\Delta ,f):=\sum _{k=1}^{n}(x_{k}-x_{k-1})\sup _{x\in [x_{k-1},x_{k}]}f(x)
\end{align*}

Die Definition der Untersumme lautet:

\begin{align*}
U(\Delta ,f):=\sum _{k=1}^{n}(x_{k}-x_{k-1})\inf _{x\in [x_{k-1},x_{k}]}f(x)
\end{align*}

\end{definition*}

\subsection{Oberes und unteres Integral}

Mit jeder Obersumme $O(\Delta ,f)$ haben wir eine Abschätzung des Flächeninhalts nach oben, die bei feineren Zerlegungen immer besser wird. Im Grenzwert beliebig feiner Zerlegungen sollte die Obersumme $O(\Delta ,f)$ gegen den tatsächlichen Flächeninhalt streben. Die „kleinstmögliche“ Obersumme sollte also der gesuchte Flächeninhalt sein. „Kleinstmöglich“ steht in Anführungszeichen, da sich jede Obersumme vom tatsächlichen Flächeninhalt unterscheiden kann. Der Unterschied kann aber beliebig klein werden (wenn die Zerlegung hinreichend fein gewählt wird). Deswegen müssen wir „kleinstmöglich“ durch den „kleinstmöglichen Grenzwert von Obersummen“ bzw. die „größtmögliche untere Schranke für alle Obersummen“ ersetzen. Wir bilden also das Infimum $\inf _{\Delta {\text{ Zerlegung}}}O(\Delta ,f)$ der Menge aller möglichen Obersummen und dieses sollte der orientierten Fläche unter dem Graphen entsprechen. Wir nennen dieses Infimum \emph{oberes Integral}, da es durch Abschätzungen nach oben gewonnen wird. Als Schreibweise wählen wir ${\overline {\int _{a}^{b}}}f(x)\,\mathrm {d} x$. Analog können wir das untere Integral als Supremum aller Untersummen definieren:

\begin{definition*}[Oberes und unteres Integral]
Sei $f:[a,b]\to \mathbb {R} $ eine beschränkte Funktion. Wir definieren das obere Integral ${\overline {\int _{a}^{b}}}f(x)\,\mathrm {d} x$ und das untere Integral ${\underline {\int _{a}^{b}}}f(x)\,\mathrm {d} x$ über

\begin{align*}
{\overline {\int _{a}^{b}}}f(x)\,\mathrm {d} x&:=\inf _{\Delta {\text{ Zerlegung}}}O(\Delta ,f)\\[0.3em]{\underline {\int _{a}^{b}}}f(x)\,\mathrm {d} x&:=\sup _{\Delta {\text{ Zerlegung}}}U(\Delta ,f)
\end{align*}

\end{definition*}

\subsection{Definition des Riemannintegrals}

Was passiert bei Funktionen, denen man nicht sinnvoll einen Flächeninhalt unter dem Graphen zuordnen kann? Denken wir an die Dirichlet-Funktion $f:[a,b]\to \mathbb {R} $, die bei rationalen Zahlen den Wert $1$ und bei irrationalen Zahlen den Wert $0$ hat. Jedes Teilintervall $[x_{k-1},x_{k}]$ besitzt sowohl rationale als auch irrationale Zahlen. Damit ist der maximale Funktionswert von $f$ auf $[x_{k-1},x_{k}]$ stets $1$ und der minimale Funktionswert ist gleich $0$. Unabhängig von der Zerlegung $\Delta $ erhalten wir:

\begin{align*}
O(\Delta ,f)&=\sum _{k=1}^{n}(x_{k}-x_{k-1})\underbrace {\sup _{x\in [x_{k-1},x_{k}]}f(x)} _{=\ 1}=\sum _{k=1}^{n}(x_{k}-x_{k-1})\\[0.5em]&=(x_{1}-x_{0})+(x_{2}-x_{1})+\ldots +(x_{n}-x_{n-1})=x_{n}-x_{0}=b-a\\[0.5em]U(\Delta ,f)&=\sum _{k=1}^{n}(x_{k}-x_{k-1})\underbrace {\inf _{x\in [x_{k-1},x_{k}]}f(x)} _{=\ 0}=\sum _{k=1}^{n}0=0
\end{align*}

Damit haben wir

\begin{align*}
{\overline {\int _{a}^{b}}}f(x)\,\mathrm {d} x&=\inf _{\Delta {\text{ Zerlegung}}}O(\Delta ,f)=\inf _{\Delta {\text{ Zerlegung}}}(b-a)=b-a\\[0.5em]{\underline {\int _{a}^{b}}}f(x)\,\mathrm {d} x&=\sup _{\Delta {\text{ Zerlegung}}}U(\Delta ,f)=\sup _{\Delta {\text{ Zerlegung}}}0=0
\end{align*}

Bei der Dirichlet-Funktion stimmt das obere mit dem unteren Integral nicht überein und so erhalten wir kein eindeutiges Ergebnis für den orientierten Flächeninhalt unter dem Graphen. Was tun? Wir führen eine Klassifikation in {''}schöne{''} und {''}unschöne{''} Funktionen ein. Bei {''}schönen{''} Funktionen stimmt das obere mit dem unteren Integral überein. Beide Verfahren liefern dasselbe Ergebnis und wir können dieses als orientierten Flächeninhalt unter dem Graphen definieren. Solche {''}schönen{''} Funktionen nennen wir \emph{riemannintegrierbar} oder kurz \emph{integrierbar}.

Bei {''}unschönen{''} Funktionen liefern das obere und das untere Integral unterschiedliche Ergebnisse. Es ist nicht eindeutig, was der Flächeninhalt unter dem Graphen sein soll und wir behaupten deshalb, dass dieses (nach unserem Verfahren) nicht existiert. Solche {''}unschönen{''} Funktionen heißen deshalb \emph{nicht riemannintegrierbar}.

\begin{definition*}[Riemannintegral]
Eine Funktion $f:[a,b]\to \mathbb {R} $ heißt \emph{riemannintegrierbar}, wenn $f$ beschränkt ist und ${\overline {\int _{a}^{b}}}f(x)\,\mathrm {d} x={\underline {\int _{a}^{b}}}f(x)\,\mathrm {d} x$. Dann definieren wir das \emph{(Riemann-)Integral} durch

\begin{align*}
\int _{a}^{b}f(x)\,\mathrm {d} x:={\overline {\int _{a}^{b}}}f(x)\,\mathrm {d} x={\underline {\int _{a}^{b}}}f(x)\,\mathrm {d} x
\end{align*}

\end{definition*}

\section{Abschätzung zwischen oberem und unterem Integral}

\begin{figure}[h]
\vspace{\baselineskip}
\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Mfnf-riemann-uplo.svg}{\textbf{Mfnf\allowbreak-riemann\allowbreak-uplo.svg}} by Autorenkollektiv „Auswahlaxiom“ (Charlotte Dietze, Matthias Paulsen, Anne Reif) \textit{(CC BY-SA 4.0)}}\centering
\adjincludegraphics[max width=.5\textwidth, max height=0.2\textheight]{file58mfnf45riemann45uplo958027b44ca6f4383415545ea0f7b98dc31c300565}
\caption*{Die Obersumme ist nie kleiner als die Untersumme – auch wenn sich die Zerlegung bei der Ober und Untersumme unterscheidet. (\arabic{imagelabel})}
\end{figure}
Nach unserem Verfahren sollte gelten:

\begin{align*}
{\underline {\int _{a}^{b}}}f(x)\,\mathrm {d} x\leq \int _{a}^{b}f(x)\,\mathrm {d} x\leq {\overline {\int _{a}^{b}}}f(x)\,\mathrm {d} x
\end{align*}

Damit müsste gelten:

\begin{align*}
{\underline {\int _{a}^{b}}}f(x)\,\mathrm {d} x\leq {\overline {\int _{a}^{b}}}f(x)\,\mathrm {d} x
\end{align*}

Damit unser Verfahren sinnvoll ist, müssen wir obige Ungleichung beweisen. Das untere Integral soll schließlich den Flächeninhalt nach unten und das obere Integral den Flächeninhalt nach oben abschätzen. Um diese Ungleichung herzuleiten, können wir folgendermaßen vorgehen:

\begin{enumerate}
\item Wir zeigen, dass die Obersumme größer als die Untersumme bei gleicher Zerlegung ist.
\item Als nächstes beweisen wir, dass die Obersumme bei einer Verfeinerung der Zerlegung kleiner und die Untersumme bei einer Verfeinerung größer wird.
\item Nun betrachten wir beliebige Zerlegungen $\Delta _{1}$ und $\Delta _{2}$. Da wir eine gemeinsame Verfeinerung beider Zerlegungen finden, können wir mit Hilfe der ersten beiden Schritte zeigen, dass jede Obersumme größer als jede Untersumme ist.
\item Aus dem dritten Schritt können wir folgern, dass das untere Integral kleiner gleich dem oberen Integral sein muss.
\end{enumerate}

\subsection{Abschätzung zwischen Ober- und Untersummen bei gleicher Zerlegung}

\begin{theorem*}[Abschätzung zwischen Ober- und Untersummen bei gleicher Zerlegung]
Sei $f:[a,b]\to \mathbb {R} $ eine beschränkte Funktion und $\Delta =(x_{0},x_{1},\ldots ,x_{n})$ eine Zerlegung des Intervalls $[a,b]$. Dann gilt

\begin{align*}
O(\Delta ,f)\geq U(\Delta ,f)
\end{align*}

\end{theorem*}

\begin{proof*}[Abschätzung zwischen Ober- und Untersummen bei gleicher Zerlegung]
Es gilt:

\begin{align*}
&O(\Delta ,f)\\[0.3em]=\ &\sum _{k=1}^{n}(x_{k}-x_{k-1})\sup _{x\in [x_{k-1},x_{k}]}f(x)\\[0.3em]&{\color {OliveGreen}\left\downarrow \ \sup _{x\in [x_{k},x_{k+1}]}f(x)\geq \inf _{x\in [x_{k},x_{k+1}]}f(x)\right.}\\[0.3em]\geq \ &\sum _{k=1}^{n}(x_{k}-x_{k-1})\inf _{x\in [x_{k-1},x_{k}]}f(x)\\[0.3em]=\ &U(\Delta ,f)
\end{align*}

\end{proof*}

\subsection{Abschätzung von Unter- bzw. Obersummen bezüglich Verfeinerungen}

\begin{theorem*}[Abschätzung von Unter- bzw. Obersummen bezüglich Verfeinerungen]
Sei $f:[a,b]\to \mathbb {R} $ eine beschränkte Funktion und ${\tilde {\Delta }}=(y_{0},y_{1},\ldots ,y_{m})$ eine Verfeinerung der Zerlegung $\Delta =(x_{0},x_{1},\ldots ,x_{n})$ des Intervalls $[a,b]$. Dann gelten die beiden Abschätzungen:

\begin{align*}
O({\tilde {\Delta }},f)&\leq O(\Delta ,f)\\[0.3em]U({\tilde {\Delta }},f)&\geq U(\Delta ,f)
\end{align*}

\end{theorem*}

\begin{proof*}[Abschätzung von Unter- bzw. Obersummen bezüglich Verfeinerungen]
Da ${\tilde {\Delta }}$ eine Verfeinerung von $\Delta $ ist, sind alle Stützstellen von ${\tilde {\Delta }}$ auch in $\Delta $ enthalten. Für jede Stützstelle $x_{k}$ aus $\Delta $ muss es also eine Stützstelle $y_{l}$ aus ${\tilde {\Delta }}$ mit $x_{k}=y_{l}$ geben. Sei $l_{k}$ der Index für eine Stützstelle aus ${\tilde {\Delta }}$, so dass $x_{k}=y_{l_{k}}$ ist. Durch geschickte Umformungen erhalten wir:

\begin{align*}
O({\tilde {\Delta }},f)&=\sum _{i=1}^{m}(y_{i}-y_{i-1})\sup _{x\in [y_{i-1},y_{i}]}f(x)\\[0.3em]&=\sum _{k=0}^{n-1}\sum _{i=l_{k}+1}^{l_{k+1}}(y_{i}-y_{i-1})\sup _{x\in [y_{i-1},y_{i}]}f(x)\\[0.3em]&{\color {OliveGreen}\left\downarrow \ {\begin{aligned}&[y_{i-1},y_{i}]\subseteq \left[y_{l_{k}},y_{l_{k+1}}\right]=[x_{k},x_{k+1}]\\[0.3em]\implies &\sup _{x\in [y_{i-1},y_{i}]}f(x)\leq \sup _{x\in [x_{k},x_{k+1}]}f(x)\end{aligned}}\right.}\\[0.3em]&\leq \sum _{k=0}^{n-1}\sum _{i=l_{k}+1}^{l_{k+1}}(y_{i}-y_{i-1})\sup _{x\in [x_{k},x_{k+1}]}f(x)\\[0.3em]&=\sum _{k=0}^{n-1}\left(\sum _{i=l_{k}+1}^{l_{k+1}}(y_{i}-y_{i-1})\right)\cdot \sup _{x\in [x_{k},x_{k+1}]}f(x)\\[0.3em]&=\sum _{k=0}^{n-1}\left(y_{l_{k+1}}-y_{l_{k}}\right)\cdot \sup _{x\in [x_{k},x_{k+1}]}f(x)\\[0.3em]&=\sum _{k=0}^{n-1}\left(x_{k+1}-x_{k}\right)\cdot \sup _{x\in [x_{k},x_{k+1}]}f(x)\\[0.3em]&=\sum _{k=1}^{n}\left(x_{k}-x_{k-1}\right)\cdot \sup _{x\in [x_{k-1},x_{k}]}f(x)\\[0.3em]&=O(\Delta ,f)
\end{align*}

Analog kann $U({\tilde {\Delta }},f)\geq U(\Delta ,f)$ bewiesen werden.

\end{proof*}

\subsection{Abschätzung zwischen beliebigen Ober- und Untersummen}

Jede Obersumme ist mindestens so groß wie eine beliebige Untersumme:

\begin{theorem*}[Abschätzung zwischen beliebigen Ober- und Untersummen]
Sei $f:[a,b]\to \mathbb {R} $ eine beschränkte Funktion und seien $\Delta _{1}=(x_{0},x_{1},\ldots ,x_{n})$ und $\Delta _{2}=(w_{0},w_{1},\ldots ,w_{l})$ Zerlegungen des Intervalls $[a,b]$. Dann gilt

\begin{align*}
O(\Delta _{1},f)\geq U(\Delta _{2},f)
\end{align*}

\end{theorem*}

\begin{proof*}[Abschätzung zwischen beliebigen Ober- und Untersummen]
Sortieren wir die Elemente der Vereinigung $\{x_{0},x_{1},\ldots ,x_{n}\}\cup \{w_{0},w_{1},\ldots ,w_{l}\}$ aufsteigend nach ihrer Größe, so erhalten wir eine Zerlegung ${\tilde {\Delta }}=(y_{0},y_{1},\ldots ,y_{m})$. Diese erfüllt die Eigenschaft $\{x_{0},x_{1},\ldots ,x_{n}\}\cup \{w_{0},w_{1},\ldots ,w_{l}\}=\{y_{0},y_{1},\ldots ,y_{m}\}$. Damit ist ${\tilde {\Delta }}$ sowohl eine Verfeinerung von $\Delta _{1}$ als auch von $\Delta _{2}$. Nach dem Satz zu Ober- bzw. Untersummen für Verfeinerungen ist $O({\tilde {\Delta }},f)\leq O(\Delta _{1},f)$ und $U({\tilde {\Delta }},f)\geq U(\Delta _{2},f)$. Außerdem gilt $O({\tilde {\Delta }},f)\geq U({\tilde {\Delta }},f)$ nach dem Satz zur Ober- und Untersumme bei gleicher Verfeinerung. Insgesamt ergibt sich die Ungleichungskette

\begin{align*}
O(\Delta _{1},f)\geq O({\tilde {\Delta }},f)\geq U({\tilde {\Delta }},f)\geq U(\Delta _{2},f)
\end{align*}

\end{proof*}

\subsection{Abschätzung zwischen oberem und unterem Integral}

\begin{theorem*}[Abschätzung zwischen oberem und unterem Integral]
Sei $f:[a,b]\to \mathbb {R} $ eine beschränkte Funktion. Dann gilt:

\begin{align*}
{\overline {\int _{a}^{b}}}f(x)\,\mathrm {d} x\geq {\underline {\int _{a}^{b}}}f(x)\,\mathrm {d} x
\end{align*}

\end{theorem*}

\begin{proof*}[Abschätzung zwischen oberem und unterem Integral]
Es gilt:

\begin{align*}
{\overline {\int _{a}^{b}}}f(x)\,\mathrm {d} x&=\inf _{\Delta {\text{ Zerlegung}}}O(\Delta ,f)\\[0.3em]&{\color {OliveGreen}\left\downarrow \ O(\Delta _{1},f)\geq U(\Delta _{2},f){\text{ für alle Zerlegungen }}\Delta _{1},\Delta _{2}\right.}\\[0.3em]&\geq \sup _{\Delta {\text{ Zerlegung}}}U(\Delta ,f)={\underline {\int _{a}^{b}}}f(x)\,\mathrm {d} x
\end{align*}

\end{proof*}

\section{Kriterien für Riemannintegrierbarkeit}

\subsection{Epsilon-Kriterium für Riemannintegrierbarkeit}

Das Epsilon-Kriterium sagt aus, dass eine Funktion genau dann riemannintegrierbar ist, wenn der Unterschied zwischen Ober- und Untersumme beliebig klein gemacht werden kann:

\begin{theorem*}[$\epsilon $-Kriterium]
Sei $f:[a,b]\to \mathbb {R} $ eine beschränkte Funktion. Dann ist $f$ genau dann riemannintegrierbar, wenn es für alle $\epsilon >0$ eine Zerlegung ${\tilde {\Delta }}$ des Intervalls $[a,b]$ gibt, sodass gilt

\begin{align*}
O({\tilde {\Delta }},f)-U({\tilde {\Delta }},f)<\epsilon 
\end{align*}

\end{theorem*}

\begin{proof*}[$\epsilon $-Kriterium]
\proofstep{Beweisschritt:}
 Hinrichtung\begin{indentblock}
Wir beweisen zunächst, dass aus der angegebenen Bedingung die Riemannintegrierbarkeit von $f$ folgt. Sei $\epsilon >0$ und ${\tilde {\Delta }}$ eine Zerlegung mit $O({\tilde {\Delta }},f)-U({\tilde {\Delta }},f)<\epsilon $. Es gilt

\begin{align*}
U({\tilde {\Delta }},f)&\leq \sup _{\Delta {\text{ Zerlegung}}}U(\Delta ,f)={\underline {\int _{a}^{b}}}f(x)\,\mathrm {d} x\\[0.3em]O({\tilde {\Delta }},f)&\geq \inf _{\Delta {\text{ Zerlegung}}}O(\Delta ,f)={\overline {\int _{a}^{b}}}f(x)\,\mathrm {d} x
\end{align*}

Es folgt

\begin{align*}
\epsilon >O({\tilde {\Delta }},f)-U({\tilde {\Delta }},f)\geq {\overline {\int _{a}^{b}}}f(x)\,\mathrm {d} x-{\underline {\int _{a}^{b}}}f(x)\,\mathrm {d} x
\end{align*}

Nun ist ${\overline {\int _{a}^{b}}}f(x)\,\mathrm {d} x\geq {\underline {\int _{a}^{b}}}f(x)\,\mathrm {d} x$ und damit ${\overline {\int _{a}^{b}}}f(x)\,\mathrm {d} x-{\underline {\int _{a}^{b}}}f(x)\,\mathrm {d} x\geq 0$. Für jedes $\epsilon >0$ ist damit folgende Ungleichungskette erfüllt:

\begin{align*}
\epsilon >{\overline {\int _{a}^{b}}}f(x)\,\mathrm {d} x-{\underline {\int _{a}^{b}}}f(x)\,\mathrm {d} x\geq 0
\end{align*}

Damit diese Ungleichungskette für alle $\epsilon >0$ erfüllt ist, muss ${\overline {\int _{a}^{b}}}f(x)\,\mathrm {d} x={\underline {\int _{a}^{b}}}f(x)\,\mathrm {d} x$ gelten. Es ist also $f$ riemannintegrierbar.

\end{indentblock}

\proofstep{Beweisschritt:}
 Rückrichtung\begin{indentblock}
Wir setzen nun umgekehrt voraus, dass $f$ riemannintegrierbar ist. Sei ein $\epsilon >0$ vorgegeben. Wegen ${\underline {\int _{a}^{b}}}f(x)\,\mathrm {d} x=\sup _{\Delta {\text{ Zerlegung}}}U(\Delta ,f)$ existiert eine Zerlegung $\Delta _{-}$ mit $U(\Delta _{-},f)>{\underline {\int _{a}^{b}}}f(x)\,\mathrm {d} x-{\tfrac {\epsilon }{2}}$. Dies ist eine Folgerung aus der Definition des Supremums.

Analog finden wir eine Zerlegung $\Delta _{+}$ mit $O(\Delta _{+},f)<{\overline {\int _{a}^{b}}}f(x)\,\mathrm {d} x+{\tfrac {\epsilon }{2}}$. Sei nun ${\tilde {\Delta }}$ eine gemeinsame Verfeinerung von $\Delta _{-}$ und $\Delta _{+}$. Wir wissen, dass $O({\tilde {\Delta }},f)\leq O(\Delta _{+},f)$ und $U({\tilde {\Delta }},f)\geq U(\Delta _{-},f)$ gilt. Also ist

\begin{align*}
&O({\tilde {\Delta }},f)-U({\tilde {\Delta }},f)\\[0.3em]\leq \ &O(\Delta _{+},f)-U(\Delta _{-},f)\\[0.3em]<\ &\left({\overline {\int _{a}^{b}}}f(x)\,\mathrm {d} x+{\frac {\epsilon }{2}}\right)-\left({\underline {\int _{a}^{b}}}f(x)\,\mathrm {d} x-{\frac {\epsilon }{2}}\right)\\[0.3em]=\ &{\overline {\int _{a}^{b}}}f(x)\,\mathrm {d} x-{\underline {\int _{a}^{b}}}f(x)\,\mathrm {d} x+\epsilon \\[0.3em]&{\color {OliveGreen}\left\downarrow \ {\overline {\int _{a}^{b}}}f(x)\,\mathrm {d} x={\underline {\int _{a}^{b}}}f(x)\,\mathrm {d} x{\text{, da }}f{\text{ riemannintegrierbar ist}}\right.}\\[0.3em]=\ &\epsilon 
\end{align*}

\end{indentblock}

\end{proof*}

\chapter{Eigenschaften des Riemannintegrals}

\section{Übersicht: Eigenschaften des Riemannintegrals}

\begin{itemize}
\item Stetige Funktionen sind riemannintegrierbar.
\item \emph{Monotonie:} Aus $f(x)\leq g(x)$ für alle $x\in [a,b]$ folgt $\int _{a}^{b}f(x)\mathrm {d} x\leq \int _{a}^{b}g(x)\mathrm {d} x$.
\item \emph{Summenregel:} Wenn $f$ und $g$ riemannintegrierbar sind, dann sind auch $f+g$ riemannintegrierbar und es gilt $\int _{a}^{b}\left(f(x)+g(x)\right)\mathrm {d} x=\int _{a}^{b}f(x)\mathrm {d} x+\int _{a}^{b}g(x)\mathrm {d} x$.
\item \emph{Faktorregel:} Wenn $f$ riemannintegrierbar ist, dann ist es auch die Funktion $\lambda \cdot f$ mit $\lambda \in \mathbb {R} $ und es gilt $\int _{a}^{b}\lambda f(x)\mathrm {d} x=\lambda \int _{a}^{b}f(x)\mathrm {d} x$.
\item \emph{Additivität der Grenzen:} Seien $a,b,c\in \mathbb {R} $ mit $a\leq c\leq b$ und sei $f:[a,b]\to \mathbb {R} $ eine Funktion. Dann ist $f$ genau dann riemannintegrierbar auf dem Intervall $[a,b]$, wenn $f$ auf den Intervallen $[a,c]$ und $[c,b]$ jeweils riemannintegrierbar ist. In diesem Fall gilt $\int _{a}^{b}f(x)\mathrm {d} x=\int _{a}^{c}f(x)\mathrm {d} x+\int _{c}^{b}f(x)\mathrm {d} x$.
\item \emph{Dreiecksungleichung:} Sei $f:[a,b]\to \mathbb {R} $ eine riemannintegrierbare Funktion, wobei $a$ und $b$ reelle Zahlen mit $a\leq b$ sind. Dann ist die Funktion $|f|:[a,b]\to \mathbb {R} ,x\mapsto |f(x)|$ riemannintegrierbar und es gilt $\left|\int _{a}^{b}f(x)\mathrm {d} x\right|\leq \int _{a}^{b}|f(x)|\mathrm {d} x$.
\item \emph{Produktregel:} Seien $f:[a,b]\to \mathbb {R} $ und $g:[a,b]\to \mathbb {R} $ zwei riemannintegrierbare Funktionen, wobei $a$ und $b$ reelle Zahlen mit $a\leq b$ sind. Dann ist die Funktion $(f\cdot g):[a,b]\to \mathbb {R} ,x\mapsto f(x)\cdot g(x)$ riemannintegrierbar.
\item Monotone Funktionen sind riemannintegrierbar.
\item Wenn sich eine Funktion von einer riemannintegrierbaren Funktion nur an endlich vielen Stellen unterscheidet, dann ist auch sie riemannintegrierbar und ihr Integral ist gleich dem Integral der anderen Funktion.
\end{itemize}

\chapter{Mittelwertsatz für Integrale}

Nach dem Mittelwertsatz für Integrale nehmen stetige Funktionen auf einem kompakten Intervall ihren durchschnittlichen Wert an. Dieser Satz kann unter anderem zum Beweis des \href{https://de.wikibooks.org/wiki/Mathe\_für\_Nicht-Freaks:\_Hauptsatz\_der\_Differential-\_und\_Integralrechnung}
{Hauptsatzes der Differential- und Integralrechnung} verwendet werden. Dieser stellt einen Zusammenhang zwischen Integral und Ableitung her.

\section{Das Integral als Durchschnittswert}

\subsection{Wiederholung: Durchschnitt und gewichteter Durchschnitt}

Mit dem Integral kann der Durchschnittswert einer Funktion bestimmt werden. Bei $n$ verschiedenen Werten $y_{1}$ bis $y_{n}$ kann ihr Durchschnitt bzw. der Mittelwert ${\overline {y}}$ bestimmt werden über

\begin{align*}
{\overline {y}}={\frac {y_{1}+y_{2}+\ldots +y_{n}}{n}}
\end{align*}

So ist der Durchschnitt der Werte $(1,1,2,3)$ gleich ${\tfrac {1}{4}}(1+1+2+3)={\tfrac {7}{4}}$. Wenn die einzelnen Werte $y_{1}$ bis $y_{n}$ in der Berechnung des Durchschnitts durch unterschiedliche Faktoren $w_{1}$ bis $w_{n}$ gewichtet werden sollen, lautet die Formel:

\begin{align*}
{\overline {y}}={\frac {w_{1}\cdot y_{1}+w_{2}\cdot y_{2}+\ldots +w_{n}\cdot y_{n}}{w_{1}+w_{2}+\ldots +w_{n}}}
\end{align*}

Eine Gewichtung $w_{i}=2$ bedeutet beispielsweise, dass der Wert $y_{i}$ doppelt so stark in den Durchschnitt eingehen soll, als wenn $w_{i}=1$ wäre.

\subsection{Durchschnittsberechnung einer Funktion}

Eine Funktion $f:[a,b]\to \mathbb {R} $ hat unendlich viele Argumente und nimmt damit unendlich oft Funktionswerte an. Die Formel zur Mittelwertsberechnung von endlich vielen Werten kann also nicht verwendet werden, um den durchschnittlichen Funktionswert von $f$ zu bestimmen. Wir können diesen aber annähern. Hierzu zerlegen wir das Intervall $[a,b]$ in Teilintervalle. Durch die Wahl von Stützstellen $(x_{0},x_{1},\ldots ,x_{n})$ mit $x_{0}=a$ und $x_{n}=b$ wird das Intervall $[a,b]$ in $n$ Intervalle $[x_{k-1},x_{k}]$ mit $1\leq k\leq n$ unterteilt:

\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Mfnf-riemann-zerl.svg}{\textbf{Mfnf\allowbreak-riemann\allowbreak-zerl.svg}} by Autorenkollektiv „Auswahlaxiom“ (Charlotte Dietze, Matthias Paulsen, Anne Reif) \textit{(CC BY-SA 4.0)}}\begin{center}
\adjincludegraphics[max width=0.5\textwidth,max height=0.2\textheight]{file58mfnf45riemann45zerl95404689a14779fbfd3d78e1f722fc9733d2116aba}\end{center}

Sei nun $M_{k,n}$ das Supremum und $m_{k,n}$ das Infimum der Funktionswerte von $f$ im Teilintervall $[x_{k-1},x_{k}]$. Damit das Supremum und Infimum existiert, nehmen wir zusätzlich an, dass $f$ beschränkt ist. Nun können zwei Treppenfunktionen definiert werden, die jeweils die Funktion $f$ von oben bzw. von unten annähern. Bei der oberen Treppenfunktion $T:[a,b]\to \mathbb {R} $ definieren wir $T(x)=M_{k,n}$ für $x\in [x_{k-1},x_{k})$ und $T(x_{n})=f(x_{n})$. Bei der unteren Treppenfunktion $t:[a,b]\to \mathbb {R} $ ist $t(x)=m_{k,n}$ bei $x\in [x_{k-1},x_{k})$ und $t(x_{n})=f(x_{n})$:

\begin{tabularx}{\linewidth}{XX}
\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Mfnf-riemann-lower.svg}{\textbf{Mfnf\allowbreak-riemann\allowbreak-lower.svg}} by Autorenkollektiv „Auswahlaxiom“ (Charlotte Dietze, Matthias Paulsen, Anne Reif) \textit{(CC BY-SA 4.0)}}\begin{minipage}[t]{\linewidth}
\begin{figure}[H]
\begin{minipage}[t][0.2\textheight][c]{\linewidth}
\centering
\adjincludegraphics[max width=1.\linewidth, max height=0.2\textheight]{file58mfnf45riemann45lower95c4592814da133f76fbae9983b253e22315e24385}
\end{minipage}
\caption*{Treppenfunktion $t$, die die Funktion von unten approximiert. (\arabic{imagelabel})}
\end{figure}

\end{minipage}
&
\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Mfnf-riemann-upper.svg}{\textbf{Mfnf\allowbreak-riemann\allowbreak-upper.svg}} by Autorenkollektiv „Auswahlaxiom“ (Charlotte Dietze, Matthias Paulsen, Anne Reif) \textit{(CC BY-SA 4.0)}}\begin{minipage}[t]{\linewidth}
\begin{figure}[H]
\begin{minipage}[t][0.2\textheight][c]{\linewidth}
\centering
\adjincludegraphics[max width=1.\linewidth, max height=0.2\textheight]{file58mfnf45riemann45upper957c1a35631411e7d3a729b68b9be0f9cbb37e7ad7}
\end{minipage}
\caption*{Treppenfunktion $T$, die die Funktion von oben approximiert. (\arabic{imagelabel})}
\end{figure}

\end{minipage}
\end{tabularx}

Beide Treppenfunktionen nehmen nur endlich viele Werte an und nähern beide den Funktionsverlauf von $f$ an. Da die Werte von $T$ immer über den Werten von $f$ liegen, sollte auch der durchschnittliche Wert von $T$ größer gleich dem Mittelwert von $f$ sein. Der Durchschnittswert der oberen Treppenfunktion schätzt also den gesuchten Funktionsmittelwert nach oben ab. Analog ist der Durchschnittswert der unteren Treppenfunktion eine Abschätzung nach unten für den Mittelwert von $f$.

Um den Mittelwert einer Treppenfunktion zu bestimmen, reicht es nicht aus, den Durchschnitt der angenommenen Funktionswerte zu bilden. So nehmen die folgenden Treppenfunktionen dieselben Funktionswerte an. Wegen der unterschiedlichen Größe der Teilintervalle sollte sich aber der Durchschnittswert der beiden Teilintervalle unterscheiden:

\begin{tabularx}{\linewidth}{XX}
\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Step function with three values (version 1).svg}{\textbf{Step function with three values (version 1).svg}} by Stephan Kulla \textit{(CC0)}}\begin{minipage}[t]{\linewidth}
\begin{figure}[H]
\begin{minipage}[t][0.1\textheight][c]{\linewidth}
\centering
\adjincludegraphics[max width=1.\linewidth, max height=0.1\textheight]{file58step32function32with32three32values3240version32141956257148effb24f66611736a16582d619148ec895}
\end{minipage}
\caption*{Treppenfunktion mit den Werten $0$, $1$ und $2$. Der durchschnittliche Wert sollte aber größer als ${\tfrac {1}{3}}(0+1+2)=1$ sein. (\arabic{imagelabel})}
\end{figure}

\end{minipage}
&
\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Step function with three values (version 2).svg}{\textbf{Step function with three values (version 2).svg}} by Stephan Kulla \textit{(CC0)}}\begin{minipage}[t]{\linewidth}
\begin{figure}[H]
\begin{minipage}[t][0.1\textheight][c]{\linewidth}
\centering
\adjincludegraphics[max width=1.\linewidth, max height=0.1\textheight]{file58step32function32with32three32values3240version32241958d2e19f127b92fccf8f2fcf9f0e475fdaf4fc6c8}
\end{minipage}
\caption*{Auch diese Treppenfunktion nimmt nur die Werte $0$, $1$ und $2$ an. Ihr durchschnittlicher Wert sollte aber geringer als $1$ sein. (\arabic{imagelabel})}
\end{figure}

\end{minipage}
\end{tabularx}

Vielmehr müssen wir die Funktionswerte mit den Längen der Teilintervalle an der Stelle gewichten, wo diese Funktionswerte angenommen werden. Das $k$-te Teilintervall $[x_{k-1},x_{k}]$ hat die Länge $x_{k}-x_{k-1}$. Bei der oberen Treppenfunktion $T$ bilden wir also den Mittelwert ${\overline {T}}$ der Zahlen $M_{k,n}$ mit den Gewichten $x_{k}-x_{k-1}$:

\begin{align*}
{\overline {T}}&={\frac {M_{1,n}\cdot (x_{1}-x_{0})+M_{2,n}\cdot (x_{2}-x_{1})+\ldots +M_{n,n}\cdot (x_{n}-x_{n-1})}{(x_{1}-x_{0})+(x_{2}-x_{1})+\ldots +(x_{n}-x_{n-1})}}\\[0.5em]&={\frac {\sum _{k=1}^{n}M_{k,n}\cdot (x_{k}-x_{k-1})}{x_{n}-x_{0}}}={\frac {1}{b-a}}\cdot \sum _{k=1}^{n}M_{k,n}\cdot (x_{k}-x_{k-1})
\end{align*}

Analog können wir den Durchschnittswert ${\overline {t}}$ der unteren Treppenfunktion bestimmen. Insgesamt erhalten wir die Abschätzung:

\begin{align*}
{\overline {t}}={\frac {1}{b-a}}\cdot \underbrace {\sum _{k=1}^{n}m_{k,n}\cdot (x_{k}-x_{k-1})} _{\text{Untersumme}}\leq {\overline {f}}\leq {\frac {1}{b-a}}\cdot \underbrace {\sum _{k=1}^{n}M_{k,n}\cdot (x_{k}-x_{k-1})} _{\text{Obersumme}}={\overline {T}}
\end{align*}

Wir konnten also den durchschnittlichen Funktionswert ${\overline {f}}$ der gegebenen Funktion $f$ abschätzen. Als Summen treten dabei die Ober- und Untersummen auf, die selbst den orientierten Flächeninhalt unter dem Graphen approximieren. Um die obige Abschätzung weiter zu verbessern, müssen wir das Grundintervall immer feiner zerlegen. Unter der Voraussetzung, dass die Funktion $f$ riemannintegrierbar ist, strebt dabei die Unter- sowie die Obersumme gegen das Integral $\int _{a}^{b}f(x)\,\mathrm {d} x$. Mit Hilfe des Sandwichsatzes können wir aus obiger Abschätzung folgern:

\begin{align*}
{\overline {f}}={\frac {1}{b-a}}\int _{a}^{b}f(x)\,\mathrm {d} x
\end{align*}

Mit Hilfe des Integrals kann also der Durchschnittswert einer Funktion bestimmt werden. Hierzu muss das Integral durch die Länge $b-a$ des Grundintervalls geteilt werden.

\subsection{Geometrische Herleitung}

Der Zusammenhang zwischen Integral und Mittelwert einer Funktion $f:[a,b]\to \mathbb {R} $ kann geometrisch hergeleitet werden. Betrachten wir hierzu eine integrierbare Funktion $f:[a,b]\to \mathbb {R} $. Das Integral $\int _{a}^{b}f(x)\,\mathrm {d} x$ entspricht dem orientiertem Flächeninhalt zwischen dem Graphen von $f$ und der $x$-Achse:

\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:IntegralArea.svg}{\textbf{IntegralArea.svg}} by Autorenkollektiv „Auswahlaxiom“ (Charlotte Dietze, Matthias Paulsen, Anne Reif) \textit{(CC BY-SA 4.0)}}\begin{center}
\adjincludegraphics[max width=0.5\textwidth,max height=0.2\textheight]{file58integralarea958c1c156dde03f999712ebb796ca0dc6da4330890}\end{center}

Wenn wir die Funktion so verändern, dass sie nur den durchschnittlichen Funktionswert annimmt, dann sollte sich ihr Flächeninhalt unter dem Graphen nicht ändern. Wir können den Durchschnittswert ${\overline {f}}$ der Funktion $f$ also darüber definieren, dass der Flächeninhalt des Rechtecks mit der Grundseite $[a,b]$ auf der $x$-Achse und der Höhe ${\overline {f}}$ gleich dem orientierten Flächeninhalt unter dem Graphen von $f$ ist:

\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:Illustration_for_the_mean_value_of_a_function.svg}{\textbf{Illustration\allowbreak\_for\allowbreak\_the\allowbreak\_mean\allowbreak\_value\allowbreak\_of\allowbreak\_a\allowbreak\_function.svg}} by Stephan Kulla \textit{(CC BY-SA 4.0)}}\begin{center}
\adjincludegraphics[max width=0.4\textwidth,max height=0.2\textheight]{file58illustration95for95the95mean95value95of95a95function95e01084016839bdfeb7c71147ba0c755632b2395c}\end{center}

Damit erhalten wir die Gleichung ${\overline {f}}\cdot (b-a)=\int _{a}^{b}f(x)\,\mathrm {d} x$. Diese können wir umformen zu:

\begin{align*}
{\overline {f}}={\frac {1}{b-a}}\int _{a}^{b}f(x)\,\mathrm {d} x
\end{align*}

\section{Der Mittelwertsatz}

\subsection{Bedeutung}

Bei einer stetigen Funktion $f:[a,b]\to \mathbb {R} $ liegt der Durchschnittswert im Bereich der Werte, welche die Funktion annimmt. Es gibt also ein $\xi $, so dass $f(\xi )$ gleich dem durchschnittlichen Funktionswert von $f$ ist. Das Rechteck mit der Breite $b-a$ und der Höhe $f(\xi )$ besitzt dann denselben Flächeninhalt wie die Funktion $f$ unter ihrem Graphen:

\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:MVT.svg}{\textbf{MVT.svg}} by Autorenkollektiv „Auswahlaxiom“ (Charlotte Dietze, Matthias Paulsen, Anne Reif) \textit{(CC BY-SA 4.0)}}\begin{center}
\adjincludegraphics[max width=0.4\textwidth,max height=0.2\textheight]{file58mvt953abd82ae65d7310dee0f59ccddb67cbdcd549e30}\end{center}

Dies ist bereits die Aussage des Mittelwertsatzes: Eine stetige Funktion nimmt ihren Mittelwert als Funktionswert an. Es gibt also für alle stetigen Funktionen $f:[a,b]\to \mathbb {R} $ mindestens ein Argument $\xi \in [a,b]$ mit:

\begin{align*}
f(\xi )={\frac {1}{b-a}}\int _{a}^{b}f(x)\,\mathrm {d} x
\end{align*}

Um auch den Fall $a=b$ zu erlauben, stellen wir obige Gleichung um und erhalten so ein $\xi \in [a,b]$ mit:

\begin{align*}
f(\xi )\cdot (b-a)=\int _{a}^{b}f(x)\,\mathrm {d} x
\end{align*}

\subsection{Notwendigkeit der Stetigkeitsvoraussetzung}

Dass der Mittelwertsatz für Integrale nicht für beliebige Funktionen gilt und dass die Stetigkeit als Voraussetzung wichtig ist, zeigt die Funktion $g:[0,2]\to \mathbb {R} $ mit

\begin{align*}
g(x)={\begin{cases}0&{\text{falls }}0\leq x<1\\1&{\text{falls }}1\leq x\leq 2\end{cases}}
\end{align*}

Diese Funktion ist riemannintegrierbar, weil sie aus zwei konstanten Funktionen zusammengesetzt ist. Es gilt

\begin{align*}
\int _{0}^{2}g(x)\,\mathrm {d} x=\int _{0}^{1}g(x)\,\mathrm {d} x+\int _{1}^{2}g(x)\,\mathrm {d} x=0+1=1
\end{align*}

Der durchschnittliche Funktionswert beträgt also

\begin{align*}
{\frac {1}{2-0}}\int _{0}^{2}g(x)\,\mathrm {d} x={\frac {1}{2}}
\end{align*}

Nach Betrachtung des Funktionsverlaufs macht dies auch Sinn. Allerdings wird der Wert ${\tfrac {1}{2}}$ von $g$ nicht angenommen, da $0$ und $1$ die einzigen Funktionswerte sind. Bei unstetigen Funktion wie $g$ ($g$ ist an der Stelle $1$ unstetig) ist der Mittelwertsatz nicht unbedingt erfüllt.
\clearpage
\subsection{Satz und Beweis}

\begin{theorem*}[Mittelwertsatz für Integrale]
Sei $f:[a,b]\to \mathbb {R} $ eine stetige Funktion. Dann gibt es ein $\xi \in [a,b]$ mit

\begin{align*}
f(\xi )\cdot (b-a)=\int _{a}^{b}f(x)\,\mathrm {d} x
\end{align*}

\end{theorem*}

\begin{solutionprocess*}[Mittelwertsatz für Integrale]
Sei ${\overline {f}}$ der durchschnittliche Funktionswert von $f$. Im Fall $a<b$ müssen wir zeigen, dass es ein $\xi \in [a,b]$ mit $f(\xi )={\overline {f}}$ gibt. Die Idee ist nun, den \href{https://de.wikibooks.org/wiki/Mathe\_für\_Nicht-Freaks:\_Zwischenwertsatz}
{Zwischenwertsatz} anzuwenden. Wenn wir zeigen können, dass der durchschnittliche Funktionswert zwischen dem Minimum $m$ und dem Maximum $M$ der Funktion liegt, so muss dieser aufgrund der Stetigkeit von $f$ selbst als Funktionswert angenommen werden:

\stepcounter{imagelabel}
\addxcontentsline{lof}{section}[]{Abb. \arabic{imagelabel}: \protect\href{https://commons.wikimedia.org/wiki/File:MVTProof.svg}{\textbf{MVTProof.svg}} by Autorenkollektiv „Auswahlaxiom“ (Charlotte Dietze, Matthias Paulsen, Anne Reif) \textit{(CC BY-SA 4.0)}}\begin{center}
\adjincludegraphics[max width=0.5\textwidth,max height=0.2\textheight]{file58mvtproof9586dc3208ccafc10256d04a08ef5c7b41f5913f05}\end{center}

Um $m\leq {\overline {f}}\leq M$ zu beweisen, schätzen wir das Integral $\int _{a}^{b}f(x)\,\mathrm {d} x$ nach unten durch das konstante Integral $\int _{a}^{b}m\,\mathrm {d} x=(b-a)\cdot m$ und nach oben durch das konstante Integral $\int _{a}^{b}M\,\mathrm {d} x=(b-a)\cdot M$ ab. Diese Abschätzung können wir wegen $m\leq f(x)\leq M$ vornehmen:

\begin{align*}
(b-a)\cdot m\leq \int _{a}^{b}f(x)\,\mathrm {d} x\leq (b-a)\cdot M
\end{align*}

Division durch $b-a$ liefert dann die gewünschte Ungleichung:

\begin{align*}
m\leq \underbrace {{\frac {1}{b-a}}\int _{a}^{b}f(x)\,\mathrm {d} x} _{={\overline {f}}}\leq M
\end{align*}

Für den formalen Beweis müssen wir unsere Argumente noch in eine logisch korrekte Reihenfolge bringen. Im Wesentlichen müssen wir hierzu die Reihenfolge der Argumente umkehren. Außerdem müssen wir den Fall $a=b$ beachten, bei dem wir nicht durch $b-a$ teilen dürfen. Im Fall $a=b$ sind beide Seiten der Gleichung stets Null und damit ist die Gleichung für $\xi =a=b$ trivialerweise erfüllt.

\end{solutionprocess*}

\begin{proof*}[Mittelwertsatz für Integrale]
Falls $a=b$ gilt, wählen wir (gezwungenermaßen) $\xi =a=b$ und es gilt

\begin{align*}
\int _{a}^{b}f(x)\,\mathrm {d} x=0=(b-a)\cdot f(\xi )
\end{align*}

Im Folgenden sei nun $a<b$. Nach dem \href{https://de.wikibooks.org/wiki/Mathe\_für\_Nicht-Freaks:\_Satz\_vom\_Minimum\_und\_Maximum}
{Satz vom Minimum und Maximum} nimmt die stetige Funktion $f$ auf dem kompakten Intervall $[a,b]$ ihr Minimum $m\in \mathbb {R} $ und ihr Maximum $M\in \mathbb {R} $ an. Aufgrund der Monotonie des Integrals gilt

\begin{align*}
\int _{a}^{b}f(x)\,\mathrm {d} x&\geq \int _{a}^{b}m\,\mathrm {d} x=(b-a)\cdot m\\\int _{a}^{b}f(x)\,\mathrm {d} x&\leq \int _{a}^{b}M\,\mathrm {d} x=(b-a)\cdot M
\end{align*}

Wir erhalten also

\begin{align*}
m\leq {\frac {1}{b-a}}\int _{a}^{b}f(x)\,\mathrm {d} x\leq M
\end{align*}

Somit gibt es nach dem \href{https://de.wikibooks.org/wiki/Mathe\_für\_Nicht-Freaks:\_Zwischenwertsatz}
{Zwischenwertsatz} ein $\xi \in [a,b]$ mit

\begin{align*}
&&f(\xi )={\frac {1}{b-a}}\int _{a}^{b}f(x)\,\mathrm {d} x\\[0.5em]&\implies &(b-a)\cdot f(\xi )=\int _{a}^{b}f(x)\,\mathrm {d} x
\end{align*}

\end{proof*}

\chapter{Hauptsatz der Differential- und Integralrechnung}

Der \emph{Hauptsatz der Differential- und Integralrechnung} (kurz HDI) ist einer der bedeutendsten Sätze der Analysis. Nach ihm kann über das Integral die Gesamtänderung einer Funktion bestimmt werden, wenn ihre Ableitung überall bekannt ist. So kann beispielsweise die Veränderung eines Systems ausgerechnet werden, wenn man zu jedem Zeitpunkt die momentane Änderungsrate (also die Ableitung) kennt.

Der Hauptsatz der Differential- und Integralrechnung stellt so eine Beziehung zwischen der Ableitung und dem Integral her und zeigt, dass sich Ableitung und Integration in gewisser Weise umkehren. Dies kann beispielsweise ausgenutzt werden, um Integrale leichter auszurechnen. Dabei werden zwei Versionen des Hauptsatzes unterschieden: Die eine Version trifft eine Aussage darüber, was das Integral der Ableitungsfunktion ist und die andere beschreibt, was die Ableitung der sogenannten Integralfunktion ist.

Häufig wird die Definition des Integrals aus der Grundvorstellung hergeleitet, dass es die orientierte Fläche zwischen dem Graphen und der $x$-Achse wiedergibt. Der Hauptsatz der Differential- und Integralrechnung zeigt, dass diese orientierte Fläche unter dem Graphen einer Ableitung als Funktionsänderung der ursprünglichen Funktion interpretiert werden kann.

\section{Erste Variante des Hauptsatzes}

\subsection{Version: Integral der Ableitungsfunktion}

Eine Variante des Hauptsatzes kann so formuliert werden:

\begin{importantparagraph*}
Das Integral $\int _{a}^{b}f(x)\,\mathrm {d} x$ entspricht der Gesamtänderung einer Funktion, die an jeder Stelle $x\in [a,b]$ die momentane Änderungsrate $f(x)$ besitzt.

\end{importantparagraph*}

Sei $F:[a,b]\to \mathbb {R} $ eine solche Funktion, die an jeder Stelle $x\in [a,b]$ die momentane Änderungsrate $f(x)$ besitzt. Da die Ableitung die momentane Änderungsrate einer Funktion beschreibt, ist also $F'(x)=f(x)$ für alle $x\in [a,b]$. Eine solche Funktion $F$ wird \emph{Stammfunktion von $f$} genannt. Die Gesamtänderung der Funktion $F$ im Intervall $[a,b]$ entspricht der Differenz $F(b)-F(a)$. Es muss also gelten:

\begin{align*}
\int _{a}^{b}f(x)\,\mathrm {d} x=F(b)-F(a)
\end{align*}

Dies ist die erste Version des Hauptsatzes. Da wir im Beweis auf den Mittelwertsatz der Integralrechnung zurückgreifen, werden wir die Stetigkeit von $f$ zusätzlich voraussetzen:

\begin{theorem*}[Haupsatz der Differential- und Integralrechnung: Integral der Ableitungsfunktion]
Sei $f\colon [a,b]\to \mathbb {R} $ eine stetige Funktion. Für jede Stammfunktion $F\colon [a,b]\to \mathbb {R} $ gilt:

\begin{align*}
\int _{a}^{b}f(x)\,\mathrm {d} x=F(b)-F(a)
\end{align*}

\end{theorem*}

\subsection{Stammfunktion}

In der ersten Variante des Hauptsatzes ist die Rede von einer Funktion $F$, deren Ableitungsfunktion gleich $f$ ist. Eine solche Funktion wird Stammfunktion von $f$ genannt:

\begin{definition*}[Stammfunktion]
Eine differenzierbare Funktion $F:D\to \mathbb {R} $ heißt \emph{Stammfunktion} einer gegebenen Funktion $f:D\to \mathbb {R} $, wenn $f$ die Ableitung von $F$ ist. Es muss also $F'(x)=f(x)$ für alle $x\in D$ gelten.

\end{definition*}

Salopp gesprochen ist eine Stammfunktion das „Gegenteil“ der Ableitungsfunktion. Sie ist jedoch im Gegensatz zur Ableitungsfunktion nicht eindeutig. Betrachte die Funktion $f:\mathbb {R} \to \mathbb {R} $ mit $f(x)=3x^{2}$. Eine mögliche Stammfunktion ist die Funktion $F:\mathbb {R} \to \mathbb {R} $ mit $F(x)=x^{3}+4$. Denn es gilt nach den Ableitungsregeln $F'(x)=3x^{2}$. Es fällt auf, dass wir anstelle der Konstanten $4$ auch eine andere hätten wählen können, da diese bei der Ableitung verschwindet. Tatsächlich ist jede Funktion der Form $F(x)=x^{3}+C$ mit einer beliebigen Konstanten $C\in \mathbb {R} $ eine Stammfunktion von $f$.

\subsection{Differenz von Stammfunktionen}

Hat also eine Funktion $f$ eine Stammfunktion $F$, so hat sie auch unendlich viele weitere Stammfunktionen, nämlich alle Funktionen ${\tilde {F}}(x)=F(x)+C$ mit einer beliebigen Konstante $C\in \mathbb {R} $. Das liegt daran, dass eine (additive) Konstante beim Ableiten wegfällt. Wir können sogar zeigen, dass man auf diese Weise alle Stammfunktionen von $f$ erhält:

\begin{theorem*}
Seien $F$ und ${\tilde {F}}$ Stammfunktionen der gleichen Funktion $f$. Dann unterscheiden sich $F$ und ${\tilde {F}}$ nur um eine additive Konstante, d.h. es existiert eine reelle Zahl $C$ mit ${\tilde {F}}(x)=F(x)+C$ für alle $x\in \mathbb {R} $.

\end{theorem*}

\begin{proof*}
Wir betrachten die Differenz $G(x):={\tilde {F}}(x)-F(x)$ der beiden Stammfunktionen. Für diese gilt nach der Differenzregel

\begin{align*}
G'(x)={\tilde {F}}'(x)-F'(x)=f(x)-f(x)=0
\end{align*}

für alle $x\in \mathbb {R} $. Wir wollen zeigen, dass die Funktion $G$ konstant ist, denn aus $G(x)=C$ für alle $x\in \mathbb {R} $ folgt, dass ${\tilde {F}}(x)=F(x)+C$ für alle $x\in \mathbb {R} $.

Angenommen, $G$ ist nicht konstant. Dann gibt es $a\neq b$ mit $G(a)\neq G(b)$. Nach dem \href{https://de.wikibooks.org/wiki/Mathe\_für\_Nicht-Freaks:\_Mittelwertsatz}
{Mittelwertsatz} gibt es ein $\xi \in \mathbb {R} $ mit

\begin{align*}
G'(\xi )={\frac {G(b)-G(a)}{b-a}}
\end{align*}

Wegen $G(a)\neq G(b)$ gilt einerseits $G'(\xi )\neq 0$. Andererseits haben wir uns oben bereits überlegt, dass $G'(x)=0$ für alle $x\in \mathbb {R} $ gilt, also auch $G'(\xi )=0$. Das ist ein Widerspruch und somit muss $G$ konstant sein.

\end{proof*}

\section{Anwendung: (Re-)konstruktion der Stammfunktion}

Über das Integral kann aus der Ableitung die Gesamtänderung einer Funktion berechnet werden. Damit können wir das Integral benutzen, um aus einer bekannten Ableitung die ursprüngliche Funktion zu rekonstruieren bzw. eine gesuchte Funktion zu bestimmen, deren Ableitung bekannt ist. Da wir nur Änderungen einer Funktion bestimmen können, brauchen wir noch einen Anfangswert, den die Funktion an einer festgelegten Stelle haben soll.

Nehmen wir an, dass wir eine Funktion $F:\mathbb {R} \to \mathbb {R} $ bestimmen wollen. Ihre Ableitung sei die für uns bekannte Funktion $f:\mathbb {R} \to \mathbb {R} $. Außerdem wissen wir, dass an der Stelle $a\in \mathbb {R} $ die Funktion $F$ den Wert $y_{a}\in \mathbb {R} $ besitzt. Es gilt also $F(a)=y_{a}$. Aus diesen beiden Informationen können wir mit Hilfe des Integrals die Funktion $F$ (re-)konstruieren:

\begin{align*}
F(x)&={\color {OliveGreen}\underbrace {F(a)} _{\text{Anfangswert}}}+{\color {NavyBlue}\underbrace {F(x)-F(a)} _{{\text{Änderung zwischen }}a{\text{ und }}x}}\\[0.3em]&={\color {OliveGreen}y_{a}}+{\color {NavyBlue}\int _{a}^{x}f(t)\,\mathrm {d} t}
\end{align*}

Mit dieser Formel kann der Wert einer Funktion bestimmt werden, wenn man deren Ableitung und einen Anfangswert kennt.

\section{Zweite Variante des Hauptsatzes}

\subsection{Version: Ableitung der Integralfunktion}

In der Herleitung der Formel $F(x)=y_{a}+\int _{a}^{x}f(t)\,\mathrm {d} t$ haben wir angenommen, dass $f$ die Ableitung von $F$ ist. Können wir umgekehrt die Gleichung $F'(x)=f(x)$ zeigen, wenn wir $F$ über $F(x)=y_{a}+\int _{a}^{x}f(t)\,\mathrm {d} t$ definieren? Hierzu müsste gelten:

\begin{align*}
f(x)&=F'(x)={\frac {\mathrm {d} }{\mathrm {d} x}}\left(y_{a}+\int _{a}^{x}f(t)\,\mathrm {d} t\right)\\[0.5em]&=\underbrace {{\frac {\mathrm {d} }{\mathrm {d} x}}y_{a}} _{=\ 0}+{\frac {\mathrm {d} }{\mathrm {d} x}}\left(\int _{a}^{x}f(t)\,\mathrm {d} t\right)={\frac {\mathrm {d} }{\mathrm {d} x}}\left(\int _{a}^{x}f(t)\,\mathrm {d} t\right)
\end{align*}

Damit unsere Vermutung stimmt, müssen wir $f(x)={\frac {\mathrm {d} }{\mathrm {d} x}}\left(\int _{a}^{x}f(t)\,\mathrm {d} t\right)$ beweisen. Sprich: Für eine Funktion $H:\mathbb {R} \to \mathbb {R} $ mit $H(x)=\int _{a}^{x}f(t)\,\mathrm {d} t$ muss $H'(x)=f(x)$ gelten. Eine solche Funktion $H$ werden wir \emph{Integralfunktion} nennen. Auch bei dieser neuen Version des Hauptsatzes werden wir voraussetzen, dass die Funktion $f$ stetig ist:

\begin{theorem*}[Hauptsatz der Differential- und Integralrechnung: Ableitung der Integralfunktion]
Sei $f:[a,b]\to \mathbb {R} $ stetig. Dann ist die Integralfunktion $F:[a,b]\to \mathbb {R} $ mit $F(x)=\int _{a}^{x}f(t)\,\mathrm {d} t$ eine Stammfunktion von $f$. Es gilt also $F'({\tilde {x}})=f({\tilde {x}})$ für alle ${\tilde {x}}\in [a,b]$.

\end{theorem*}
\clearpage
\section{Beweis}

Wir werden erst die zweite Variante des Hauptsatzes beweisen und aus dieser dann die erste Variante herleiten.

\subsection{Variante: Ableitung der Integralfunktion}

\begin{theorem*}[Hauptsatz der Differential- und Integralrechnung: Ableitung der Integralfunktion]
Sei $f:[a,b]\to \mathbb {R} $ stetig. Dann ist die Integralfunktion $F:[a,b]\to \mathbb {R} $ mit $F(x)=\int _{a}^{x}f(t)\,\mathrm {d} t$ eine Stammfunktion von $f$. Es gilt also $F'({\tilde {x}})=f({\tilde {x}})$ für alle ${\tilde {x}}\in [a,b]$.

\end{theorem*}

\begin{proof*}[Hauptsatz der Differential- und Integralrechnung: Ableitung der Integralfunktion]
Sei $f:[a,b]\to \mathbb {R} $ stetig und $F:[a,b]\to \mathbb {R} $ mit $F(x)=\int _{a}^{x}f(t)\,\mathrm {d} t$ die Integralfunktion von $f$. Wir müssen zeigen, dass der Differentialquotient $\lim _{h\to 0}{\frac {F({\tilde {x}}+h)-F({\tilde {x}})}{h}}$ existiert und gleich $f({\tilde {x}})$ ist. Sei dazu ${\tilde {x}}\in [a,b]$ fest und $h\neq 0$ mit ${\tilde {x}}+h\in [a,b]$. Aufgrund der Additivität der Grenzen des Integrals gilt

\begin{align*}
{\frac {F({\tilde {x}}+h)-F({\tilde {x}})}{h}}={\frac {1}{h}}\left(\int _{c}^{{\tilde {x}}+h}f(t)\,{\rm {d}}t-\int _{c}^{\tilde {x}}f(t)\,{\rm {d}}t\right)={\frac {1}{h}}\int _{\tilde {x}}^{{\tilde {x}}+h}f(t)\,{\rm {d}}t
\end{align*}

Nach dem \href{https://de.wikibooks.org/wiki/Mathe\_für\_Nicht-Freaks:\_Mittelwertsatz\_für\_Integrale}
{Mittelwertsatz für Integrale} existiert eine reelle Zahl $\xi _{h}\in [{\tilde {x}},{\tilde {x}}+h]$ mit:

\begin{align*}
\int _{\tilde {x}}^{{\tilde {x}}+h}f(t)\,{\rm {d}}t=h\cdot f(\xi _{h})
\end{align*}

Im Grenzwert $h\to 0$ haben wir $\xi _{h}\to {\tilde {x}}$ wegen ${\tilde {x}}\leq \xi _{h}\leq {\tilde {x}}+h$. Mit der Stetigkeit von $f$ folgt

\begin{align*}
F'({\tilde {x}})&=\lim _{h\to 0}{\frac {F({\tilde {x}}+h)-F({\tilde {x}})}{h}}=\lim _{h\to 0}{\frac {1}{h}}\cdot h\cdot f(\xi _{h})\\[0.5em]&=\lim _{h\to 0}f(\xi _{h})=f\left(\lim _{h\to 0}\xi _{h}\right)=f({\tilde {x}})
\end{align*}

Die Ableitung von $F$ in ${\tilde {x}}$ existiert also und hat den Wert $f({\tilde {x}})$.

\end{proof*}

\subsection{Variante: Integral der Ableitungsfunktion}

\begin{theorem*}[Hauptsatz der Differential- und Integralrechnung: Integral der Ableitungsfunktion]
Sei $f\colon [a,b]\to \mathbb {R} $ eine stetige Funktion. Für jede Stammfunktion $F\colon [a,b]\to \mathbb {R} $ gilt:

\begin{align*}
\int _{a}^{b}f(x)\,\mathrm {d} x=F(b)-F(a)
\end{align*}

\end{theorem*}

\begin{proof*}[Hauptsatz der Differential- und Integralrechnung: Integral der Ableitungsfunktion]
Sei $f\colon [a,b]\to \mathbb {R} $ eine stetige und damit integrierbare Funktion. Sei ${\tilde {F}}:[a,b]\to \mathbb {R} $ mit ${\tilde {F}}(x)=\int _{a}^{x}f(t)\,\mathrm {d} t$ die Integralfunktion von $f$. Nach der Variante „Ableitung der Integralfunktion“ ist wegen der Stetigkeit von $f$ die Integralfunktion ${\tilde {F}}$ eine Stammfunktion von $f$. Außerdem gilt wegen ${\tilde {F}}(a)=\int _{a}^{a}f(t)\,\mathrm {d} t=0$:

\begin{align*}
\int _{a}^{b}f(x)\,\mathrm {d} x={\tilde {F}}(b)={\tilde {F}}(b)-{\tilde {F}}(a)
\end{align*}

Die zu beweisende Gleichung ist also für die Integralfunktion als spezielle Stammfunktion von $f$ erfüllt. Sei nun $F:[a,b]\to \mathbb {R} $ eine beliebige Stammfunktion von $f$. Da sich zwei Stammfunktionen nur um eine Konstante unterscheiden, gibt es einen Wert $C\in \mathbb {R} $ mit ${\tilde {F}}(x)=F(x)+C$. Damit ist:

\begin{align*}
\int _{a}^{b}f(t)\,\mathrm {d} t={\tilde {F}}(b)-{\tilde {F}}(a)=(F(b)+C)-(F(a)+C)=F(b)-F(a)
\end{align*}

\end{proof*}

\section{Anwendung}

Mit dem Hauptsatz können bestimmte Integrale berechnet werden. Sofern eine Stammfunktion $F$ des Integranden $f$ bekannt ist, kann das Integral $\int _{a}^{b}f\;{\rm {d}}x$ über die Differenz $F(b)-F(a)$ bestimmt werden. In der Praxis wird häufig der Ausdruck $\left[F(x)\right]_{a}^{b}$ oder $\left.F(x)\right|_{a}^{b}$ für die Differenz $F(b)-F(a)$ verwendet. Dabei spielt es keine Rolle, welche Stammfunktion gewählt wird. Da diese sich nur um eine Konstante unterscheiden, fällt diese bei der Differenz $F(b)-F(a)$ weg.

\begin{example*}[Bestimmtes Integral der Quadratfunktion]
Wir berechnen das Integral von $f(x):=x^{2}$ im Intervall $[1,3]$ mittels Umkehrung der Potenzregel. Nach der allgemeinen Ableitungsregel für Potenzen ist die Ableitungsfunktion der Polynomfunktion $f(x)=x^{3}$ gleich $f'(x)=3x^{2}$. Daher ist die Ableitungsfunktion von $f(x)={\tfrac {1}{3}}x^{3}$ gleich $f'(x)={\tfrac {1}{3}}\cdot 3x^{2}=x^{2}$. Mit dem Hauptsatz erhalten wir

\begin{align*}
\int _{1}^{3}x^{2}\,\mathrm {d} x=\left[{\frac {1}{3}}x^{3}\right]_{1}^{3}={\frac {1}{3}}\cdot 3^{3}-{\frac {1}{3}}\cdot 1^{3}={\frac {26}{3}}
\end{align*}

\end{example*}

\section{Unbestimmte Integrale}

\subsection{Definition des unbestimmten Integrals}

Zu einer Funktion gibt es mehrere Stammfunktionen. Über das unbestimmte Integral kann die Menge aller Stammfunktionen bestimmt werden:

\begin{definition*}[Unbestimmtes Integral]
Die Menge aller Stammfunktionen einer Funktion $f$ wird mit $\int f\,\mathrm {d} x$ bezeichnet und heißt \emph{unbestimmtes Integral von $f$}.

\end{definition*}

\begin{example*}[Unbestimmtes Integral]
Jede Stammfunktion der Funktion $f:\mathbb {R} \to \mathbb {R} $ mit $f(x)=3x^{2}$ hat die Zuordnungsvorschrift $F(x)=x^{3}+C$. Damit gilt:

\begin{align*}
\int 3x^{2}\,\mathrm {d} x=\{F:\mathbb {R} \to \mathbb {R} \mid F(x)=x^{3}+C,\ C\in \mathbb {R} \}
\end{align*}

Der Einfachheit halber schreiben wir kürzer:

\begin{align*}
\int 3x^{2}\,\mathrm {d} x=x^{3}+C
\end{align*}

\end{example*}

\subsection{Zusammenhang zwischen bestimmtem und unbestimmtem Integral}

Es ist wichtig, dass du zwischen bestimmtem und unbestimmtem Integral sauber unterscheidest. Das bestimmte Integral $\int _{a}^{b}f(x)\,\mathrm {d} x$ ist eine Zahl und gibt die orientierte Fläche unter den Graphen von $f$ zurück. Das unbestimmte Integral $\int f(x)\,\mathrm {d} x$ ist eine Menge von Funktionen, nämlich die Menge aller Stammfunktionen von $f$. Wie beide Begriffe zusammenhängen, wird im Hauptsatz der Differential- und Integralrechnung deutlich. Die Variante „Integral der Ableitungsfunktion“ kann folgendermaßen formuliert werden:

\begin{importantparagraph*}
Für alle Funktionen $F$ aus $\int f(x)\,\mathrm {d} x$ gilt $\int _{a}^{b}f(x)\,\mathrm {d} x=F(b)-F(a)$.

\end{importantparagraph*}

Auch die Version „Ableitung der Integralfunktion“ kann mit Hilfe des unbestimmten Integrals ausgedrückt werden:

\begin{importantparagraph*}
Die Funktion $H:\mathbb {R} \to \mathbb {R} $ mit $H(x)=\int _{a}^{x}f(t)\,\mathrm {d} t$ ist eine Funktion der Menge $\int f(x)\,\mathrm {d} x$.

\end{importantparagraph*}

Beide Aussagen gelten, wenn $f$ eine stetige Funktion ist.

\subsection{Liste von unbestimmten Integralen}

Folgende Liste gibt eine Übersicht über die wichtigsten unbestimmten Integrale. Es gilt überall $C\in \mathbb {R} $:

\begin{itemize}
\item $\int x^{s}\,\mathrm {d} x={\frac {1}{s+1}}x^{s+1}+C$ mit $s\in \mathbb {R} \setminus \{-1\}$ und $x>0$
\item $\int {\frac {1}{x}}\,\mathrm {d} x=\ln |x|+C$
\item $\int \sin(x)\,\mathrm {d} x=-\cos(x)+C$
\item $\int \cos(x)\,\mathrm {d} x=\sin(x)+C$
\item $\int \tan(x)\,\mathrm {d} x=-\ln(\cos(x))+C$
\item $\int \exp(x)\,\mathrm {d} x=\exp(x)+C$
\item $\int \ln(x)\,\mathrm {d} x=x\ln(x)-x+C$
\item $\int {\frac {1}{1+x^{2}}}\,\mathrm {d} x=\arctan(x)+C$
\item $\int {\frac {1}{\sqrt {1-x^{2}}}}\,\mathrm {d} x=\arcsin(x)+C$
\item $\int {\frac {-1}{\sqrt {1-x^{2}}}}\,\mathrm {d} x=\arccos(x)+C$
\end{itemize}

\chapter{Substitutionsregel}

Die Substitutionsregel ist eine Methode zur Bestimmung von bestimmten und unbestimmten Integralen. Diese wird aus der Kettenregel der Ableitung mit Hilfe des Hauptsatzes der Differential- und Integralrechnung gewonnen. Durch diese Regel wird das ursprüngliche Integral in ein anderes Integrationsproblem überführt, welches idealerweise leichter zu lösen ist.

\section{Herleitung}

Leider gibt es im Allgemeinen keine „Formeln“ zur Bestimmung von Stammfunktionen wie es zum Beispiel bei Ableitungen der Fall ist. Jedoch können aus den Ableitungsregeln über den Hauptsatz der Differential- und Integralrechnung Umformungsregeln für Integrale gewonnen werden. Die Substitutionsregel ist ein solches Beispiel: Wir wissen, dass die Kettenregel $h'(x)=u'(v(x))\cdot v'(x)$ für eine Funktion $h(x)=u(v(x))$ mit differenzierbaren Funktionen $u$ und $v$ gilt. Daher muss umgekehrt eine Funktion $f$ mit der Zuordnungsvorschrift $f(x)=u'(v(x))\cdot v'(x)$ die Funktion $F(x)=u(v(x))$ als Stammfunktion besitzen. Dies kann für die Integralrechnung ausgenutzt werden.

\begin{example*}[Umkehrung Kettenregel]
Sei die Funktion $f(x):=2x\sin(x^{2})$ gegeben. Wir erkennen, dass die vordere Funktion $x\mapsto 2x$ genau die Ableitung der Funktion $x\mapsto x^{2}$ ist, die „innerhalb“ des Sinus steht. Setzen wir $v'(x)=2x$, $v(x)=x^{2}$ und $u'(x)=\sin(x)$, so ist $f$ von der Form $u'(v(x))\cdot v'(x)$. Nach der Kettenregel hat eine Stammfunktion die Gestalt $F(x)=u(v(x))$. Die Stammfunktion von $u'=\sin $ lautet $u=-\cos $. Daraus folgt, dass $F(x)=-\cos(x^{2})$ eine Stammfunktion von $f(x)=2x\sin(x^{2})$ ist. Durch Ableiten von $F$ können wir dies leicht verifizieren. Für das Integrationsproblem $\int _{0}^{\sqrt {\pi }}2x\sin(x^{2})\,\mathrm {d} x$ folgt nun mit dem Hauptsatz der Differential- und Integralrechnung:

\begin{align*}
\int _{0}^{\sqrt {\pi }}2x\sin(x^{2})\,\mathrm {d} x&=\int _{0}^{\sqrt {\pi }}\underbrace {{\color {NavyBlue}\sin \left({\color {WildStrawberry}x^{2}}\right)}\cdot {\color {VioletRed}2x}} _{{\color {NavyBlue}u'({\color {WildStrawberry}v(x)})}\cdot {\color {VioletRed}v'(x)}}\,\mathrm {d} x=\left.\underbrace {\color {NavyBlue}-\cos({\color {WildStrawberry}x^{2}})} _{\color {NavyBlue}u({\color {WildStrawberry}v(x)})}\right|_{0}^{\sqrt {\pi }}\\[0.3em]&=-\underbrace {\cos \left({\sqrt {\pi }}^{2}\right)} _{=\ -1}-(-\underbrace {\cos(0)} _{=\ 1})=1+1=2
\end{align*}

\end{example*}

\section{Allgemeine Formulierung}

Damit die Substitutionsregel aus dem Hauptsatz der Differential- und Integralrechnung hergeleitet werden kann, nehmen wir alle Voraussetzungen des Hauptsatzes an:

\begin{theorem*}[Substitutionsregel]
Sei $I$ ein reelles Intervall. Seien $v\colon [a,b]\to I$ und $u\colon I\to \mathbb {R} $ zwei stetig differenzierbare Funktionen. Es ist dann

\begin{align*}
\int _{a}^{b}{\color {NavyBlue}u'({\color {WildStrawberry}v(x)})}\cdot {\color {VioletRed}v'(x)}\,\mathrm {d} x=\int _{\color {WildStrawberry}v(a)}^{\color {WildStrawberry}v(b)}{\color {NavyBlue}u'(t)}\,\mathrm {d} t=[{\color {NavyBlue}u(x)}]_{\color {WildStrawberry}v(a)}^{\color {WildStrawberry}v(b)}
\end{align*}

Insbesondere gilt

\begin{align*}
\int {\color {NavyBlue}u'({\color {WildStrawberry}v(x)})}\cdot {\color {VioletRed}v'(x)}\,\mathrm {d} x={\color {NavyBlue}u({\color {WildStrawberry}v(x)})}+C
\end{align*}

\end{theorem*}

\section{Anwendung}

Das Ziel bei der Substitution ist es, einen Integranden der Form $f(x)=u'(v(x))\cdot v'(x)$ durch Anwendung der Substitutionsmethode in den „einfacheren“ Integranden $u'(t)$ umzuwandeln.

\subsection{Anwendungsbeispiel}

\begin{example*}
Gegeben sei das Integral $\textstyle \int _{1}^{3}8x\cdot \sin(x^{2}+4)\;\mathrm {d} x$. Der Integrand ist hier $f(x)=8x\cdot \sin(x^{2}+4)$. Wichtig ist es nun zu erkennen, dass die „äußere“ Funktion $8x$ ein Vielfaches der Ableitung der „inneren“ Funktion $v(x)=x^{2}+4$ ist. Genauer gilt $v'(x)=2x$ und damit $8x=4\cdot v'(x)$. Daher „packen“ wir den Faktor $4$ zur „äußeren“ Funktion $u'$, d.h. wir setzen $u'(x)=4\cdot \sin(x)$. Wir können nun die Substitutionsregel anwenden:

\begin{align*}
\int _{1}^{3}8x\cdot \sin(x^{2}+4)\;\mathrm {d} x&=\int _{1}^{3}\underbrace {{\color {NavyBlue}4\sin \left({\color {WildStrawberry}x^{2}+4}\right)}\cdot {\color {VioletRed}2x}} _{{\color {NavyBlue}u'({\color {WildStrawberry}v(x)})}\cdot {\color {VioletRed}v'(x)}}\,\mathrm {d} x=\underbrace {\int _{\color {WildStrawberry}1^{2}+4}^{\color {WildStrawberry}3^{2}+4}{\color {NavyBlue}4\cdot \sin(t)}\;\mathrm {d} t} _{\int _{\color {WildStrawberry}v(1)}^{\color {WildStrawberry}v(3)}{\color {NavyBlue}u'(t)}\;\mathrm {d} t}\\[0.3em]&=\int _{5}^{13}4\cdot \sin(t)\;\mathrm {d} t=[-4\cdot \cos(t)]_{5}^{13}\\[0.3em]&=4\cdot (-\cos(13)-(-\cos(5)))=4\cdot (\cos(5)-\cos(13))
\end{align*}

\end{example*}

\chapter{Partielle Integration}

Bei der partiellen Integration handelt es sich um eine weitere wichtige Methode zur Berechnung von bestimmten bzw. unbestimmten Integralen. Bei dieser Regel wird mit Hilfe des Hauptsatzes der Differential- und Integralrechnung aus der Produktregel eine Formel für Integrale hergeleitet. Dabei wird das ursprüngliche Integral in ein anderes Integrationsproblem überführt, das idealerweise leichter zu lösen ist.

\section{Herleitung}

Die Formel für die partielle Integration kann aus der Produktregel für Ableitungen hergeleitet werden. Diese lautet für zwei Funktionen $f$ und $g$:

\begin{align*}
(fg)'=f'g+fg'
\end{align*}

Nehmen wir an, dass die Ableitungen $f'$ und $g'$ stetig sind, so dass wir die rechte Seite integrieren können. Wenn wir nun auf beiden Seiten das (unbestimmte) Integral bilden, erhalten wir:

\begin{align*}
&\int (f'(x)g(x)+f(x)g'(x))\mathrm {d} x=\int (fg)'(x)\,\mathrm {d} x\\[0.5em]&{\color {OliveGreen}\left\downarrow \ {\begin{aligned}&{\text{Summenregel für Integrale und }}\\&fg{\text{ ist die Stammfunktion von }}(fg)'\end{aligned}}\right.}\\[0.5em]\implies &\int f'(x)g(x)\,\mathrm {d} x+\int f(x)g'(x)\;\mathrm {d} x=f(x)\cdot g(x)\\[0.5em]\implies &\int f'(x)g(x)\,\mathrm {d} x=f(x)\cdot g(x)-\int f(x)g'(x)\;\mathrm {d} x
\end{align*}

Damit haben wir folgende Formel für das unbestimmte Integral gefunden:

\begin{align*}
\int {\color {WildStrawberry}f'(x)}\cdot {\color {NavyBlue}g(x)}\,\mathrm {d} x={\color {WildStrawberry}f(x)}\cdot {\color {NavyBlue}g(x)}-\int {\color {WildStrawberry}f(x)}\cdot {\color {NavyBlue}g'(x)}\,\mathrm {d} x
\end{align*}

Für das bestimmte Integral kann analog eine Formel gefunden werden. Diese lautet:

\begin{align*}
\int _{a}^{b}{\color {WildStrawberry}f'(x)}\cdot {\color {NavyBlue}g(x)}\,\mathrm {d} x=[{\color {WildStrawberry}f(x)}\cdot {\color {NavyBlue}g(x)}]_{a}^{b}-\int _{a}^{b}{\color {WildStrawberry}f(x)}\cdot {\color {NavyBlue}g'(x)}\,\mathrm {d} x
\end{align*}

Wir haben so eine Formel gefunden, mit der man das Integrationsproblem in ein anderes überführen kann. In der Praxis lohnt sich die Anwendung dieser Formel, wenn das Integral $\int f(x)g'(x)\;\mathrm {d} x$ einfacher zu berechnen ist als das Ausgangsintegral $\int f'(x)g(x)\,\mathrm {d} x$. Insbesondere muss hierfür eine Stammfunktion von $f'$ bekannt sein.

Betrachten wir zum Einstieg das unbestimmte Integral $\int e^{x}x\;\mathrm {d} x$. Eine Stammfunktion von $e^{x}x$ ist nicht direkt erkennbar. Wählen wir jedoch $f'(x)=e^{x}$ und $g(x)=x$ in der obigen Formel, so erhalten wir mit $f(x)=e^{x}$ und $g'(x)=1$:

\begin{align*}
\int \underbrace {{\color {WildStrawberry}e^{x}}\cdot {\color {NavyBlue}x}} _{{\color {WildStrawberry}f'(x)}\cdot {\color {NavyBlue}g(x)}}\;\mathrm {d} x=\underbrace {{\color {WildStrawberry}e^{x}}\cdot {\color {NavyBlue}x}} _{{\color {WildStrawberry}f(x)}\cdot {\color {NavyBlue}g(x)}}-\int \underbrace {{\color {WildStrawberry}e^{x}}\cdot {\color {NavyBlue}1}} _{{\color {WildStrawberry}f(x)}\cdot {\color {NavyBlue}g'(x)}}\mathrm {d} x=e^{x}x-e^{x}+c
\end{align*}

Damit haben wir, ohne allzu großen Aufwand, eine Stammfunktion von $e^{x}x$ berechnet. Der entscheidende Punkt war, dass wir das „neue“ Integral $\int e^{x}\;\mathrm {d} x$ im Gegensatz zum ursprünglichen Integral $\int e^{x}x\;\mathrm {d} x$ bestimmen konnten.

\section{Anwendungsbeispiele}

Um die partielle Integration anwenden zu können, muss der Integrand die Form $f'\cdot g$ haben oder in diese gebracht werden. Hier muss man sich überlegen, welcher der Faktoren des Produkts die Rolle von $f'$ übernehmen soll. Auch muss die Stammfunktion von $f'$ bekannt sein. Im Folgenden werden wir typische Anwendungsmöglichkeiten der partiellen Integration betrachten.

\subsection{Typ: $\int p(x)\cdot f'(x)\,\mathrm {d} x$ mit einer Polynomfunktion $p$}

Die partielle Integration ist bei Funktionen nützlich, die sich als Produkt einer Polynomfunktion und einer integrierbaren Funktion schreiben lassen. Das hat den Hintergrund, dass der Grad der Polynomfunktion mit jeder Ableitung um einen Grad reduziert wird. Die integrierbare Funktion wird dabei als $f'$ und die Polynomfunktion als $g$ gewählt. Dabei sollte jedoch die Stammfunktion $f$ nicht „komplizierter“ als $f'$ sein.

\begin{example*}
Als Beispiel betrachten wir das unbestimmte Integral $\int e^{x}\cdot \left(2-x^{2}\right)\,\mathrm {d} x$. Setzen wir bei jedem partiellen Integrationsschritt $f'(x)=e^{x}$ und $g(x)$ den übrigen (Polynom-)Term unter dem Integral, so ergibt sich:

\begin{align*}
\int \underbrace {{\color {WildStrawberry}e^{x}}\cdot {\color {NavyBlue}\left(2-x^{2}\right)}} _{{\color {WildStrawberry}f'(x)}\cdot {\color {NavyBlue}g(x)}}\,\mathrm {d} x&=\underbrace {{\color {WildStrawberry}e^{x}}\cdot {\color {NavyBlue}\left(2-x^{2}\right)}} _{{\color {WildStrawberry}f(x)}\cdot {\color {NavyBlue}g(x)}}-\int \underbrace {{\color {WildStrawberry}e^{x}}\cdot {\color {NavyBlue}(-2x)}} _{{\color {WildStrawberry}f(x)}\cdot {\color {NavyBlue}g'(x)}}\,\mathrm {d} x\\[0.5em]&=e^{x}\cdot \left(2-x^{2}\right)+\int \underbrace {{\color {WildStrawberry}e^{x}}\cdot {\color {NavyBlue}2x}} _{{\color {WildStrawberry}f'(x)}\cdot {\color {NavyBlue}g(x)}}\,\mathrm {d} x\\[0.5em]&=e^{x}\cdot \left(2-x^{2}\right)+\left(\underbrace {{\color {WildStrawberry}e^{x}}\cdot {\color {NavyBlue}2x}} _{{\color {WildStrawberry}f(x)}\cdot {\color {NavyBlue}g(x)}}-\int \underbrace {{\color {WildStrawberry}e^{x}}\cdot {\color {NavyBlue}2}} _{{\color {WildStrawberry}f(x)}\cdot {\color {NavyBlue}g'(x)}}\,\mathrm {d} x\right)\\[0.5em]&=e^{x}\cdot \left(2-x^{2}\right)+e^{x}\cdot 2x-2\cdot e^{x}+C\\[0.3em]&=e^{x}\cdot \left(2-x^{2}+2x-2\right)+C=e^{x}\cdot \left(2x-x^{2}\right)+C
\end{align*}

Hier mussten wir mehrfach partiell integrieren, um die gewünschte Stammfunktion zu erhalten. Da die trigonometrischen Funktionen $\sin(x)$ und $\cos(x)$ sich analog zu der Exponentialfunktion ebenfalls leicht integrieren lassen, bietet sich obige Methode auch für diese Funktionen als $f'$ an.

\end{example*}

\subsection{Indirekte Berechnung von Integralen}

Bei der partiellen Integration wird häufig das ursprüngliche Integral durch partielle Integration vereinfacht, um es anschließend berechnen zu können. Bei manchen Integralen gibt es durch (mehrfache) partielle Integration die Möglichkeit, dass das ursprüngliche Integral wiederkehrt. Durch Äquivalenzumformungen kann dieses dann bestimmt werden. Mittels eines Beispiels lässt sich der Trick am besten nachvollziehen:

\begin{example*}
Als Beispiel wollen wir das unbestimmte Integral $\int \sin(x)\cdot \cos(x)\,\mathrm {d} x$ berechnen. Wir setzen $g(x)=\cos(x)$ und $f'(x)=\sin(x)$ erhalten:

\begin{align*}
\int \underbrace {{\color {WildStrawberry}\sin(x)}\cdot {\color {NavyBlue}\cos(x)}} _{{\color {WildStrawberry}f'(x)}\cdot {\color {NavyBlue}g(x)}}\,\mathrm {d} x&=\underbrace {{\color {WildStrawberry}-\cos(x)}\cdot {\color {NavyBlue}\cos(x)}} _{{\color {WildStrawberry}f(x)}\cdot {\color {NavyBlue}g(x)}}-\int \underbrace {{\color {WildStrawberry}(-\cos(x))}\cdot {\color {NavyBlue}(-\sin(x))}} _{{\color {WildStrawberry}f(x)}\cdot {\color {NavyBlue}g'(x)}}\,\mathrm {d} x\\[0.5em]&=-\cos ^{2}(x)-\int \sin(x)\cdot \cos(x)\,\mathrm {d} x
\end{align*}

Addieren wir auf beiden Seiten der Gleichung das Ausgangsintegral $\int \sin(x)\cdot \cos(x)\,\mathrm {d} x$, so folgt

\begin{align*}
&2\int \sin(x)\cdot \cos(x)\,\mathrm {d} x=-\cos ^{2}(x)\\[0.5em]\implies &\int \sin(x)\cdot \cos(x)\,\mathrm {d} x=-{\frac {1}{2}}\cos ^{2}(x)
\end{align*}

So haben wir eine Stammfunktion gefunden. Alle Stammfunktionen haben somit die Form

\begin{align*}
\int \sin(x)\cdot \cos(x)\,\mathrm {d} x=-{\frac {1}{2}}\cos ^{2}(x)+C
\end{align*}

\end{example*}

\includepdf[pages=-]{predesigned_pages/mfnf_fundraising}

\pagebreak
\ColoredLOF

\clearpage%
\thispagestyle{empty}%
\null%
\clearpage

\includepdf[pages=-2]{predesigned_pages/mfnf_epilogue}

\end{document}
